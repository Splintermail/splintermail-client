import abc
import argparse
import contextlib
import io
import re
import sys
import textwrap


class FirstFirst(Exception):
    pass

class FirstFollow(Exception):
    pass

class InfiniteRecursion(Exception):
    pass

def add_to_prev(prev, name, call):
    if prev is None:
        prev = []
    if name in prev:
        raise InfiniteRecursion(
            "circular " + call + "() detected in " + name + ": " + str(prev)
        )
    return list(prev) + [name]

def cacheable(fn):
    cache = None
    prop = "_cached_" + fn.__name__
    def _fn(self, *args, **kwargs):
        if not hasattr(self, prop):
            setattr(self, prop, fn(self, *args, **kwargs))
        return getattr(self, prop)
    return _fn


class Parsable(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def title(self):
        pass

    @abc.abstractmethod
    def get_first(self, prev=None):
        pass

    @abc.abstractmethod
    def get_disallowed_after(self, prev=None):
        pass

    @abc.abstractmethod
    def check(self):
        pass


class Token(Parsable):
    def __init__(self, name, typ=None):
        self.name = name
        self.type = typ

    @property
    def title(self):
        return self.name

    @cacheable
    def get_first(self, prev=None):
        return {self.name}

    @cacheable
    def get_disallowed_after(self, prev=None):
        return set()

    @cacheable
    def check(self):
        pass


class Snippet:
    def __init__(self, text, tag=None):
        self.text = text
        self.tag = tag


class Maybe(Parsable):
    def __init__(self, name):
        self.name = name
        self.seq = Sequence(self.name + ".seq")

    @property
    def title(self):
        return "maybe(" + self.name + ")"

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        # TODO: is this necessary at all?
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        return first.union({None})

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        assert None not in disallowed, "invalid .maybe() on maybe-empty expression"
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first()
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        # always include None, since this term may be missing
        return disallowed.union(first).union({None})

    @cacheable
    def check(self):
        self.seq.check()



class ZeroOrMore(Parsable):
    def __init__(self, name):
        self.name = name
        self.seq = Sequence(self.name + '.seq')

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    @property
    def title(self):
        return "zero_or_more(" + self.name + ")"

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        assert None not in first, "invalid .zero_or_more() on maybe-empty expression: " + self.seq.title
        return first.union({None})

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        assert None not in disallowed, "invalid .maybe() on maybe-empty expression"
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first(prev)
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        # always include None, since this term may be missing
        return disallowed.union(first).union({None})

    @cacheable
    def check(self):
        self.seq.check()


class Branches(Parsable):
    def __init__(self, name):
        self.name = name
        self.branches = []
        self._branch = None

    @property
    def title(self):
        return self.name

    def add_term(self, term, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_term(term, tag)

    def add_code(self, code, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_code(code, tag)

    @contextlib.contextmanager
    def branch(self):
        assert self._branch is None, "can't nest calls to branch"
        self._branch = Sequence(self.name + '[' + str(len(self.branches)) + ']')
        yield
        self.branches.append(self._branch)
        self._branch = None

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for branch in self.branches:
            first = first.union(branch.get_first(list(prev)))

        return frozenset(first)

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        prev = list(prev) + [self.name]
        disallowed = set()
        for branch in self.branches:
            disallowed = disallowed.union(branch.get_disallowed_after(list(prev)))

        return frozenset(disallowed)

    @cacheable
    def check(self):
        first = set()
        for branch in self.branches:
            branch.check()

            # no possibly-empty branches
            branch_first = branch.get_first()
            assert None not in branch_first

            # Ensure that no branches begin with the same tokens
            conflicts = first.intersection(branch_first)
            assert not conflicts, "FIRST/FIRST conflicts for tokens " + str(conflicts)

            first = first.union(branch_first)


class Recovery(Parsable):
    def __init__(self, name, code=None, tag=None):
        self.name = name
        # allow defining the recovery code upfront
        self.code = [] if code is None else [Snippet(code, tag)]
        self.seq = Sequence(self.name + '.seq')
        self.after = None

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    def add_recovery_code(self, code, tag=None):
        self.code.append(Snippet(code, tag))

    def set_after(self, after):
        assert self.after is None, "can't call set_after() twice!"
        self.after = after

    @property
    def title(self):
        return "recovery(" + self.name + ")"

    @cacheable
    def get_first(self, prev=None):
        return self.seq.get_first(prev)

    @cacheable
    def get_disallowed_after(self, prev):
        return self.seq.get_disallowed_after(prev)

    @cacheable
    def check(self):
        self.seq.check()


class Sequence(Parsable):
    """
    A sequence of terms which must be matched consecutively.
    """
    def __init__(self, name):
        self.name = name
        self.scopes = [self]
        self.precode = []
        # tuples of (Parsable, tag=None, code=[])
        self.terms = []

    def add_term(self, term, tag=None):
        self.terms.append((term, tag, []))

    def add_code(self, code, tag=None):
        if not self.terms:
            self.precode.append(Snippet(code, tag))
        else:
            self.terms[-1][2].append(Snippet(code, tag))

    @property
    def title(self):
        return self.name

    def get_first_ex(self, start, prev):
        """
        Get the list of all tokens that this expression could start with.

        Ex: at runtime, when are processing a .maybe(), we need to be able to
        look ahead one token and know if that token can be the first token
        of this expression.
        """
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for term, _, _ in self.terms[start:]:
            term_first = term.get_first(list(prev))
            # detect conflicts in .check(), not here.
            first = first.union(term_first)
            # if None is not in the set, we have our answer.
            if None not in first:
                break
            # if None is in the set, check the next term too.
            first.remove(None)
        else:
            # restore the None from the final term_first
            first.add(None)

        return frozenset(first)

    @cacheable
    def get_first(self, prev=None):
        return self.get_first_ex(0, prev)

    @cacheable
    def get_disallowed_after(self, prev=None):
        """
        Get the list of all tokens which would cause ambiguities after this
        expression.

        Ex. a maybe(EOL) term could not be followed by a match(EOL) term; it
        would be ambiguous if an EOL matched to the maybe() or the match().
        """
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = set()
        for term, _, _ in reversed(self.terms):
            term_disallowed = term.get_disallowed_after(list(prev))
            disallowed = disallowed.union(term_disallowed)
            # if None is not in this term's disallowed, we have our answer
            if None not in disallowed:
                break
            # if None is in the set, check the next term too
            disallowed.remove(None)
        else:
            # restore the None from the final term_disallowed
            disallowed.add(None)

        return frozenset(disallowed)

    @cacheable
    def check(self):
        """
        At generation time, we need to ensure that for every Term in the
        expression, there is never a point where the lookahead token could
        possibly match the next token.
        """
        prev = {self.name}

        for term, _, _ in self.terms:
            # don't allow recursion
            if not isinstance(term, Expression):
                term.check()

        disallowed = set()
        for i, (term, _, _) in enumerate(self.terms):
            term_first = term.get_first(list(prev))
            # Ignore None when checking for conflicts
            disallowed = disallowed.difference({None})
            conflicts = disallowed.intersection(term_first)
            if conflicts:
                raise FirstFollow(
                    "FIRST/FOLLOW conflicts:" + str(conflicts) + "\n"
                    "found while checking expression " + self.title + "\n"
                    "at least one token that starts " + term.title + "\n"
                    "is disallowed by that point."
                )
            term_disallowed = term.get_disallowed_after(list(prev))
            if None in term_disallowed:
                # disallowed should grow
                disallowed = disallowed.union(term_disallowed)
            else:
                # disallowed is reset
                disallowed = term_disallowed

            # Special case: check Recovery terms and call their set_after().
            if isinstance(term, Recovery):
                after_recovery = self.get_first_ex(i+1, None)
                if None in after_recovery:
                    raise ValueError(
                        "a Recovery must be followed by a never-empty match "
                        "within the same Sequence"
                    )
                term.set_after(after_recovery)


class Expression(Parsable):
    """
    Each Expression will generate one function in the parser.
    """
    def __init__(self, name, typ=None):
        self.name = name
        self.type = typ
        self.seq = Sequence(self.name + ".seq")
        self.scopes = [self.seq]
        self.nmaybe = 0
        self.nzom = 0
        self.nbranches = 0
        self.nrec = 0

    # API for defining the grammar.

    def __call__(self, fn):
        """Provide a definition for a fowards-declared expression"""
        assert fn.__name__ == self.name, fn.__name__ + " != " + self.name
        fn(self)
        return self

    def match(self, term, tag=None):
        self.scopes[-1].add_term(term, tag)
        return term

    @contextlib.contextmanager
    def maybe(self):
        term = Maybe(self.name + '.maybe' + str(self.nmaybe))
        self.nmaybe += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def zero_or_more(self):
        term = ZeroOrMore(self.name + '.zom' + str(self.nzom))
        self.nzom += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def branches(self):
        term = Branches(self.name + '.br' + str(self.nbranches))
        self.nbranches += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def recovery(self, code=None, tag=None):
        term = Recovery(self.name + ".recovery" + str(self.nrec), code, tag)
        self.nrec += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    def exec(self, code, tag=None):
        self.scopes[-1].add_code(code, tag)

    # Non-user-facing methods.

    @property
    def title(self):
        return self.name

    def get_first(self, prev=None):
        # Refer to our Sequence.
        return self.seq.get_first(prev)

    def get_disallowed_after(self, prev=None):
        # Refer to our Sequence.
        return self.seq.get_disallowed_after(prev)

    def check(self):
        return self.seq.check()


class Grammar:
    def __init__(self):
        self.exprs = {}

    def get_name(self, name):
        """
        Creates a name if it doesn't yet exist.  Mostly for the meta parser.
        """
        if name in self.exprs:
            return self.exprs[name]
        if name[0].upper() == name[0]:
            # starts uppercase, we guess it's a token
            e = self.token(name)
        else:
            # starts lowercase, we guess it's a expression
            e = self.expr(name)
        return e

    def expr(self, val):
        if isinstance(val, str):
            # Forward declaration case:
            #     @g.expr
            #     def my_expr(e):
            #         e.match(...)
            fn = None
            name = val
        else:
            # Declaration/definition case.
            #     my_expr = g.expr("my_expr")
            #
            #     ...
            #
            #     @my_expr
            #     def my_expr(e):
            #         e.match(...)
            fn = val
            name = fn.__name__

        assert name not in self.exprs
        e = Expression(name)
        self.exprs[name] = e
        if fn is not None:
            fn(e)
        return e

    def token(self, name, typ=None):
        assert name not in self.exprs
        e = Token(name, typ)
        self.exprs[name] = e
        return e

    def sorted_tokens(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Token):
                yield name, expr

    def sorted_expressions(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Expression):
                yield name, expr

    def check(self):
        for _, expr in self.sorted_expressions():
            expr.check()


class C:
    """
    An object with all the methods for generating C code.
    """
    code_tag = "c"

    class Variables:
        def __init__(self, semval, semloc):
            # a list of lists of available variables
            self.scopes = [[]]
            self.semval = semval
            self.semloc = semloc

        def new_scope(self):
            self.scopes.append([])

        def define(self, tag, typ, pos, f):
            for scope in self.scopes:
                assert tag not in scope, "tag " + tag + " shadows another tag"
            suffix = "OUT" if tag == "$" else "_" + tag
            if self.semval:
                print(
                    "    #define YYVAL" + suffix
                    + " (p->semstack[call->stack + " + str(pos) + "].val." + typ + ")",
                    file=f,
                )
            if self.semloc:
                print(
                    "    #define YYLOC" + suffix
                    + " (p->semstack[call->stack + " + str(pos) + "].loc." + typ + ")",
                    file=f,
                )
            self.scopes[-1].append(tag)

        def pop_scope(self, f):
            popped = self.scopes.pop()
            for tag in popped:
                suffix = "OUT" if tag == "$" else "_" + tag
                if self.semval:
                    print("    #undef YYVAL" + suffix, file=f)
                if self.semloc:
                    print("    #undef YYPOS" + suffix, file=f)

    def __init__(
        self,
        *,
        # required kwargs
        root,
        # optional kwargs
        user_args=(),
        semval_type=None,
        semloc_type=None,
        span_fn=None,
    ):
        self.root = root
        self.semval_type=semval_type
        assert bool(semloc_type) + bool(span_fn) in (0, 2), (
            "semloc_type and span_fn must be both set or both unset"
        )
        self.semloc_type=semloc_type
        self.span_fn=span_fn
        self.u = (
            user_args.split(",") if isinstance(user_args, str) else user_args
        )

    def print_code(self, code, i, f, v=None):
        if code is None:
            return
        for c in code:
            if c.tag in (None, "c"):
                print(i + "// USER CODE", file=f)
                break
        for c in code:
            if c.tag in (None, "c"):
                # print(textwrap.indent(c.text, i), file=f)
                text = re.sub("\\$\\$", "YYVALOUT", c.text)
                text = re.sub("\\$([a-zA-Z_][a-zA-Z0-9_]*)", "YYVAL_\\1", text)
                text = re.sub("@@", "YYVALOUT", text)
                text = re.sub("@([a-zA-Z_][a-zA-Z0-9_]*)", "YYLOC_\\1", text)
                print(textwrap.indent(text, i), file=f)

        # # Check all references are in-scope (currently done by compiler)
        # for c in code:
        #     if c.tag in (None, self.code_tag):
        #         errors = []
        #         # check scoping of references
        #         matches = re.findall("\\$([a-zA-Z_][a-zA-Z0-9_]*)", c.text)
        #         for m in matches:
        #             if not v:
        #                 errors.append("$" + m)
        #                 continue
        #             for scope in v.scopes:
        #                 if m in scope:
        #                     break
        #             else:
        #                 errors.append("$" + m)
        #         if errors:
        #             avail = []
        #             if v:
        #                 for scope in v.scopes:
        #                     avail += list(scope)
        #             raise ValueError(
        #                 "found one or more references (" + str(matches) + ") "
        #                 "not valid in the current scope (" + str(avail) + ")"
        #             )

    def token_check(self, first):
        t = sorted(t for t in first if t is not None)
        return "token_in(*token, " + str(len(t)) + ", " + ", ".join(t) + ")"

    def mask_set(self, tokens):
        t = sorted(t for t in tokens if t is not None)
        return "mask_set(p->mask, " + str(len(t)) + ", " + ", ".join(t) + ")"

    def stackpersist(self, obj):
        """Return the persistent created during a match to obj."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Expression):
            return 1
        return 0

    def stackmax(self, obj):
        """Return the maximum stack size during a match to obj."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Expression):
            return 1
        if isinstance(obj, Maybe):
            return self.stackmax(obj.seq)
        if isinstance(obj, ZeroOrMore):
            return self.stackmax(obj.seq)
        if isinstance(obj, Branches):
            return max(self.stackmax(b) for b in obj.branches)
        if isinstance(obj, Recovery):
            return self.stackmax(obj.seq)
        if isinstance(obj, Sequence):
            persist = 0
            peak = 0
            for t, _, _ in obj.terms:
                peak = max(peak, persist + self.stackmax(t))
                persist += self.stackpersist(t)
            return peak
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def nstates(self, obj):
        """Count how many states we need for every entry."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Maybe):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, ZeroOrMore):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, Branches):
            return sum(self.nstates(b) for b in obj.branches) + 1
        if isinstance(obj, Recovery):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, Sequence):
            return sum(self.nstates(t) for t, _, _ in obj.terms)
        if isinstance(obj, Expression):
            return 1
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def gen(self, obj, state, stack, var, f, tag=None):
        """Generate code to match an object."""
        if isinstance(obj, Token):
            print("    // match a " + obj.title + " token", file=f)
            print("    " + self.mask_set(obj.get_first()) + ";", file=f)
            print("    call->state = " + str(state) + ";", file=f)
            print("    AWAIT_TOKEN;", file=f)
            print("yy" + str(state) + ":", file=f)
            print("    if(*token != " + obj.title + ") SYNTAX_ERROR;", file=f)
            print("    p->semstack[call->stack + " + str(stack) + "] = sem;", file=f)
            print("    CONSUME_TOKEN;", file=f)
            print("    mask_clear(p->mask);", file=f)
            return state + self.nstates(obj), stack + 1

        if isinstance(obj, Maybe):
            print("    // maybe match " + obj.title, file=f)
            print("    call->state = " + str(state) + ";", file=f)
            print("    AWAIT_TOKEN;", file=f)
            print("yy" + str(state) + ":", file=f)
            print("    if(!" + self.token_check(obj.get_first()) + "){", file=f)
            print("        " + self.mask_set(obj.get_first()) + ";", file=f)
            print("        goto yy" + str(state) + "_done;", file=f)
            print("    }", file=f)
            self.gen(obj.seq, state+1, stack, var, f)
            print("yy" + str(state) + "_done:", file=f)
            return state + self.nstates(obj), stack

        if isinstance(obj, ZeroOrMore):
            print("    // match zero or more " + obj.title, file=f)
            print("yy" + str(state) + ":", file=f)
            print("    call->state = " + str(state) + ";", file=f)
            print("    AWAIT_TOKEN;", file=f)
            print("    if(!" + self.token_check(obj.get_first()) + "){", file=f)
            print("        " + self.mask_set(obj.get_first()) + ";", file=f)
            print("        goto yy" + str(state) + "_done;", file=f)
            print("    }", file=f)
            print("", file=f)
            self.gen(obj.seq, state + 1, stack, var, f)
            print("    // try matching another " + obj.title, file=f)
            print("    goto yy" + str(state) + ";", file=f)
            print("yy" + str(state) + "_done:", file=f)
            return state + self.nstates(obj), stack

        if isinstance(obj, Branches):
            print("    // match branches " + obj.title, file=f)
            print("    call->state = " + str(state) + ";", file=f)
            print("    AWAIT_TOKEN;", file=f)
            print("yy" + str(state) + ":", file=f)
            print("    switch(*token){", file=f)
            bstate = state + 1
            for i, b in enumerate(obj.branches):
                for t in b.get_first():
                    print(
                        "        case " + t + ": goto yy"
                        + str(state) + "_br_" + str(i) + ";"
                    , file=f)
                bstate += self.nstates(b)
            print("        default:", file=f)
            print("            " + self.mask_set(obj.get_first()) + ";", file=f)
            print("            SYNTAX_ERROR;", file=f)
            print("    }", file=f)
            print("", file=f)
            # branch parsing
            bstate = state + 1
            for i, b in enumerate(obj.branches):
                print("yy" + str(state) + "_br_" + str(i) + ":", file=f)
                bstate, stack = self.gen(b, bstate, stack, var, f)
                print("    goto yy" + str(state) + "_done;", file=f)
                print("", file=f)
            print("yy"+ str(state) + "_done:", file=f)
            return state + self.nstates(obj), stack

        if isinstance(obj, Recovery):
            assert obj.after is not None, ".set_after() was never called"
            print("    // prepare for recovery " + obj.title, file=f)
            print("    call->recover = " + str(state) + ";", file=f)
            print("", file=f)
            self.gen(obj.seq, state + 1, stack, var, f)
            print("", file=f)
            print("    call->recover = 0;", file=f)
            print("    goto yy" + str(state) + "_done;", file=f)
            print("yy"+ str(state) + ":", file=f)
            print("    // recovery: consume all tokens until a valid one", file=f)
            print("    AWAIT_TOKEN;", file=f)
            print("    if(!" + self.token_check(obj.after) + "){", file=f)
            print("        CONSUME_TOKEN;", file=f)
            print("        return 0;", file=f)
            print("    }", file=f)
            self.print_code(obj.code, "    ", f, var)
            print("yy"+ str(state) + "_done:", file=f)
            return state + self.nstates(obj), stack

        if isinstance(obj, Sequence):
            var.new_scope()
            self.print_code(obj.precode, "    ", f, var)
            for i, (t, tag, code) in enumerate(obj.terms):
                if tag is not None:
                    assert isinstance(t, (Token, Expression)), "wrong type for tag: " + type(t).__name__
                    assert t.type is not None, "obj " + t.title + " has tag " + tag + " but no type!"
                    var.define(tag, t.type, stack, f)
                state, stack = self.gen(t, state, stack, var, f, tag)
                self.print_code(code, "    ", f, var)
                if i+1 != len(obj.terms):
                    print("", file=f)
            var.pop_scope(f)
            return state, stack

        if isinstance(obj, Expression):
            # code generated when calling this function
            print("    // match " + obj.title, file=f)
            print("    call->state = " + str(state) + ";", file=f)
            print("    CALL(" + self.parse_fn(obj) + ", " + str(stack) + ");", file=f)
            print("yy" + str(state) + ":", file=f)
            return state + self.nstates(obj), stack + 1

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def parse_fn(self, expr):
        """Get the generated function name for an Expression."""
        return "_parse_" + expr.title

    def declare_fn(self, expr, f):
        print("static int " + self.parse_fn(expr) + "(PARSE_FN_ARGS);", file=f)

    def define_fn(self, expr, f):
        """Generate the function definition for an Expression."""
        var = []
        print("static int " + self.parse_fn(expr) + "(PARSE_FN_ARGS){", file=f)
        # Ensure we have enough stack space to operate.
        print("    assert_sems_available(p, " + str(1 + self.stackmax(expr.seq)) + ");", file=f)
        # Jump to state.
        print("    switch(call->state){", file=f)
        print("        case " + str(0) + ": break;", file=f)
        for n in range(self.nstates(expr.seq)):
            print("        case " + str(n+1) + ": goto yy" + str(n+1) + ";", file=f)
        print("    }", file=f)
        print("", file=f)
        var = C.Variables(semval=self.semval_type, semloc=self.semloc_type)
        if expr.type is not None:
            var.define("$", expr.type, 0, f)

        # state starts at 1, since 0 is only the first time we're called
        # stack starts at 1, since we have always allocate the output
        self.gen(expr.seq, 1, 1, var, f)

        var.pop_scope(f)
        print("    // cleanup this call", file=f);
        print("    free_sems(p, call->stack+1);", file=f)
        print("    p->callslen--;", file=f);
        print("    return 0;", file=f)
        print("}", file=f)
        print("", file=f)

    def gen_file(self, g, precode=None, postcode=None, file=None):
        f = file
        self.print_code(precode, "", f)
        print(textwrap.dedent("""
            #include <stdio.h>
            #include <stdbool.h>
            #include <stdlib.h>
            #include <stdarg.h>
            #include <stdint.h>
            #include <string.h>
        """.strip("\n")), file=f)

        print("typedef enum {", file=f)
        print("    _NOT_A_TOKEN = 0,", file=f)
        for _, token in g.sorted_tokens():
            print("    " + token.title + ",", file=f)
        print("} imf_token_t;", file=f)
        print("", file=f)

        print(textwrap.dedent(r"""
            struct imf_parser_t;
            typedef struct imf_parser_t imf_parser_t;
            struct call_t;
            typedef struct call_t call_t;
            struct sem_t;
            typedef struct sem_t sem_t;

            #define PARSE_FN_ARGS \
                call_t *call, imf_parser_t *p, imf_token_t *token, sem_t sem

            typedef int (*parse_fn_t)(PARSE_FN_ARGS);

            struct call_t {
                // a function to call
                parse_fn_t fn;

                // our starting position in the semstack
                int stack;

                // our position in the call
                int state;

                // the state to enter if we have to recover
                int recover;
            };
            """).strip('\n'), file=f)

        print("", file=f)
        print("struct sem_t {", file=f)
        print("    int type;", file=f)
        if self.semval_type:
            print("    // semantic value", file=f)
            print("    " + self.semval_type + " val;", file=f)
        else:
            print("    // no semantic value; semval_type not configured", file=f)

        if self.semloc_type:
            print("    // semantic location", file=f)
            print("    " + self.semloc_type + " val;", file=f)
        else:
            print("    // no semantic location; semloc_type not configured", file=f)
        print("};", file=f)
        print("", file=f)

        masklen = (len([t for t in g.sorted_tokens()]) + 7) // 8
        print("// we need MASKLEN chars to have one bit per token", file=f)
        print("#define MASKLEN %d"%masklen, file=f)
        print("", file=f)

        print(textwrap.dedent(r"""
            static void mask_clear(unsigned char *mask){
                memset(mask, 0, MASKLEN);
            }

            // accepts `n` ints, representing token values
            static void mask_set(unsigned char *mask, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                for(size_t i = 0; i < n; i++){
                    int t = va_arg(ap, int);
                    mask[t/8] |= ((unsigned char)1) << (t%8);
                }

                va_end(ap);
            }

            static const char *token_name(int t);

            static void mask_print(unsigned char *mask, char *joiner){
                bool first = true;
                for(int i = 0; i < MASKLEN; i++){
                    for(int j = 0; j < 8; j++){
                        if(!(mask[i] & ((unsigned char)1)<<j)) continue;
                        printf(
                            "%s%s",
                            first ? (first=false,"") : joiner,
                            token_name(i*8+j)
                        );
                    }
                }
            }

            struct imf_parser_t {
                // stack of calls
                call_t *callstack;
                size_t callslen;
                size_t callsmax;
                // stack of semvals
                sem_t *semstack;
                size_t semslen;
                size_t semsmax;
                // a mask of all possible tokens
                unsigned char mask[MASKLEN];
            };

            call_t *calls_append(imf_parser_t *p){
                if(p->callslen == p->callsmax){
                    printf("callstack overflow!\n");
                    exit(1);
                }
                return &p->callstack[p->callslen++];
            }

            void assert_sems_available(imf_parser_t *p, size_t n){
                if(p->semslen + n >= p->semsmax){
                    printf("semstack overflow!\n");
                    exit (1);
                }
            }

            sem_t *sems_append(imf_parser_t *p){
                if(p->semslen == p->semsmax){
                    printf("semstack overflow!\n");
                    exit(1);
                }
                return &p->semstack[p->semslen++];
            }

            void free_sems(imf_parser_t *p, size_t first){
                for(size_t i = first; i < p->semsmax; i++){
                    // TODO: actually free stuff
                    p->semstack[i] = (sem_t){0};
                }
            }

            static const char *_fn_name(parse_fn_t fn);

            static void print_stack(imf_parser_t *p){
                for(size_t i = 0; i < p->callslen; i++){
                    call_t call = p->callstack[i];
                    printf("parsing %s (%d)\n", _fn_name(call.fn), call.state);
                }

            }

            #define CALL(_fn, _stack) do { \
                /* allocate call */ \
                call_t *subcall = calls_append(p); \
                *subcall = (call_t){ .fn = _fn,  .stack = call->stack + _stack }; \
                return 0; \
            } while(0)

            #define AWAIT_TOKEN \
                if(!*token) return 0

            #define CONSUME_TOKEN \
                *token = _NOT_A_TOKEN

            bool token_in(imf_token_t token, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                bool out = false;
                for(size_t i = 0; i < n; i++){
                    if(token != va_arg(ap, int)) continue;
                    out = true;
                    break;
                }

                va_end(ap);
                return out;
            }

            #define SYNTAX_ERROR do { \
                printf("syntax error!\n"); \
                print_stack(p); \
                printf("expected one of: {"); \
                mask_print(p->mask, ","); \
                printf("} but got %s\n", token_name(*token)); \
                return 1; \
            } while(0)

            void do_parse(
                imf_parser_t *p,
                parse_fn_t entrypoint,
                imf_token_t token,
                sem_t sem,
                bool *ok
            ){
                *ok = false;

                if(p->callslen == 0){
                    // first token, initialize the call
                    call_t *call = calls_append(p);
                    *call = (call_t){
                        .fn = entrypoint, .stack = p->semslen-1
                    };
                }

                while(p->callslen){
                    size_t last = p->callslen;
                    call_t *call = &p->callstack[p->callslen-1];
                    int syntax_error = call->fn(call, p, &token, sem);
                    if(!syntax_error){
                        if(p->callslen == last){
                            // pause until the next token
                            return;
                        }
                        continue;
                    }
                    // syntax error; pop from the stack until we can recover
                    while(p->callslen){
                        call_t *call = &p->callstack[p->callslen-1];
                        if(call->recover){
                            // this call has a recovery state we can enter
                            call->state = call->recover;
                            call->recover = 0;
                            break;
                        }else{
                            free_sems(p, call->stack);
                            p->callslen--;
                        }
                    }
                }

                // parsed everything!
                *ok = true;
            }
        """).strip('\n'), file=f)

        # declare functions
        for _, expr in g.sorted_expressions():
            self.declare_fn(expr, f)
        print("", file=f)

        print("static const char *token_name(int t){", file=f)
        print("    switch(t){", file=f)
        for name, _ in g.sorted_tokens():
            print("        case " + name + ": return \"" +name+ "\";", file=f)
        print("        default: return \"unknown\";", file=f)
        print("    }", file=f)
        print("}", file=f)
        print("", file=f)

        # get function names
        print("static const char *_fn_name(parse_fn_t fn){", file=f)
        for _, expr in g.sorted_expressions():
            print(
                "    if(fn == " + self.parse_fn(expr) + ")"
                + " return \""+self.parse_fn(expr)+"\";"
            , file=f)
        print("    return \"unknown\";", file=f)
        print("}", file=f)
        print("", file=f)

        # define functions
        for _, expr in g.sorted_expressions():
            self.define_fn(expr, f)

        print(textwrap.dedent("""
            void parse_""" + self.root + """(imf_parser_t *p, imf_token_t token, sem_t sem, bool *ok){
                do_parse(p, _parse_""" + self.root + """, token, sem, ok);
            }
        """).strip('\n'), file=f)

        self.print_code(postcode, "", f)


class Python:
    """
    An object with all the methods for generating Python code.
    """
    code_tag = "py"

    def __init__(
        self,
        *,
        # required keywords
        root,
        # optional keywords
        prefix="",
        span_fn=None,
        user_args=(),
    ):
        self.root = root
        self.prefix = prefix
        self.span_fn = span_fn
        self.u = user_args.split(",") if isinstance(user_args, str) else user_args

    def print_code(self, code, i, f):
        if code is None:
            return
        for c in code:
            if c.tag in (None, self.code_tag):
                print(i + "# USER CODE", file=f)
                break
        for c in code:
            if c.tag in (None, self.code_tag):
                text = re.sub("\\$\\$", "(out.val)", c.text)
                text = re.sub("\\$([a-zA-Z_][a-zA-Z0-9_]*)", "(yy_\\1.val)", text)
                text = re.sub("@@", "(out.loc)", text)
                text = re.sub("@([a-zA-Z_][a-zA-Z0-9_]*)", "(yy_\\1.loc)", text)
                print(textwrap.indent(text, i), file=f)

    def syntax_error(self, loc, i, f):
        print(i + "raise " + self.prefix + "SyntaxError(" + loc + ")", file=f)

    def fn_name(self, title):
        if not self.prefix:
            return "_parse_" + title
        return "_" + self.prefix + "_parse_" + title

    def token_list(self, tokens):
        assert tokens
        tokens = sorted(t for t in tokens if t is not None)
        if len(tokens) > 1:
            return "(" + ", ".join(tokens) + ")"
        else:
            return "(" + tokens[0] + ",)"

    def user_args(self, u, after=False):
        if not u:
            return ""
        if after:
            return ", ".join(u) + ","
        return ", " + ", ".join(u)

    def if_token_in(self, tokens, i, f, pos=0):
        token_list = self.token_list(tokens)
        lead = "if" if pos == 0 else "elif"
        print(i + lead + " (yield from tokens.peek()) in " + token_list + ":", file=f)

    def if_token_not_in(self, tokens, i, f, pos=0):
        token_list = self.token_list(tokens)
        lead = "if" if pos == 0 else "elif"
        print(i + lead + " (yield from tokens.peek()) not in " + token_list + ":", file=f)

    def while_token_in(self, tokens, i, f):
        token_list = self.token_list(tokens)
        print(i + "while (yield from tokens.peek()) in " + token_list + ":", file=f)

    def while_token_not_in(self, tokens, i, f):
        token_list = self.token_list(tokens)
        print(i + "while (yield from tokens.peek()) not in " + token_list + ":", file=f)

    def gen(self, obj, tag, u, i, f):
        try:
            self._gen(obj, tag, u, i, f)
        except Exception as e:
            raise RuntimeError("generating for " + obj.title) from e

    def _gen(self, obj, tag, u, i, f):
        if isinstance(obj, Token):
            print(i + "if (yield from tokens.peek()) != " + obj.title + ":", file=f)
            self.syntax_error("(yield from next(tokens)).loc", i + "    ", f)
            if tag is not None:
                print(i + "yy_" + tag + " = (yield from next(tokens))", file=f)
            else:
                print(i + "yield from next(tokens)", file=f)
            return

        if isinstance(obj, Maybe):
            self.if_token_in(obj.get_first(), i, f)
            self.gen(obj.seq, None, u, i+"    ", f)
            return

        if isinstance(obj, ZeroOrMore):
            self.while_token_in(obj.get_first(), i, f)
            self.gen(obj.seq, None, u, i+"    ", f)
            return

        if isinstance(obj, Branches):
            for n, seq in enumerate(obj.branches):
                self.if_token_in(seq.get_first(), i, f, pos=n)
                self.gen(seq, None, u, i+"    ", f)
            print(i + "else:", file=f)
            self.syntax_error("(yield from next(tokens)).loc", i + "    ", f)
            return

        if isinstance(obj, Recovery):
            print(i + "try:", file=f)
            self.gen(obj.seq, None, u, i+"    ", f)
            self.if_token_not_in(obj.after, i+"    ", f)
            print(i + "        raise " + self.prefix + "SyntaxError()", file=f)
            print(i + "except " + self.prefix + "SyntaxError:", file=f)
            self.while_token_not_in(obj.after, i+"    ", f)
            print(i + "        yield from next(tokens)", file=f)
            self.print_code(obj.code, i+"    ", f)
            return

        if isinstance(obj, Sequence):
            self.print_code(obj.precode, i, f)
            for term, tag, code in obj.terms:
                self.gen(term, tag, u, i, f)
                self.print_code(code, i, f)
            return

        if isinstance(obj, Expression):
            if tag is not None:
                print(
                    i + "yy_" + tag + " = yield from "
                    + self.fn_name(obj.title) +
                    "(tokens" + self.user_args(u) + ")"
                , file=f)
            else:
                print(
                    i + "yield from from " + self.fn_name(obj.title) +
                    "(tokens" + self.user_args(u) + ")"
                , file=f)
            return

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def define_fn(self, expr, u, f):
        print(
            "def " + self.fn_name(expr.title) +
            "(tokens" + self.user_args(u) + "):"
        , file=f)
        print("    out = " + self.prefix + "Expression()", file=f)
        self.gen(expr.seq, None, u, "    ", f)
        print("    return out", file=f)

    def gen_file(
        self, g, *, precode=None, postcode=None, file=None
    ):
        f = file
        u = self.u
        self.print_code(precode, "", f)
        print(textwrap.dedent(r"""
            class """.lstrip("\n") + self.prefix + """SyntaxError(Exception):
                def __init__(self, loc=None):
                    self.loc = loc

                def __repr__(self):
                    return str(self.loc)

            class """ + self.prefix + '''Expression:
                def __init__(self, val=None, loc=None):
                    self.val = val
                    self.loc = loc

                def __repr__(self):
                    return "''' + self.prefix + """Expression(val=%s)"%(str(self.val))

            class """ + self.prefix + '''Token:
                def __init__(self, typ, val=None, loc=None):
                    self.type = typ
                    self.val = val
                    self.loc = loc

                def __repr__(self):
                    return "''' + self.prefix + '''Token(%s, %s)"%(str(self.type), str(self.val))

            class ''' + self.prefix + """TokenStream:
                def __init__(self):
                    self.first = None

                def peek(self):
                    if self.first is None:
                        self.first = yield
                    return self.first.type

                def __next__(self):
                    if self.first:
                        out = self.first
                        self.first = None
                        return out
                    t = yield
                    return t
        """), file=f)

        if self.span_fn is not None:
            print(textwrap.dedent("""
                def _""".lstrip("\n") + self.prefix + """_span(loc, token_loc):
                    if loc is None:
                        return token_loc
                    return """ + self.span_fn + """(loc, token_loc)
            """), file=f)

        for i, (name, _) in enumerate(g.sorted_tokens()):
            print(name + " = " + str(i), file=f)
        print("", file=f)

        for name, expr in g.sorted_expressions():
            self.define_fn(expr, u, f)
            print("", file=f)

        print("class " + self.prefix + "Parser:", file=f)
        print("    def __init__(", file=f)
        print("        self" + self.user_args(u) + ", fn=" + self.fn_name(self.root) + ", repeat=False", file=f)
        print("    ):", file=f)
        print(textwrap.indent(textwrap.dedent(r"""
                    assert fn is not None, "fn is a required keyword argument"
                    self.fn = fn
                    self.tokens = """ + self.prefix + """TokenStream()
                    self.out = None
                    self.user_args = (""" + self.user_args(u, after=True) + """)
                    self.gen = self.mkgen()
                    self.repeat = repeat
                    self.done = False

                def mkgen(self):
                    def _gen():
                        self.out = yield from self.fn(self.tokens, *self.user_args)
                    gen = _gen()
                    next(gen)
                    return gen

                def feed(self, t):
                    if self.done:
                        raise RuntimeError("parser is already done!")
                    try:
                        return self.gen.send(t)
                    except StopIteration:
                        self.gen = None
                        out = self.out
                        self.out = None
                        self.gen = self.mkgen()
                        if not self.repeat:
                            self.done = True
                        return out
        """.strip("\n")), "    "), end="", file=f)

        self.print_code(postcode, "", f)


### Meta Parser ###

class ParsedName:
    def __init__(self, name, tag):
        self.name = name
        self.tag = tag

    def process(self, g, e):
        e.match(g.get_name(self.name), self.tag)

class ParsedSnippet:
    def __init__(self, text, tag=None):
        self.text = text
        self.tag = tag

    def process(self, g, e):
        e.exec(self.text, self.tag)

class ParsedBranches:
    def __init__(self):
        self.branches = []

    def process(self, g, e):
        # Collapse single-branch expressions.
        if len(self.branches) == 1:
            self.branches[0].process(g, e)
            return

        with e.branches() as b:
            for parsed_branch in self.branches:
                with b.branch():
                    parsed_branch.process(g, e)

class ParsedRecovery:
    def __init__(self, sub):
        self.sub = sub
        self.code = []

    def process(self, g, e):
        with e.recovery() as r:
            self.sub.process(g, e)
            for c in self.code:
                r.add_recovery_code(c.text, c.tag)

class ParsedMultiplier:
    def __init__(self, term, multiplier):
        self.term = term
        self.multiplier = multiplier

    def process(self, g, e):
        if self.multiplier == "1":
            self.term.process(g, e)
        elif self.multiplier == "?":
            with e.maybe():
                self.term.process(g, e)
        elif self.multiplier == "*":
            with e.zero_or_more():
                self.term.process(g, e)
        else:
            raise RuntimeError("unrecognized multiplier: " + str(self.multiplier))

class ParsedSequence:
    def __init__(self):
        self.elems = []

    def process(self, g, e):
        for elem in self.elems:
            elem.process(g, e)

class ParsedGenerator:
    def __init__(self, name):
        self.name = name

class ParsedKwarg:
    def __init__(self, key, value, tag=None):
        self.key = key
        self.value = value
        self.tag = tag

class ParsedDestructor:
    def __init__(self, typ, snippet, tag=None):
        self.type = typ
        self.snippet = snippet
        self.tag = tag

class ParsedDoc:
    def __init__(self):
        self.precode = []
        self.defs = []
        self.postcode = []
        # directive handling
        self.generator = None
        self.kwargs = []
        self.destructors = []

    def add_conf(self, conf):
        if isinstance(conf, ParsedGenerator):
            if self.generator is not None:
                raise RuntimeError("received multiple %generator directives")
            self.generator = conf.name
            return

        if isinstance(conf, ParsedKwarg):
            self.kwargs.append(conf)
            return

        if isinstance(conf, ParsedDestructor):
            self.destructors.append(conf)
            return

        # definitions
        self.defs.append(conf)


    def build_grammar(self):
        g = Grammar()

        # first declare all tokens and expressions
        for name, val in self.defs:
            if val is None:
                _ = g.token(name.name, name.tag)
            else:
                e = g.expr(name.name)
                e.type = name.tag

        # then define all the expressions
        for name, val in self.defs:
            if val is None:
                continue
            e = g.get_name(name.name)
            val.process(g, e)

        g.check()

        return self.precode, g, self.postcode

# BEGIN GENERATED CODE
# END GENERATED CODE

class TextLocation:
    def __init__(self, line, char):
        self.line = line
        self.char = char

    def __repr__(self):
        return "(line=%d, char=%d)"%(self.line, self.char)

class TextSpan:
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __repr__(self):
        return "(start=%s, end=%s)"%(self.start, self.end)

def text_span(loc, token_loc):
    return TextSpan(loc.start, token_loc.end)

class CharStream:
    def __init__(self):
        self._lookahead = []
        # We start in line 1, char 1.
        self.line = 1
        # char is a position within this line
        self.char = 1

    def peek(self, index=0):
        while len(self._lookahead) <= index:
            temp = yield
            self._lookahead.append(temp)
        return self._lookahead[index]

    def __next__(self):
        if self._lookahead:
            c = self._lookahead.pop(0)
        else:
            c = yield
        # newlines are considered the end of this current line
        loc = TextLocation(self.line, self.char)
        if c == '\n':
            self.line += 1
            self.char = 1
        else:
            self.char += 1
        return c, loc

    def empty(self):
        return not self._lookahead

class Discard:
    pass

class Tokenizer:
    def __init__(self):
        self.cs = CharStream()
        self.out = []

        def _gen():
            yield from self._tokenize_many()

        self.gen = _gen()
        next(self.gen)

    def feed(self, c):
        if self.gen is None:
            raise RuntimeError("tokenizer is already done!")

        if c is None:
            try:
                # This should finish the generator.
                self.gen.send(None)
                raise RuntimeError("tokenizer failed to exit after None")
            except StopIteration:
                return self.out

        self.gen.send(c)
        out = self.out
        self.out = []
        return out

    def _tokenize_many(self):
        while True:
            token = yield from self._tokenize()
            if token is None:
                break
            if isinstance(token, Discard):
                continue
            self.out.append(token)

    def _tokenize(self):
        c = yield from self.cs.peek()

        if c is None:
            return None

        if c in " \r\n\t":
            return (yield from self._ignore_whitespace())

        if c in "#":
            return (yield from self._ignore_comment())

        singles = {
            "*": ASTERISK,
            ":": COLON,
            "=": EQ,
            "<": LANGLE,
            "[": LBRACKET,
            "(": LPAREN,
            "%": PERCENT,
            "|": PIPE,
            "?": QUESTION,
            ">": RANGLE,
            "]": RBRACKET,
            ")": RPAREN,
            ";": SEMI,
        }
        if c in singles:
            _, loc = yield from next(self.cs)
            return MetaToken(singles[c], c, TextSpan(loc, loc))

        if c == "{":
            return (yield from self._tokenize_code())

        if c in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_":
            return (yield from self._tokenize_text())

        raise RuntimeError("invalid character in tokenizer" + str(ord(c)))

    def _ignore_whitespace(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c not in " \r\n\t":
                break
            yield from next(self.cs)
        return Discard()

    def _ignore_comment(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c in "\r\n":
                break
            yield from next(self.cs)
        return Discard()

    def _tokenize_code(self):
        # Some number of leading '{' chars which we discard...
        count = 0
        start = None
        while True:
            c = yield from self.cs.peek()
            if c != "{":
                break
            _, loc = yield from next(self.cs)
            if start is None:
                start = loc
            count += 1

        # ... and code body, which we keep...
        text = ""
        while True:
            c = yield from self.cs.peek()
            if c is None:
                raise RuntimeError("unterminated code block:\n" + text)
            if c != "}":
                c, loc = yield from next(self.cs)
                text += c
                continue

            # ... and a matching number of closing '}' chars which we discard.
            matched = 1
            while matched < count:
                c = yield from self.cs.peek(matched)
                if c != "}":
                    break
                matched += 1

            if matched != count:
                # oops, these were actually part of the code body
                for _ in range(matched):
                    c, _ = yield from next(self.cs)
                    text += c
                continue

            # discard these, they're not part of the code body
            for _ in range(matched):
                _, loc = yield from next(self.cs)
            return MetaToken(CODE, text, TextSpan(start, loc))

    def _tokenize_text(self):
        text = ""
        start = None
        loc = None
        while True:
            c = yield from self.cs.peek()
            if c is None:
                break
            if c not in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_0123456789":
                break
            c, loc = yield from next(self.cs)
            text += c
            if start is None:
                start = loc
        keywords = {
            "generator": GENERATOR,
            "kwarg": KWARG,
            "destructor": DESTRUCTOR,
        }
        typ = keywords.get(text, TEXT)
        return MetaToken(typ, text, TextSpan(start, loc))

    def iter(self, chars):
        def _chars(self):
            yield from chars
            yield None

        for c in chars:
            tokens = self.feed(c)
            if tokens is None:
                break
            yield from tokens
        yield MetaToken(EOF, "", None)


@contextlib.contextmanager
def read_template(template, tag="GENERATED CODE", file=None):
    if template is None:
        yield 0
        return

    with open(template) as f:
        lines = iter(enumerate(f))
        start_offset = 0
        # passtru pre-tag lines
        for i, line in lines:
            print(line, end="", file=file)
            if "BEGIN " + tag in line:
                start_offset = i + 1
                break
        else:
            print('did not find "BEGIN ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # discard inter-tag lines
        for _, line in lines:
            if "END " in line:
                yield  # generated code gets printed here
                print(line, end="", file=file)
                break
        else:
            print('did not find "END ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # passtru post-tag lines
        for _, line in lines:
            print(line, end="", file=file)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("input", nargs="?")
    parser.add_argument("-t", "--template")
    parser.add_argument("-T", "--template-tag", default="GENERATED CODE")
    parser.add_argument("-o", "--output")
    parser.add_argument("-c", "--check", action="store_true")
    parser.add_argument("-g", "--generator")
    parser.add_argument("kwarg", nargs="*")
    args = parser.parse_args()

    assert not args.check or args.output, "--check requires --output"

    if args.input is None:
        text = sys.stdin.read()
    else:
        with open(args.input) as f:
            text = f.read()

    out = io.StringIO()
    with read_template(args.template, args.template_tag, out) as start_offset:
        parser = MetaParser()

        try:
            for t in Tokenizer().iter(text):
                # the last time this is called it should return non-None
                parsed_doc = parser.feed(t)
        except MetaSyntaxError as e:
            line = start_offset + e.loc.start.line
            char = e.loc.start.char
            print("syntax error at line=%d, char=%d:"%(line, char), file=sys.stderr)
            print(text.splitlines()[e.loc.start.line - 1].strip(), file=sys.stderr)
            print(" "*(char - 1) + "^", file=sys.stderr)
            exit(1)

        assert parsed_doc, "didn't get a doc!"

        precode, g, postcode = parsed_doc.val.build_grammar()

        # TODO: support importing a user-defined generator at runtime
        generator_name = parsed_doc.val.generator
        if args.generator:
            # cli --generator overrides %generator directive
            generator_name = args.generator
        assert generator_name is not None, "neither --generator nor %generator was found"
        generator_cls = {"py": Python, "c": C}[generator_name]

        kwargs = {}
        # extract %kwarg directives
        for kwarg in parsed_doc.val.kwargs:
            if kwarg.tag not in (None, generator_cls.code_tag):
                continue
            assert kwarg.key not in kwargs, "got multiple %%kwarg for key '%s'"%kwarg.key
            kwargs[kwarg.key] = kwarg.value

        # cli kwargs override %kwarg directives
        for kwarg in args.kwarg:
            assert "=" in kwarg, "kwarg '%s' is not of the form KEY=VALUE"%kwarg
            k, v = kwarg.split("=", 1)
            kwargs[k] = v

        generator = generator_cls(**kwargs)

        generator.gen_file(g, precode=precode, postcode=postcode, file=out)

    if args.check:
        with open(args.output) as f:
            old = f.read()
        if out.getvalue() == old:
            sys.exit(0)
        print(args.output + " would be modified", file=sys.stderr)
        sys.exit(1)

    if args.output is not None:
        with open(args.output, "w") as f:
            f.write(out.getvalue())
    else:
        print(out.getvalue(), end="", flush=True)
