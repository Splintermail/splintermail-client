import abc
import argparse
import contextlib
import io
import re
import sys
import textwrap


class FirstFirst(Exception):
    pass

class FirstFollow(Exception):
    pass

class InfiniteRecursion(Exception):
    pass

def add_to_prev(prev, name, call):
    if prev is None:
        prev = []
    if name in prev:
        raise InfiniteRecursion(
            "circular " + call + "() detected in " + name + ": " + str(prev)
        )
    return list(prev) + [name]

def cacheable(fn):
    cache = None
    prop = "_cached_" + fn.__name__
    def _fn(self, *args, **kwargs):
        if not hasattr(self, prop):
            setattr(self, prop, fn(self, *args, **kwargs))
        return getattr(self, prop)
    return _fn


class Parsable(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def title(self):
        pass

    @abc.abstractmethod
    def get_first(self, prev=None):
        pass

    @abc.abstractmethod
    def get_disallowed_after(self, prev=None):
        pass

    @abc.abstractmethod
    def check(self):
        pass


class Token(Parsable):
    def __init__(self, name, typ=None):
        self.name = name
        self.type = typ

    @property
    def title(self):
        return self.name

    @cacheable
    def get_first(self, prev=None):
        return {self.name}

    @cacheable
    def get_disallowed_after(self, prev=None):
        return set()

    @cacheable
    def check(self):
        pass


class Snippet:
    def __init__(self, text, tag=None):
        self.text = text
        self.tag = tag


class Maybe(Parsable):
    def __init__(self, name):
        self.name = name
        self.seq = Sequence(self.name + ".seq")

    @property
    def title(self):
        return "maybe(" + self.name + ")"

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        # TODO: is this necessary at all?
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        return first.union({None})

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        assert None not in disallowed, "invalid .maybe() on maybe-empty expression"
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first()
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        # always include None, since this term may be missing
        return disallowed.union(first).union({None})

    @cacheable
    def check(self):
        self.seq.check()



class ZeroOrMore(Parsable):
    def __init__(self, name):
        self.name = name
        self.seq = Sequence(self.name + '.seq')

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    @property
    def title(self):
        return "zero_or_more(" + self.name + ")"

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        assert None not in first, "invalid .zero_or_more() on maybe-empty expression: " + self.seq.title
        return first.union({None})

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        assert None not in disallowed, "invalid .maybe() on maybe-empty expression"
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first(prev)
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        # always include None, since this term may be missing
        return disallowed.union(first).union({None})

    @cacheable
    def check(self):
        self.seq.check()


class Branches(Parsable):
    def __init__(self, name):
        self.name = name
        self.branches = []
        self._branch = None

    @property
    def title(self):
        return self.name

    def add_term(self, term, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_term(term, tag)

    def add_code(self, code, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_code(code, tag)

    @contextlib.contextmanager
    def branch(self):
        assert self._branch is None, "can't nest calls to branch"
        self._branch = Sequence(self.name + '[' + str(len(self.branches)) + ']')
        yield
        self.branches.append(self._branch)
        self._branch = None

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for branch in self.branches:
            first = first.union(branch.get_first(list(prev)))

        return frozenset(first)

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        prev = list(prev) + [self.name]
        disallowed = set()
        for branch in self.branches:
            disallowed = disallowed.union(branch.get_disallowed_after(list(prev)))

        return frozenset(disallowed)

    @cacheable
    def check(self):
        first = set()
        for branch in self.branches:
            branch.check()

            # no possibly-empty branches
            branch_first = branch.get_first()
            assert None not in branch_first

            # Ensure that no branches begin with the same tokens
            conflicts = first.intersection(branch_first)
            assert not conflicts, "FIRST/FIRST conflicts for tokens " + str(conflicts)

            first = first.union(branch_first)


class Recovery(Parsable):
    def __init__(self, name, code=None, tag=None):
        self.name = name
        # allow defining the recovery code upfront
        self.code = [] if code is None else [Snippet(code, tag)]
        self.seq = Sequence(self.name + '.seq')
        self.after = None

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    def add_recovery_code(self, code, tag=None):
        self.code.append(Snippet(code, tag))

    def set_after(self, after):
        assert self.after is None, "can't call set_after() twice!"
        self.after = after

    @property
    def title(self):
        return "recovery(" + self.name + ")"

    @cacheable
    def get_first(self, prev=None):
        return self.seq.get_first(prev)

    @cacheable
    def get_disallowed_after(self, prev):
        return self.seq.get_disallowed_after(prev)

    @cacheable
    def check(self):
        self.seq.check()


class Sequence(Parsable):
    """
    A sequence of terms which must be matched consecutively.
    """
    def __init__(self, name):
        self.name = name
        self.scopes = [self]
        self.precode = []
        # tuples of (Parsable, tag=None, code=[])
        self.terms = []

    def add_term(self, term, tag=None):
        self.terms.append((term, tag, []))

    def add_code(self, code, tag=None):
        if not self.terms:
            self.precode.append(Snippet(code, tag))
        else:
            self.terms[-1][2].append(Snippet(code, tag))

    @property
    def title(self):
        return self.name

    def get_first_ex(self, start, prev):
        """
        Get the list of all tokens that this expression could start with.

        Ex: at runtime, when are processing a .maybe(), we need to be able to
        look ahead one token and know if that token can be the first token
        of this expression.
        """
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for term, _, _ in self.terms[start:]:
            term_first = term.get_first(list(prev))
            # detect conflicts in .check(), not here.
            first = first.union(term_first)
            # if None is not in the set, we have our answer.
            if None not in first:
                break
            # if None is in the set, check the next term too.
            first.remove(None)
        else:
            # restore the None from the final term_first
            first.add(None)

        return frozenset(first)

    @cacheable
    def get_first(self, prev=None):
        return self.get_first_ex(0, prev)

    @cacheable
    def get_disallowed_after(self, prev=None):
        """
        Get the list of all tokens which would cause ambiguities after this
        expression.

        Ex. a maybe(EOL) term could not be followed by a match(EOL) term; it
        would be ambiguous if an EOL matched to the maybe() or the match().
        """
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = set()
        for term, _, _ in reversed(self.terms):
            term_disallowed = term.get_disallowed_after(list(prev))
            disallowed = disallowed.union(term_disallowed)
            # if None is not in this term's disallowed, we have our answer
            if None not in disallowed:
                break
            # if None is in the set, check the next term too
            disallowed.remove(None)
        else:
            # restore the None from the final term_disallowed
            disallowed.add(None)

        return frozenset(disallowed)

    @cacheable
    def check(self):
        """
        At generation time, we need to ensure that for every Term in the
        expression, there is never a point where the lookahead token could
        possibly match the next token.
        """
        prev = {self.name}

        for term, _, _ in self.terms:
            # don't allow recursion
            if not isinstance(term, Expression):
                term.check()

        disallowed = set()
        for i, (term, _, _) in enumerate(self.terms):
            term_first = term.get_first(list(prev))
            # Ignore None when checking for conflicts
            disallowed = disallowed.difference({None})
            conflicts = disallowed.intersection(term_first)
            if conflicts:
                raise FirstFollow(
                    "FIRST/FOLLOW conflicts:" + str(conflicts) + "\n"
                    "found while checking expression " + self.title + "\n"
                    "at least one token that starts " + term.title + "\n"
                    "is disallowed by that point."
                )
            term_disallowed = term.get_disallowed_after(list(prev))
            if None in term_disallowed:
                # disallowed should grow
                disallowed = disallowed.union(term_disallowed)
            else:
                # disallowed is reset
                disallowed = term_disallowed

            # Special case: check Recovery terms and call their set_after().
            if isinstance(term, Recovery):
                after_recovery = self.get_first_ex(i+1, None)
                if None in after_recovery:
                    raise ValueError(
                        "a Recovery must be followed by a never-empty match "
                        "within the same Sequence"
                    )
                term.set_after(after_recovery)


class Expression(Parsable):
    """
    Each Expression will generate one function in the parser.
    """
    def __init__(self, name, typ=None):
        self.name = name
        self.type = typ
        self.seq = Sequence(self.name + ".seq")
        self.scopes = [self.seq]
        self.nmaybe = 0
        self.nzom = 0
        self.nbranches = 0
        self.nrec = 0

    # API for defining the grammar.

    def __call__(self, fn):
        """Provide a definition for a fowards-declared expression"""
        assert fn.__name__ == self.name, fn.__name__ + " != " + self.name
        fn(self)
        return self

    def match(self, term, tag=None):
        self.scopes[-1].add_term(term, tag)
        return term

    @contextlib.contextmanager
    def maybe(self):
        term = Maybe(self.name + '.maybe' + str(self.nmaybe))
        self.nmaybe += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def zero_or_more(self):
        term = ZeroOrMore(self.name + '.zom' + str(self.nzom))
        self.nzom += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def branches(self):
        term = Branches(self.name + '.br' + str(self.nbranches))
        self.nbranches += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def recovery(self, code=None, tag=None):
        term = Recovery(self.name + ".recovery" + str(self.nrec), code, tag)
        self.nrec += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    def exec(self, code, tag=None):
        self.scopes[-1].add_code(code, tag)

    # Non-user-facing methods.

    @property
    def title(self):
        return self.name

    def get_first(self, prev=None):
        # Refer to our Sequence.
        return self.seq.get_first(prev)

    def get_disallowed_after(self, prev=None):
        # Refer to our Sequence.
        return self.seq.get_disallowed_after(prev)

    def check(self):
        return self.seq.check()


class Grammar:
    def __init__(self):
        self.exprs = {}

    def get_name(self, name):
        """
        Creates a name if it doesn't yet exist.  Mostly for the meta parser.
        """
        if name in self.exprs:
            return self.exprs[name]
        if name[0].upper() == name[0]:
            # starts uppercase, we guess it's a token
            e = self.token(name)
        else:
            # starts lowercase, we guess it's a expression
            e = self.expr(name)
        return e

    def expr(self, val):
        if isinstance(val, str):
            # Forward declaration case:
            #     @g.expr
            #     def my_expr(e):
            #         e.match(...)
            fn = None
            name = val
        else:
            # Declaration/definition case.
            #     my_expr = g.expr("my_expr")
            #
            #     ...
            #
            #     @my_expr
            #     def my_expr(e):
            #         e.match(...)
            fn = val
            name = fn.__name__

        assert name not in self.exprs
        e = Expression(name)
        self.exprs[name] = e
        if fn is not None:
            fn(e)
        return e

    def token(self, name, typ=None):
        assert name not in self.exprs
        e = Token(name, typ)
        self.exprs[name] = e
        return e

    def sorted_tokens(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Token):
                yield name, expr

    def sorted_expressions(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Expression):
                yield name, expr

    def check(self):
        for _, expr in self.sorted_expressions():
            expr.check()


class Python:
    """
    An object with all the methods for generating Python code.
    """
    def __init__(self, *, prefix="", code_tag="py"):
        self.prefix = prefix
        self.code_tag = code_tag

    def print_code(self, code, i, f):
        if code is None:
            return
        for c in code:
            if c.tag in (None, self.code_tag):
                print(i + "# USER CODE", file=f)
                break
        for c in code:
            if c.tag in (None, self.code_tag):
                text = re.sub("\\$\\$", "out", c.text)
                text = re.sub("\\$", "yy_", text)
                print(textwrap.indent(textwrap.dedent(text), i), file=f)

    def fn_name(self, title):
        if not self.prefix:
            return "_parse_" + title
        return "_" + self.prefix + "_parse_" + title

    def token_list(self, tokens):
        assert tokens
        tokens = sorted(t for t in tokens if t is not None)
        if len(tokens) > 1:
            return "(" + ", ".join(tokens) + ")"
        else:
            return "(" + tokens[0] + ",)"

    def user_args(self, u, after=False):
        if not u:
            return ""
        if after:
            return ", ".join(u) + ","
        return ", " + ", ".join(u)

    def if_token_in(self, tokens, i, f, pos=0):
        token_list = self.token_list(tokens)
        lead = "if" if pos == 0 else "elif"
        print(i + lead + " (yield from tokens.peek()) in " + token_list + ":", file=f)

    def if_token_not_in(self, tokens, i, f, pos=0):
        token_list = self.token_list(tokens)
        lead = "if" if pos == 0 else "elif"
        print(i + lead + " (yield from tokens.peek()) not in " + token_list + ":", file=f)

    def while_token_in(self, tokens, i, f):
        token_list = self.token_list(tokens)
        print(i + "while (yield from tokens.peek()) in " + token_list + ":", file=f)

    def while_token_not_in(self, tokens, i, f):
        token_list = self.token_list(tokens)
        print(i + "while (yield from tokens.peek()) not in " + token_list + ":", file=f)

    def gen(self, obj, tag, u, i, f):
        try:
            self._gen(obj, tag, u, i, f)
        except Exception as e:
            raise RuntimeError("generating for " + obj.title) from e

    def _gen(self, obj, tag, u, i, f):
        if isinstance(obj, Token):
            print(i + "if (yield from tokens.peek()) != " + obj.title + ":", file=f)
            print(i + "    raise SyntaxError()", file=f)
            if tag is not None:
                print(i + "yy_" + tag + " = (yield from next(tokens)).val", file=f)
            else:
                print(i + "yield from next(tokens)", file=f)
            return

        if isinstance(obj, Maybe):
            self.if_token_in(obj.get_first(), i, f)
            self.gen(obj.seq, None, u, i+"    ", f)
            return

        if isinstance(obj, ZeroOrMore):
            self.while_token_in(obj.get_first(), i, f)
            self.gen(obj.seq, None, u, i+"    ", f)
            return

        if isinstance(obj, Branches):
            for n, seq in enumerate(obj.branches):
                self.if_token_in(seq.get_first(), i, f, pos=n)
                self.gen(seq, None, u, i+"    ", f)
            print(i + "else:", file=f)
            print(i + "    raise SyntaxError()", file=f)
            return

        if isinstance(obj, Recovery):
            print(i + "try:", file=f)
            self.gen(obj.seq, None, u, i+"    ", f)
            self.if_token_not_in(obj.after, i+"    ", f)
            print(i + "        raise SyntaxError()", file=f)
            print(i + "except SyntaxError:", file=f)
            self.while_token_not_in(obj.after, i+"    ", f)
            print(i + "        yield from next(tokens)", file=f)
            self.print_code(obj.code, i+"    ", f)
            return

        if isinstance(obj, Sequence):
            self.print_code(obj.precode, i, f)
            for term, tag, code in obj.terms:
                self.gen(term, tag, u, i, f)
                self.print_code(code, i, f)
            return

        if isinstance(obj, Expression):
            if tag is not None:
                print(
                    i + "yy_" + tag + " = yield from "
                    + self.fn_name(obj.title) +
                    "(tokens" + self.user_args(u) + ")"
                , file=f)
            else:
                print(
                    i + "yield from from " + self.fn_name(obj.title) +
                    "(tokens" + self.user_args(u) + ")"
                , file=f)
            return

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def define_fn(self, expr, u, f):
        print(
            "def " + self.fn_name(expr.title) +
            "(tokens" + self.user_args(u) + "):"
        , file=f)
        print("    out = None", file=f)
        self.gen(expr.seq, None, u, "    ", f)
        print("    return out", file=f)

    def gen_file(self, g, default, *, user_args=(), file=None):
        u = user_args
        f = file
        print(textwrap.dedent(r"""
            class """.lstrip("\n") + self.prefix + """SyntaxError(Exception):
                pass

            class """ + self.prefix + '''Token:
                def __init__(self, typ, val=None):
                    self.type = typ
                    self.val = val

                def __str__(self):
                    return "''' + self.prefix + '''Token(" + str(self.type) + ", " + str(self.val) + ")"

            class ''' + self.prefix + """TokenStream:
                def __init__(self):
                    self.first = None

                def peek(self):
                    if self.first is None:
                        self.first = yield
                    return self.first.type

                def __next__(self):
                    if self.first:
                        out = self.first
                        self.first = None
                        return out
                    t = yield
                    return t
        """), file=f)

        for i, (name, _) in enumerate(g.sorted_tokens()):
            print(name + " = " + str(i), file=f)
        print("", file=f)

        for name, expr in g.sorted_expressions():
            self.define_fn(expr, u, f)
            print("", file=f)

        print("class " + self.prefix + "Parser:", file=f)
        print("    def __init__(", file=f)
        print("        self" + self.user_args(u) + ", fn=" + self.fn_name(default) + ", repeat=False", file=f)
        print("    ):", file=f)
        print(textwrap.indent(textwrap.dedent(r"""
                    assert fn is not None, "fn is a required keyword argument"
                    self.fn = fn
                    self.tokens = """ + self.prefix + """TokenStream()
                    self.out = None
                    self.user_args = (""" + self.user_args(u, after=True) + """)
                    self.gen = self.mkgen()
                    self.repeat = repeat
                    self.done = False

                def mkgen(self):
                    def _gen():
                        self.out = yield from self.fn(self.tokens, *self.user_args)
                    gen = _gen()
                    next(gen)
                    return gen

                def feed(self, t):
                    if self.done:
                        raise RuntimeError("parser is already done!")
                    try:
                        return self.gen.send(t)
                    except StopIteration:
                        self.gen = None
                        out = self.out
                        self.out = None
                        self.gen = self.mkgen()
                        if not self.repeat:
                            self.done = True
                        return out
        """.strip("\n")), "    "), end="", file=f)


class C:
    """
    An object with all the methods for generating C code.
    """

    class Variables:
        def __init__(self):
            # a list of lists of available variables
            self.scopes = [[]]

        def new_scope(self):
            self.scopes.append([])

        def define(self, tag, typ, pos):
            for scope in self.scopes:
                assert tag not in scope, "tag " + tag + " shadows another tag"
            print("    #define $" + tag + " (p->semstack[call->stack + " + str(pos) + "].val." + typ + ")")
            self.scopes[-1].append(tag)

        def pop_scope(self):
            popped = self.scopes.pop()
            for tag in popped:
                print("    #undef $" + tag)

    def __init__(self, *, code_tag="c"):
        self.code_tag = code_tag

    def print_code(self, code):
        if code is None:
            return
        for c in code:
            if c.tag in (None, "c"):
                print("    // USER CODE")
                break
        for c in code:
            if c.tag in (None, "c"):
                print(textwrap.indent(textwrap.dedent(c.text), "    "))

    def token_check(self, first):
        t = sorted(t for t in first if t is not None)
        return "token_in(*token, " + str(len(t)) + ", " + ", ".join(t) + ")"

    def mask_set(self, tokens):
        t = sorted(t for t in tokens if t is not None)
        return "mask_set(p->mask, " + str(len(t)) + ", " + ", ".join(t) + ")"

    def stackpersist(self, obj):
        """Return the persistent created during a match to obj."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Expression):
            return 1
        return 0

    def stackmax(self, obj):
        """Return the maximum stack size during a match to obj."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Expression):
            return 1
        if isinstance(obj, Maybe):
            return self.stackmax(obj.seq)
        if isinstance(obj, ZeroOrMore):
            return self.stackmax(obj.seq)
        if isinstance(obj, Branches):
            return max(self.stackmax(b) for b in obj.branches)
        if isinstance(obj, Recovery):
            return self.stackmax(obj.seq)
        if isinstance(obj, Sequence):
            persist = 0
            peak = 0
            for t, _, _ in obj.terms:
                peak = max(peak, persist + self.stackmax(t))
                persist += self.stackpersist(t)
            return peak
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def nstates(self, obj):
        """Count how many states we need for every entry."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Maybe):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, ZeroOrMore):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, Branches):
            return sum(self.nstates(b) for b in obj.branches) + 1
        if isinstance(obj, Recovery):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, Sequence):
            return sum(self.nstates(t) for t, _, _ in obj.terms)
        if isinstance(obj, Expression):
            return 1
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def gen(self, obj, state, stack, var, tag=None):
        """Generate code to match an object."""
        if isinstance(obj, Token):
            print("    // match a " + obj.title + " token")
            print("    " + self.mask_set(obj.get_first()) + ";")
            print("    call->state = " + str(state) + ";")
            print("    AWAIT_TOKEN;")
            print("yy" + str(state) + ":")
            print("    if(*token != " + obj.title + ") SYNTAX_ERROR;")
            print("    p->semstack[call->stack + " + str(stack) + "] = sem;")
            print("    CONSUME_TOKEN;")
            print("    mask_clear(p->mask);")
            return state + self.nstates(obj), stack + 1

        if isinstance(obj, Maybe):
            print("    // maybe match " + obj.title)
            print("    call->state = " + str(state) + ";")
            print("    AWAIT_TOKEN;")
            print("yy" + str(state) + ":")
            print("    if(!" + self.token_check(obj.get_first()) + "){")
            print("        " + self.mask_set(obj.get_first()) + ";")
            print("        goto yy" + str(state) + "_done;")
            print("    }")
            self.gen(obj.seq, state+1, stack, var)
            print("yy" + str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, ZeroOrMore):
            print("    // match zero or more " + obj.title)
            print("yy" + str(state) + ":")
            print("    call->state = " + str(state) + ";")
            print("    AWAIT_TOKEN;")
            print("    if(!" + self.token_check(obj.get_first()) + "){")
            print("        " + self.mask_set(obj.get_first()) + ";")
            print("        goto yy" + str(state) + "_done;")
            print("    }")
            print("")
            self.gen(obj.seq, state + 1, stack, var)
            print("    // try matching another " + obj.title)
            print("    goto yy" + str(state) + ";")
            print("yy" + str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, Branches):
            print("    // match branches " + obj.title)
            print("    call->state = " + str(state) + ";")
            print("    AWAIT_TOKEN;")
            print("yy" + str(state) + ":")
            print("    switch(*token){")
            bstate = state + 1
            for i, b in enumerate(obj.branches):
                for t in b.get_first():
                    print(
                        "        case " + t + ": goto yy"
                        + str(state) + "_br_" + str(i) + ";"
                    )
                bstate += self.nstates(b)
            print("        default:")
            print("            " + self.mask_set(obj.get_first()) + ";")
            print("            SYNTAX_ERROR;")
            print("    }")
            print("")
            # branch parsing
            bstate = state + 1
            for i, b in enumerate(obj.branches):
                print("yy" + str(state) + "_br_" + str(i) + ":")
                bstate, stack = self.gen(b, bstate, stack, var)
                print("    goto yy" + str(state) + "_done;")
                print("")
            print("yy"+ str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, Recovery):
            assert obj.after is not None, ".set_after() was never called"
            print("    // prepare for recovery " + obj.title)
            print("    call->recover = " + str(state) + ";")
            print("")
            self.gen(obj.seq, state + 1, stack, var)
            print("")
            print("    call->recover = 0;")
            print("    goto yy" + str(state) + "_done;")
            print("yy"+ str(state) + ":")
            print("    // recovery: consume all tokens until a valid one")
            print("    AWAIT_TOKEN;")
            print("    if(!" + self.token_check(obj.after) + "){")
            print("        CONSUME_TOKEN;")
            print("        return 0;")
            print("    }")
            self.print_code(obj.code)
            print("yy"+ str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, Sequence):
            var.new_scope()
            self.print_code(obj.precode)
            for i, (t, tag, code) in enumerate(obj.terms):
                if tag is not None:
                    assert isinstance(t, (Token, Expression)), "wrong type for tag: " + type(t).__name__
                    assert t.type is not None, "obj " + t.title + " has tag " + tag + " but no type!"
                    var.define(tag, t.type, stack)
                state, stack = self.gen(t, state, stack, var, tag)
                self.print_code(code)
                if i+1 != len(obj.terms):
                    print("")
            var.pop_scope()
            return state, stack

        if isinstance(obj, Expression):
            # code generated when calling this function
            print("    // match " + obj.title)
            print("    call->state = " + str(state) + ";")
            print("    CALL(" + self.parse_fn(obj) + ", " + str(stack) + ");")
            print("yy" + str(state) + ":")
            return state + self.nstates(obj), stack + 1

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def parse_fn(self, expr):
        """Get the generated function name for an Expression."""
        return "_parse_" + expr.title

    def declare_fn(self, expr):
        print("static int " + self.parse_fn(expr) + "(PARSE_FN_ARGS);")

    def define_fn(self, expr):
        """Generate the function definition for an Expression."""
        var = []
        print("static int " + self.parse_fn(expr) + "(PARSE_FN_ARGS){")
        # Ensure we have enough stack space to operate.
        print("    assert_sems_available(p, " + str(1 + self.stackmax(expr.seq)) + ");")
        # Jump to state.
        print("    switch(call->state){")
        print("        case " + str(0) + ": break;")
        for n in range(self.nstates(expr.seq)):
            print("        case " + str(n+1) + ": goto yy" + str(n+1) + ";")
        print("    }")
        print("")
        var = C.Variables()
        if expr.type is not None:
            var.define("$", expr.type, 0)

        # state starts at 1, since 0 is only the first time we're called
        # stack starts at 1, since we have always allocate the output
        self.gen(expr.seq, 1, 1, var)

        var.pop_scope()
        print("    // cleanup this call");
        print("    free_sems(p, call->stack+1);")
        print("    p->callslen--;");
        print("    return 0;")
        print("}")
        print("")

    def gen_file(self, g):
        print(textwrap.dedent("""
            #include <stdio.h>
            #include <stdbool.h>
            #include <stdlib.h>
            #include <stdarg.h>
            #include <stdint.h>
            #include <string.h>
        """.strip("\n")))

        print("typedef enum {")
        print("    _NOT_A_TOKEN = 0,")
        for _, token in g.sorted_tokens():
            print("    " + token.title + ",")
        print("} imf_token_t;")
        print("")

        print(textwrap.dedent(r"""
            struct imf_parser_t;
            typedef struct imf_parser_t imf_parser_t;
            struct call_t;
            typedef struct call_t call_t;
            struct sem_t;
            typedef struct sem_t sem_t;

            #define PARSE_FN_ARGS \
                call_t *call, imf_parser_t *p, imf_token_t *token, sem_t sem

            typedef int (*parse_fn_t)(PARSE_FN_ARGS);

            struct call_t {
                // a function to call
                parse_fn_t fn;

                // our starting position in the semstack
                int stack;

                // our position in the call
                int state;

                // the state to enter if we have to recover
                int recover;
            };

            struct sem_t {
                int type;
                // value
                union {
                    int i;
                } val;
                // location
                struct {
                    size_t start;
                    size_t end;
                } loc;
            };

            #define MASKLEN 1

            static void mask_clear(unsigned char *mask){
                memset(mask, 0, MASKLEN);
            }

            // accepts `n` ints, representing token values
            static void mask_set(unsigned char *mask, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                for(size_t i = 0; i < n; i++){
                    int t = va_arg(ap, int);
                    mask[t/8] |= ((unsigned char)1) << (t%8);
                }

                va_end(ap);
            }

            static const char *token_name(int t);

            static void mask_print(unsigned char *mask, char *joiner){
                bool first = true;
                for(int i = 0; i < MASKLEN; i++){
                    for(int j = 0; j < 8; j++){
                        if(!(mask[i] & ((unsigned char)1)<<j)) continue;
                        printf(
                            "%s%s",
                            first ? (first=false,"") : joiner,
                            token_name(i*8+j)
                        );
                    }
                }
            }

            struct imf_parser_t {
                // stack of calls
                call_t *callstack;
                size_t callslen;
                size_t callsmax;
                // stack of semvals
                sem_t *semstack;
                size_t semslen;
                size_t semsmax;
                // a mask of all possible tokens
                unsigned char mask[MASKLEN];
            };

            call_t *calls_append(imf_parser_t *p){
                if(p->callslen == p->callsmax){
                    printf("callstack overflow!\n");
                    exit(1);
                }
                return &p->callstack[p->callslen++];
            }

            void assert_sems_available(imf_parser_t *p, size_t n){
                if(p->semslen + n >= p->semsmax){
                    printf("semstack overflow!\n");
                    exit (1);
                }
            }

            sem_t *sems_append(imf_parser_t *p){
                if(p->semslen == p->semsmax){
                    printf("semstack overflow!\n");
                    exit(1);
                }
                return &p->semstack[p->semslen++];
            }

            void free_sems(imf_parser_t *p, size_t first){
                for(size_t i = first; i < p->semsmax; i++){
                    // TODO: actually free stuff
                    p->semstack[i] = (sem_t){0};
                }
            }

            static const char *_fn_name(parse_fn_t fn);

            static void print_stack(imf_parser_t *p){
                for(size_t i = 0; i < p->callslen; i++){
                    call_t call = p->callstack[i];
                    printf("parsing %s (%d)\n", _fn_name(call.fn), call.state);
                }

            }

            #define CALL(_fn, _stack) do { \
                /* allocate call */ \
                call_t *subcall = calls_append(p); \
                *subcall = (call_t){ .fn = _fn,  .stack = call->stack + _stack }; \
                return 0; \
            } while(0)

            #define AWAIT_TOKEN \
                if(!*token) return 0

            #define CONSUME_TOKEN \
                *token = _NOT_A_TOKEN

            bool token_in(imf_token_t token, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                bool out = false;
                for(size_t i = 0; i < n; i++){
                    if(token != va_arg(ap, int)) continue;
                    out = true;
                    break;
                }

                va_end(ap);
                return out;
            }

            #define SYNTAX_ERROR do { \
                printf("syntax error!\n"); \
                print_stack(p); \
                printf("expected one of: {"); \
                mask_print(p->mask, ","); \
                printf("} but got %s\n", token_name(*token)); \
                return 1; \
            } while(0)

            void do_parse(
                imf_parser_t *p,
                parse_fn_t entrypoint,
                imf_token_t token,
                sem_t sem,
                bool *ok
            ){
                *ok = false;

                if(p->callslen == 0){
                    // first token, initialize the call
                    call_t *call = calls_append(p);
                    *call = (call_t){
                        .fn = entrypoint, .stack = p->semslen-1
                    };
                }

                while(p->callslen){
                    size_t last = p->callslen;
                    call_t *call = &p->callstack[p->callslen-1];
                    int syntax_error = call->fn(call, p, &token, sem);
                    if(!syntax_error){
                        if(p->callslen == last){
                            // pause until the next token
                            return;
                        }
                        continue;
                    }
                    // syntax error; pop from the stack until we can recover
                    while(p->callslen){
                        call_t *call = &p->callstack[p->callslen-1];
                        if(call->recover){
                            // this call has a recovery state we can enter
                            call->state = call->recover;
                            call->recover = 0;
                            break;
                        }else{
                            free_sems(p, call->stack);
                            p->callslen--;
                        }
                    }
                }

                // parsed everything!
                *ok = true;
            }
        """.strip('\n')))

        # declare functions
        for _, expr in g.sorted_expressions():
            self.declare_fn(expr)
        print("")

        print("static const char *token_name(int t){")
        print("    switch(t){")
        for name, _ in g.sorted_tokens():
            print("        case " + name + ": return \"" +name+ "\";")
        print("        default: return \"unknown\";")
        print("    }")
        print("}")
        print("")

        # get function names
        print("static const char *_fn_name(parse_fn_t fn){")
        for _, expr in g.sorted_expressions():
            print(
                "    if(fn == " + self.parse_fn(expr) + ")"
                + " return \""+self.parse_fn(expr)+"\";"
            )
        print("    return \"unknown\";")
        print("}")
        print("")

        # define functions
        for _, expr in g.sorted_expressions():
            self.define_fn(expr)

        print(textwrap.dedent(r"""
            void parse_line(imf_parser_t *p, imf_token_t token, sem_t sem, bool *ok){
                do_parse(p, _parse_line, token, sem, ok);
            }

            int main(int argc, char **argv){
                call_t calls[100];
                sem_t sems[100];
                size_t callsmax = sizeof(calls) / sizeof(*calls);
                size_t semsmax = sizeof(sems) / sizeof(*sems);
                imf_parser_t p = {
                    .callstack = calls,
                    .callsmax = callsmax,
                    .semstack = sems,
                    .semsmax = semsmax,
                };

                int tokens[] = {
                    NUM, 0,
                    PLUS, 0,
                    LPAREN, 0,
                    NUM, 0,
                    NUM, 0,
                    RPAREN, 0,
                    EOL, 0,

                    NUM, 18,
                    PLUS, 0,
                    LPAREN, 0,
                    NUM, 6,
                    RPAREN, 0,
                    EOL, 0,
                };
                size_t ntokens = sizeof(tokens) / sizeof(*tokens);

                bool ok = false;
                for(size_t i = 0; i < ntokens; i += 2){
                    // printf("feeding %s\n", token_name(tokens[i]));
                    sem_t sem = {.val.i = tokens[i+1]};
                    parse_line(&p, tokens[i], sem, &ok);

                    if(tokens[i] == EOL){
                        if(!ok){
                            printf("didn't get OK afterwards!\n");
                            return 1;
                        }
                    }else{
                        if(ok){
                            printf("got OK too early!\n");
                            return 1;
                        }
                    }
                }

                return 0;
            }
        """.strip("\n")), end="")


### Meta Parser ###

class ParsedName:
    def __init__(self, name, tag):
        self.name = name
        self.tag = tag

    def process(self, g, e):
        e.match(g.get_name(self.name), self.tag)

class ParsedSnippet:
    def __init__(self, text, tag=None):
        self.text = text
        self.tag = tag

    def process(self, g, e):
        e.exec(self.text, self.tag)

class ParsedBranches:
    def __init__(self):
        self.branches = []

    def process(self, g, e):
        # Collapse single-branch expressions.
        if len(self.branches) == 1:
            self.branches[0].process(g, e)
            return

        with e.branches() as b:
            for parsed_branch in self.branches:
                with b.branch():
                    parsed_branch.process(g, e)

class ParsedRecovery:
    def __init__(self, sub):
        self.sub = sub
        self.code = []

    def process(self, g, e):
        with e.recovery() as r:
            self.sub.process(g, e)
            for c in self.code:
                r.add_recovery_code(c.text, c.tag)

class ParsedMultiplier:
    def __init__(self, term, multiplier):
        self.term = term
        self.multiplier = multiplier

    def process(self, g, e):
        if self.multiplier == "1":
            self.term.process(g, e)
        elif self.multiplier == "?":
            with e.maybe():
                self.term.process(g, e)
        elif self.multiplier == "*":
            with e.zero_or_more():
                self.term.process(g, e)
        else:
            raise RuntimeError("unrecognized multiplier: " + str(self.multiplier))

class ParsedSequence:
    def __init__(self):
        self.elems = []

    def process(self, g, e):
        for elem in self.elems:
            elem.process(g, e)

class ParsedDoc:
    def __init__(self):
        self.precode = []
        self.defs = []
        self.postcode = []

    def build_grammar(self):
        g = Grammar()

        # first declare all tokens and expressions
        for name, val in self.defs:
            if val is None:
                _ = g.token(name.name, name.tag)
            else:
                e = g.expr(name.name)
                e.type = name.tag

        # then define all the expressions
        for name, val in self.defs:
            if val is None:
                continue
            e = g.get_name(name.name)
            val.process(g, e)

        g.check()

        return self.precode, g, self.postcode

# BEGIN GENERATED CODE
# END GENERATED CODE

class CharStream:
    def __init__(self):
        self._lookahead = []

    def peek(self, index=0):
        while len(self._lookahead) <= index:
            temp = yield
            self._lookahead.append(temp)
        return self._lookahead[index]

    def __next__(self):
        if self._lookahead:
            return self._lookahead.pop()
        temp = yield
        return temp

    def empty(self):
        return not self._lookahead

class Discard:
    pass

class Tokenizer:
    def __init__(self):
        self.cs = CharStream()
        self.out = []

        def _gen():
            yield from self._tokenize_many()
        self.gen = _gen()
        next(self.gen)

    def feed(self, c):
        if self.gen is None:
            raise RuntimeError("tokenizer is already done!")

        if c is None:
            try:
                # This should finish the generator.
                self.gen.send(None)
                raise RuntimeError("tokenizer failed to exit after None")
            except StopIteration:
                return self.out

        self.gen.send(c)
        out = self.out
        self.out = []
        return out

    def _tokenize_many(self):
        while True:
            token = yield from self._tokenize()
            if token is None:
                break
            if isinstance(token, Discard):
                continue
            self.out.append(token)

    def _tokenize(self):
        c = yield from self.cs.peek()

        if c is None:
            return None

        if c in " \r\n\t":
            return (yield from self._ignore_whitespace())

        if c in "#":
            return (yield from self._ignore_comment())

        singles = {
            "*": ASTERISK,
            ":": COLON,
            "=": EQ,
            "<": LANGLE,
            "[": LBRACKET,
            "(": LPAREN,
            "|": PIPE,
            "?": QUESTION,
            ">": RANGLE,
            "]": RBRACKET,
            ")": RPAREN,
            ";": SEMI,
        }
        if c in singles:
            yield from next(self.cs)
            return MetaToken(singles[c], c)

        if c == "{":
            return (yield from self._tokenize_code())

        if c in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_":
            return (yield from self._tokenize_text())

        raise RuntimeError("invalid character in tokenizer" + str(ord(c)))

    def _ignore_whitespace(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c not in " \r\n\t":
                break
            yield from next(self.cs)
        return Discard()

    def _ignore_comment(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c in "\r\n":
                break
            yield from next(self.cs)
        return Discard()

    def _tokenize_code(self):
        # Some number of leading '{' chars which we discard...
        count = 0
        while True:
            c = yield from self.cs.peek()
            if c != "{":
                break
            yield from next(self.cs)
            count += 1

        # ... and code body, which we keep...
        text = ""
        while True:
            c = yield from self.cs.peek()
            if c is None:
                raise RuntimeError("unterminated code block:\n" + text)
            if c != "}":
                text += yield from next(self.cs)
                continue

            # ... and a matching number of closing '}' chars which we discard.
            matched = 1
            while matched < count:
                c = yield from self.cs.peek(matched)
                if c != "}":
                    break
                matched += 1

            if matched != count:
                # oops, these were actually part of the code body
                for _ in range(matched):
                    text += yield from next(self.cs)
                continue

            # discard these, they're not part of the code body
            for _ in range(matched):
                yield from next(self.cs)
            return MetaToken(CODE, text)

    def _tokenize_text(self):
        text = ""
        while True:
            c = yield from self.cs.peek()
            if c is None:
                break
            if c not in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_0123456789":
                break
            text += yield from next(self.cs)
        return MetaToken(TEXT, text)

    def iter(self, chars):
        def _chars(self):
            yield from chars
            yield None

        for c in chars:
            tokens = self.feed(c)
            if tokens is None:
                break
            yield from tokens
        yield MetaToken(EOF, "")


@contextlib.contextmanager
def read_template(template, tag="GENERATED CODE", file=None):
    if template is None:
        yield
        return

    with open(template) as f:
        lines = iter(f)
        # passtru pre-tag lines
        for line in lines:
            print(line, end="", file=file)
            if "BEGIN " + tag in line:
                break
        else:
            print('did not find "BEGIN ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # discard inter-tag lines
        for line in lines:
            if "END " in line:
                yield  # generated code gets printed here
                print(line, end="", file=file)
                break
        else:
            print('did not find "END ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # passtru post-tag lines
        for line in lines:
            print(line, end="", file=file)


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("input", nargs="?")
    parser.add_argument("-t", "--template")
    parser.add_argument("-T", "--template-tag", default="GENERATED CODE")
    parser.add_argument("-o", "--output")
    parser.add_argument("-c", "--check", action="store_true")
    args = parser.parse_args()

    assert not args.check or args.output, "--check requires --output"

    if args.input is None:
        text = sys.stdin.read()
    else:
        with open(args.input) as f:
            text = f.read()

    out = io.StringIO()
    with read_template(args.template, args.template_tag, out):
        parser = MetaParser()

        for t in Tokenizer().iter(text):
            # the last time this is called it should return non-None
            parsed_doc = parser.feed(t)

        assert parsed_doc, "didn't get a doc!"

        precode, g, postcode = parsed_doc.build_grammar()

        generator = Python()

        generator.print_code(precode, "", out)

        # TODO: find a better way to configure the root a grammar
        generator.gen_file(g, "line", file=out)

        generator.print_code(postcode, "", out)

    if args.check:
        with open(args.output) as f:
            old = f.read()
        if out.getvalue() == old:
            sys.exit(0)
        print(args.output + " would be modified", file=sys.stderr)
        sys.exit(1)

    if args.output is not None:
        with open(args.output, "w") as f:
            f.write(out.getvalue())
    else:
        print(out.getvalue(), end="", flush=True)
