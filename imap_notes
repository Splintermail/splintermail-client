# IMAP COMMANDS
CAPABILITY
NOOP
LOGOUT
--
STARTTLS
AUTHENTICATE <auth mechanism name> (server responds, and stuff continues)
LOGIN username password
--
SELECT <mailbox name>
EXAMINE
CREATE
DELETE
RENAME
SUBSCRIBE
UNSUBSCRIBE
LIST
LSUB
STATUS
APPEND
--
CHECK
CLOSE
EXPUNGE
SEARCH*
FETCH*
STORE
COPY
UID

# * these will be hard

# IMAP RESPONSES

Status responses:
    OK          tagged      untagged
    NO          tagged      untagged
    BAD         tagged      untagged
    PREAUTH                 untagged
    BYE                     untagged

Status response codes (optional):
    ALERT:
        human readable text, that should be passed to the user
    BADCHARSET
        optionally followed by a parenthesized list of charsets.  A SEARCH
        failed because the given charset is not supported
    CAPABILITY
    PARSE:
        human readable error after trying to parse a message
    PERMANENTFLAGS:
        a list of flags that can be changed permananently (that is, that
        changes persist across sessions)
    READ-ONLY
        A mailbox is selected read-only
    READ-WRITE
    TRYCREATE
        An append or copy attempt is failing because the target mailbox does
        not exist
    UIDNEXT
        indicates the next unique identifier value
    UIDVALIDITY
        indicates the unique identifier validity value
    UNSEEN

OK [response code] human-readable text
  - if tagged, indicates successful completion of a command
  - if untagged, indicates information-only message

NO [response code] human-readable text
  - if tagged, indicates unsuccessful completion of a command
  - if untagged, indicates a warning-only message

BAD [response code] human-readable text
  - if tagged, indicates protocol-level error in client command
  - if untagged, indicates protocol-level error or server error, for which the
    associated client command cannot be determined.

PREAUTH [response code] human-readable text
  - always untagged, possible greeting, indicates no LOGIN is necessary

BYE [response code] human-readable text:
  - may occur as part of the normal logout process, or
  - as a panic shutdown announcement, or
  - as an announcement of an inactivity autologout, or
  - as a greeting, indicating the server is not willing to accept a connection
    from this client
  - In the normal logout case, a client should continue to process commands
    because it is possible the server has not finished processing all commands

(1 = command generates 1 response, * = command generates many responses)

1 CAPABILITY
* LIST (might be multiple)
* LSUB
1 STATUS
1 SEARCH
1 FLAGS (after SELECT or EXAMINE)

* EXISTS (SELECT, EXAMINE, or size change)
* RECENT (SELECT, EXAMINE, or size change)

* EXPUNGE
    Cannot be sent when no command is in progress nor while responding to a
    FEATCH, STORE, or SEARCH command, because client and server would enter a
    race condition where synchronization of message sequence numbers would be
    ambiguous.  If a UID command is in progress, EXPUNGE *can* be sent.

* FETCH
    can be response to a command or a unilater server data (flag update)

Unilateral data:
    Bye
    untagged OK or NO
    exists
    recent
    expunge
    fetch


# general DITM architecture for IMAP:

IMAP is not designed to be a request-response architecture but rather a fully
bidirectional pipeline of synchronization information.  Many server "responses"
are not strictly "responses" but rather synchronization info that might be sent
as a response to a client request or unilaterally.  Other "responses" truly are
responses and would have no meaning if sent unilaterally.

Responses which *must* be handled whether or not we requested them (also, it is
probably not worth handling them any other way):

  - Any untagged status response
  - EXISTS
  - RECENT
  - EXPUNGE
  - FETCH (in some cases)

Responses which make no sense if sent unilaterally:
  - Any tagged status response
  - LIST
  - LSUB
  - SEARCH
  - STATUS

Responses which could be treated either way:
  - CAPABILITY
  - SEARCH
  - FLAGS

Therefore, our general architecture will be the following:

                Server                Client
     ________  connections  _______  connections  ________
    |        | <---------- |       | <---------- |        |
    |  MAIL  | <---------- | LOCAL | <---------- | EMAIL  |
    | SERVER | <---------- | STORE |             | CLIENT |
    |________|             |_______|             |________|

With the following details:
  - There is one "unselected" server connection per user, through which all
    LIST and LSUB requests are passed.
  - There is one "selected" server connection per opened LOCAL STORE mailbox.
    If multiple email clients are accessing the same LOCAL STORE's INBOX, only
    one server connection has INBOX "selected".
  - There is one server connection per user which selects the DITM_KEYS mailbox
    DITM uses this box directly and does not let email clients see the box via
    LIST or LSUB or let them SELECT it or CREATE it or anything like that.
  - When a client connection calls an IMAP command, DITM first checks that
    the corresponding server connection has brought the relevant part of the
    LOCAL STORE up-to-date, and then the response is generated locally, not
    bounced off of the mail server, like is generally the case with POP.
  - Exceptions to the "response generated locally" include the LIST, LSUB,
    and (under some circumstances) STATUS commands, as well as any commands
    which alter the server state such as CREATE, DELETE, SUBSCRIBE, and
    UNSUBSCRIBE.
  - There is also another special mailbox, with a user-facing name, perhaps
    "DITM Alerts", which exists only on the LOCAL STORE, through which DITM
    can communicate security alerts or critical failures or whatnot.


Order of work is:
    Make IMAP client capable of logging in
    Make IMAP client capable of mirroring a folder layout
    Make IMAP client capable of syncing a folder
    Make IMAP server, and all that entails...

Right now I'm going to work on:
    Parsing server responses correctly

# Socket / TLS / IMAP engines:

     (libuv)          (OpenSSL)                           (thread pool)
    ________          ________          ________           ___________
   |        | rawin  |        | decin  |        | imapin  |           |
   | SOCKET | -----> |  TLS   | -----> |  IMAP  | ------> |   IMAP    |
   | ENGINE | <----- | ENGINE | <----- | ENGINE | <------ | FUNCTIONS |
   |________| rawout |________| decout |________| imapout |___________|

Memory contexts:
    IMAP user context (ixu)
      - for each user who is connected, run an IDLE session for key updates
      - maintain a count of active connections, so we know when to disconnect
      - allocated when a new user is identified
      - cleaned up when the last of the user's sessions end
      - has list of keys

    IMAP session context: (ixs)
      - allocated by socket engine on connection, or key-related IDLE session
      - lists commands in flight
      - has rawin, rawout buffers (SSL memory buffer type)
      - has decin, decout (dstr_t type)

    IMAP command context: (ixc)
      - allocated by IMAP engine
      - tracks state of command / command-specific variables
      - has imapin, imapout buffers (dstr_t type)

Should the socket engine, the TLS engine, and the IMAP engine be separated?
    Pros:
      - Testability would be much better, as each section of the pipeline could
        be tested individually.
      - Nodes would be modular, and could potentially be strung together in
        different orders (for non-TLS connections, or a STARTTLS variety, for
        example)
    Cons:
      - each engine would need to be separated with a buffer pool, and some
        sort of event queue for passing messages, with checks for freezing
        inputs when a buffer pool is empty and callbacks for unfreezing it.
        There would be a small explosion of callback functions due to lots of
      - different situations arising (such as a allocate_before_accept()
        callback and a cleanup_after_failed_accept() callback, just in the
        libuv connection_cb function)
      - There's not a forseeable need for the improved modularity
      - Far greater complexity, primarily due to the distributed error handling
    Conclusion:
      - No, Socket Engine / TLS Engine / IMAP Engine should be one monolithic
        block, because the improved modularity wouldn't actually be useful and
        the improved testability argument is a wash compared to the massive
        increase in complexity.

buffer pool api
    bufp_init() : allocates memory and sets bufp_empty_cb and bufp_avail_cb
    bufp_get() : returns an available dstr_t*, or calls bufp_empty_cb with data
    bufp_release() : might call bufp_avail_cb with same data

Application write:
    Command lists its own PERCOM struct in the IMAP engine's queue
    IMAP engine takes PERCOM and does whatever conversion, writing to PERCLI
    IMAP engine lists PERCLI in TLS engine's queue
    TLS engine takes PERCLI and does encryption
    TLS engine lists PERCLI in libuv's queue
    Socket engine sends the packet when it can.

Read from socket: (don't know which PERCOM it goes to yet)
    Socket engine reads from a socket, puts data in PERCLI struct
    libuv puts PERCLI in TLS engine's queue, uv_read_stop()
    TLS engine takes PERCLI and does decryption, uv_read_start() and kick loop
    TLS engine puts PERCLI in IMAP Engine's queue
    IMAP Engine decides which PERCOM needs this data, decodes into that PERCOM
    Command handles the data that was read

cleaning of all resources:
    Our pipeline is bidirectional (unavoidable, since libuv and openssl require
    one thread to be in charge of reading and writing).  Therefore, our
    shutdown protol needs to be bidirectional as well, and no node shuts down
    until it has received a "quit" message in both directions:
                              ______
                     downin  |      | downout
                     ------> | some | ------>
                     <------ | node | <------
                      upout  |______|   upin

    One node's shutdown sequence:
      - recieve "quit" message from upstream, on "downin"
      - (downin queue now empty, since nothing more will be sent)
      - pass "quit" downstream, via "downout"
      - (downout queue gets emptied by next link in chain)
      - wait for "quit" to come back up from downstream, via "upin"
      - (upin queue now empty, downstream node has agreed to quit)
      - pass "quit" back upstream via "upout"
      - (upout queue now empty, and we can shutdown)

    Overall sequence:
      - libuv node initiates shutdown
      - when libuv receives a "quit" again, that means everything else has
        down_ref()'ed all relevant contexts, and libuv only has to close
        sockets for any remaining contexts, down_ref() them, and exit the loop.

Cleaning up of one connection, at any time:
  - if you call uv_close, and free all associated contexts, you might have
    (for instance) a read callback in the middle of writing to one of those
    contexts.
  - So what you need to do is have each engine promise to ignore the contexts,
    and after each engine promises, then have a callback that frees the
    contexts:
      - socket engine stops reading, closes socket, (ongoing writes are
        cancelled),
      - TLS engine stops processing packets in either direction
      - IMAP stops procesing packets or dispatching functions
      - Wait for any ongoing IMAP functions to complete
      - The last-engine-to-forget could be the one that does the cleanup
      - Or there could be a separate cleaner-upper thread that handles that
    Or:
      - each engine could own the things that it reads from.  Actually, I don't
        think this would solve anything.  It would just suck.
    Or:
      - we could try and do the whole thing on one single thread, possibly with
        the IMAP functions still offloaded to a separate thread.  This might be
        less complex than the multi-engine setup I was thinking of.

Buffer Pools:
    buffer pools are necessary for overlapped IO, like using the Windows IOCP
    API.  That is, on the read side, the kernel needs access to a new read
    buffer *before* the application has a chance to use the read buffer that
    just got filled.  This is good for network throughput, because if the
    network is the slowest part of the system, the best thing you can do is
    make sure that the slowest part runs as fast as possible.  The write side
    is similar.

    Buffer pools are also necessary in between separate nodes of a pipeline.
    That is, if you have a thread pool handling IMAP functions, if you want to
    avoid gratuitous read-stop and read-start operations on the socket, you
    need a pool of buffers that the read pipeline can write to.

    However, what if you have a single, slow, read-heavy IMAP function (such
    as a large message decryption) in flight at the same time as a lot of fast,
    chatty IMAP functions?  The read socket behind the IMAP function could fill
    all of the read-side buffer pool with useless packets.

    Basically, a mechanism that freezes the input to a node is not good enough,
    you need to be able to freeze a whole pipeline upstream from a node, and
    intelligently and efficiently unfreeze it at a later time.  Also, my
    current model for unfreezing the sockets is a FIFO model, but you would
    need a different model for unfreezing specific sockets at will.

    Of course, if you went the select() route (meaning one thread for the whole
    application) then you would avoid all of this, because every read would be
    taken to completion and every write would be taken to completion, always,
    by the single thread.  The single threaded libuv solution would similary
    only require the read buffers and the write buffers, and one thread's slow
    reading would never affect another thread's read-side buffering because
    there would only be one thread.


    OK, I am starting to think that the amount of extra complexity for having
    a multi-threaded-capable event-loop-based function is not worth the extra
    performance gain.

    I think I will build off of libuv (or maybe even just select()??) and just
    write a completely single-threaded application.



Ok... well... shit.  I see that I need a better mechanism for freezing
write pipelines no matter what.  So here goes:


New and improved modular pipelining:
        __________                  _________                  ____________
       |          | READ --------> |         | READ --------> |            |
       |          | <--- READ_DONE |  SOME   | <--- READ_DONE |            |
       | UPSTREAM |                | GENERIC |                | DOWNSTREAM |
       |          | <------- WRITE |   NODE  | <------- WRITE |            |
       |__________| WRITE_DONE --> |_________| WRITE_DONE --> |____________|

    Details:
        After a node sends a READ or a WRITE, it can decide if it needs to
        freeze this context in this direction until further notice.  "Freeze"
        just means it doesn't process any more operations until it receives the
        relevant READ_DONE/WRITE_DONE trigger.

        After handling a READ or a WRITE, it is appropriate to send a READ_DONE
        or WRITE_DONE to the previous node.  These indicate that the read or
        write was processed, and another read or write is possible.

        When a node receives a WRITE_DONE/READ_DONE, it should try and generate
        another WRITE/READ right away, if possible.

    Additional abstractions needed:
        there needs to be a buf_get and a buf_release trigger for nodes to
        get/release buffers from their neighbor nodes


Error handling in the pipeline:
    the node furthest from the network is the node most capable of gracefully
    handling errors, therefore errors should be passed in that direction, and
    not the other way (IE, along read paths and write_done paths, but not
    read_done paths or write paths).

    An example:
      - read error talking to server, but only IMAP engine knows how to send
        a useful message via the other socket to the email client
    A contrapositive example:
      - IMAP engine decides there is an inconsistency and wants to abandon the
        connections.  The libuv thread doesn't give a fuck what the error was.

New bugs:
    how exactly do I determine the "ideal" number of buffers in a pool?
        I'm sure that there is a per-processor number that would be fine, and
        that a per-connection would be too much (in the many-imap-sessions
        case), however, I don't know what that right number would be.  Or there
        might be a per-connection component to it, if you know that every
        connection is always going to have at least one buffer being written to
        in the kernel, for instance.

    Wait... if the socket is closed all of the buffers allocated for cancelled
    reads will be passed to the read callback, right?
        I dunno.  The documentation seems to say no but I don't see how else
        you would free allocated buffers.  Smells like a recipe for a memory
        leak.
