# IMAP COMMANDS
Status response codes (optional):
    ALERT:
        human readable text, that should be passed to the user
    BADCHARSET
        optionally followed by a parenthesized list of charsets.  A SEARCH
        failed because the given charset is not supported
    CAPABILITY
    PARSE:
        human readable error after trying to parse a message
    PERMANENTFLAGS:
        a list of flags that can be changed permananently (that is, that
        changes persist across sessions)
    READ-ONLY
        A mailbox is selected read-only
    READ-WRITE
    TRYCREATE
        An append or copy attempt is failing because the target mailbox does
        not exist
    UIDNEXT
        indicates the next unique identifier value
    UIDVALIDITY
        indicates the unique identifier validity value
    UNSEEN

OK [response code] human-readable text
  - if tagged, indicates successful completion of a command
  - if untagged, indicates information-only message

NO [response code] human-readable text
  - if tagged, indicates unsuccessful completion of a command
  - if untagged, indicates a warning-only message

BAD [response code] human-readable text
  - if tagged, indicates protocol-level error in client command
  - if untagged, indicates protocol-level error or server error, for which the
    associated client command cannot be determined.

PREAUTH [response code] human-readable text
  - always untagged, possible greeting, indicates no LOGIN is necessary

BYE [response code] human-readable text:
  - may occur as part of the normal logout process, or
  - as a panic shutdown announcement, or
  - as an announcement of an inactivity autologout, or
  - as a greeting, indicating the server is not willing to accept a connection
    from this client
  - In the normal logout case, a client should continue to process commands
    because it is possible the server has not finished processing all commands

(1 = command generates 1 response, * = command generates many responses)

1 CAPABILITY
* LIST (might be multiple)
* LSUB
1 STATUS
1 SEARCH
1 FLAGS (after SELECT or EXAMINE)

* EXISTS (SELECT, EXAMINE, or size change)
* RECENT (SELECT, EXAMINE, or size change)

* EXPUNGE
    Cannot be sent when no command is in progress nor while responding to a
    FEATCH, STORE, or SEARCH command, because client and server would enter a
    race condition where synchronization of message sequence numbers would be
    ambiguous.  If a UID command is in progress, EXPUNGE *can* be sent.

* FETCH
    can be response to a command or a unilateral server data (flag update)

Unilateral data:
    Bye
    untagged OK or NO
    exists (indicates new message in the selected mailbox, even after APPEND)
    recent (indicates new message, which is marked recent)
    expunge (indicates a message has been deleted)
    fetch (flag updates; nothing else is allowed to change in a message)

# parsing issue:

ALERT can be an atom.  So in bison's grammar it is impossible to distinguish
between between an ALERT or an atom, when atom might consist of a single ATOM.

Solutions:
  - add more states to scanner so that only specific keywords are passed at
    specific times
        I dislike this because it would complicate the scanner, and it would
        require a lot more states, and there would have to be synchronization
        between which states are possible in the grammar and in the scanner.
  - add an extra "mask-literal" step after the scanner, where illegal literals
    get turned into ATOMS or ASTR_ATOMS, according to the parser state
        This has the same synchronization problem but it does not complicate
        the scanner.  Also, this is a "whitelist" strategy, not a "blacklist"
        strategy, which should make this less error prone.


# general DITM architecture for IMAP:

IMAP is not designed to be a request-response architecture but rather a fully
bidirectional pipeline of synchronization information.  Many server "responses"
are not strictly "responses" but rather synchronization info that might be sent
as a response to a client request or unilaterally.  Other "responses" truly are
responses and would have no meaning if sent unilaterally.

Responses which *must* be handled whether or not we requested them (also, it is
probably not worth handling them any other way):

  - Any untagged status response
  - EXISTS
  - RECENT
  - EXPUNGE
  - FETCH (in some cases)

Responses which make no sense if sent unilaterally:
  - Any tagged status response
  - LIST
  - LSUB
  - SEARCH
  - STATUS

Responses which could be treated either way:
  - CAPABILITY
  - SEARCH
  - FLAGS

Therefore, our general architecture will be the following:

                Server                Client
     ________  connections  _______  connections  ________
    |        | <---------- |       | <---------- |        |
    |  MAIL  | <---------- | LOCAL | <---------- | EMAIL  |
    | SERVER | <---------- | STORE |             | CLIENT |
    |________|             |_______|             |________|

With the following details:
  - There is one "unselected" server connection per user, through which all
    LIST and LSUB requests are passed.
  - There is one "selected" server connection per opened LOCAL STORE mailbox.
    If multiple email clients are accessing the same LOCAL STORE's INBOX, only
    one server connection has INBOX "selected".
  - There is one server connection per user which selects the DITM_KEYS mailbox
    DITM uses this box directly and does not let email clients see the box via
    LIST or LSUB or let them SELECT it or CREATE it or anything like that.
  - When a client connection calls an IMAP command, DITM first checks that
    the corresponding server connection has brought the relevant part of the
    LOCAL STORE up-to-date, and then the response is generated locally, not
    bounced off of the mail server, like is generally the case with POP.
  - Exceptions to the "response generated locally" include the LIST, LSUB,
    and (under some circumstances) STATUS commands, as well as any commands
    which alter the server state such as CREATE, DELETE, SUBSCRIBE, and
    UNSUBSCRIBE.
  - There is also another special mailbox, with a user-facing name, perhaps
    "DITM Alerts", which exists only on the LOCAL STORE, through which DITM
    can communicate security alerts or critical failures or whatnot.

data structures:
  - Only one in-memory copy of a user's folder hierarchy, on a per-user basis.
    A per-user data structure associated with a single imap connection keeps
    track of the folders.  Each folder will have to be associated with things
    like a list of pending deletions (which have to be coordinated between
    multiple possible sessions) and a list of pending deletions, stuff like that
  - A per-connection data structure keeps in memory the contents of a single
    folder which that connection is associated with.
  -


Order of work is:
    Make IMAP client capable of logging in
    Make IMAP client capable of mirroring a folder layout
    Make IMAP client capable of syncing a folder
    Make IMAP server, and all that entails...

Right now I'm going to work on:
    Parsing server responses correctly

# Socket / TLS / IMAP engines:

     (libuv)          (OpenSSL)                           (thread pool)
    ________          ________          ________           ___________
   |        | rawin  |        | decin  |        | imapin  |           |
   | SOCKET | -----> |  TLS   | -----> |  IMAP  | ------> |   IMAP    |
   | ENGINE | <----- | ENGINE | <----- | ENGINE | <------ | FUNCTIONS |
   |________| rawout |________| decout |________| imapout |___________|

Memory contexts:
    IMAP user context (ixu)
      - for each user who is connected, run an IDLE session for key updates
      - maintain a count of active connections, so we know when to disconnect
      - allocated when a new user is identified
      - cleaned up when the last of the user's sessions end
      - has list of keys

    IMAP session context: (ixs)
      - allocated by socket engine on connection, or key-related IDLE session
      - lists commands in flight
      - has rawin, rawout buffers (SSL memory buffer type)
      - has decin, decout (dstr_t type)

    IMAP command context: (ixc)
      - allocated by IMAP engine
      - tracks state of command / command-specific variables
      - has imapin, imapout buffers (dstr_t type)

Should the socket engine, the TLS engine, and the IMAP engine be separated?
    Pros:
      - Testability would be much better, as each section of the pipeline could
        be tested individually.
      - Nodes would be modular, and could potentially be strung together in
        different orders (for non-TLS connections, or a STARTTLS variety, for
        example)
    Cons:
      - each engine would need to be separated with a buffer pool, and some
        sort of event queue for passing messages, with checks for freezing
        inputs when a buffer pool is empty and callbacks for unfreezing it.
        There would be a small explosion of callback functions due to lots of
      - different situations arising (such as a allocate_before_accept()
        callback and a cleanup_after_failed_accept() callback, just in the
        libuv connection_cb function)
      - There's not a forseeable need for the improved modularity
      - Far greater complexity, primarily due to the distributed error handling
    Conclusion:
      - No, Socket Engine / TLS Engine / IMAP Engine should be one monolithic
        block, because the improved modularity wouldn't actually be useful and
        the improved testability argument is a wash compared to the massive
        increase in complexity.

buffer pool api
    bufp_init() : allocates memory and sets bufp_empty_cb and bufp_avail_cb
    bufp_get() : returns an available dstr_t*, or calls bufp_empty_cb with data
    bufp_release() : might call bufp_avail_cb with same data

Application write:
    Command lists its own PERCOM struct in the IMAP engine's queue
    IMAP engine takes PERCOM and does whatever conversion, writing to PERCLI
    IMAP engine lists PERCLI in TLS engine's queue
    TLS engine takes PERCLI and does encryption
    TLS engine lists PERCLI in libuv's queue
    Socket engine sends the packet when it can.

Read from socket: (don't know which PERCOM it goes to yet)
    Socket engine reads from a socket, puts data in PERCLI struct
    libuv puts PERCLI in TLS engine's queue, uv_read_stop()
    TLS engine takes PERCLI and does decryption, uv_read_start() and kick loop
    TLS engine puts PERCLI in IMAP Engine's queue
    IMAP Engine decides which PERCOM needs this data, decodes into that PERCOM
    Command handles the data that was read

cleaning of all resources:
    Our pipeline is bidirectional (unavoidable, since libuv and openssl require
    one thread to be in charge of reading and writing).  Therefore, our
    shutdown protol needs to be bidirectional as well, and no node shuts down
    until it has received a "quit" message in both directions:
                              ______
                     downin  |      | downout
                     ------> | some | ------>
                     <------ | node | <------
                      upout  |______|   upin

    One node's shutdown sequence:
      - recieve "quit" message from upstream, on "downin"
      - (downin queue now empty, since nothing more will be sent)
      - pass "quit" downstream, via "downout"
      - (downout queue gets emptied by next link in chain)
      - wait for "quit" to come back up from downstream, via "upin"
      - (upin queue now empty, downstream node has agreed to quit)
      - pass "quit" back upstream via "upout"
      - (upout queue now empty, and we can shutdown)

    Overall sequence:
      - libuv node initiates shutdown
      - when libuv receives a "quit" again, that means everything else has
        down_ref()'ed all relevant contexts, and libuv only has to close
        sockets for any remaining contexts, down_ref() them, and exit the loop.

Cleaning up of one connection, at any time:
  - if you call uv_close, and free all associated contexts, you might have
    (for instance) a read callback in the middle of writing to one of those
    contexts.
  - So what you need to do is have each engine promise to ignore the contexts,
    and after each engine promises, then have a callback that frees the
    contexts:
      - socket engine stops reading, closes socket, (ongoing writes are
        cancelled),
      - TLS engine stops processing packets in either direction
      - IMAP stops procesing packets or dispatching functions
      - Wait for any ongoing IMAP functions to complete
      - The last-engine-to-forget could be the one that does the cleanup
      - Or there could be a separate cleaner-upper thread that handles that
    Or:
      - each engine could own the things that it reads from.  Actually, I don't
        think this would solve anything.  It would just suck.
    Or:
      - we could try and do the whole thing on one single thread, possibly with
        the IMAP functions still offloaded to a separate thread.  This might be
        less complex than the multi-engine setup I was thinking of.

Buffer Pools:
    buffer pools are necessary for overlapped IO, like using the Windows IOCP
    API.  That is, on the read side, the kernel needs access to a new read
    buffer *before* the application has a chance to use the read buffer that
    just got filled.  This is good for network throughput, because if the
    network is the slowest part of the system, the best thing you can do is
    make sure that the slowest part runs as fast as possible.  The write side
    is similar.

    Buffer pools are also necessary in between separate nodes of a pipeline.
    That is, if you have a thread pool handling IMAP functions, if you want to
    avoid gratuitous read-stop and read-start operations on the socket, you
    need a pool of buffers that the read pipeline can write to.

    However, what if you have a single, slow, read-heavy IMAP function (such
    as a large message decryption) in flight at the same time as a lot of fast,
    chatty IMAP functions?  The read socket behind the IMAP function could fill
    all of the read-side buffer pool with useless packets.

    Basically, a mechanism that freezes the input to a node is not good enough,
    you need to be able to freeze a whole pipeline upstream from a node, and
    intelligently and efficiently unfreeze it at a later time.  Also, my
    current model for unfreezing the sockets is a FIFO model, but you would
    need a different model for unfreezing specific sockets at will.

    Of course, if you went the select() route (meaning one thread for the whole
    application) then you would avoid all of this, because every read would be
    taken to completion and every write would be taken to completion, always,
    by the single thread.  The single threaded libuv solution would similary
    only require the read buffers and the write buffers, and one thread's slow
    reading would never affect another thread's read-side buffering because
    there would only be one thread.


    OK, I am starting to think that the amount of extra complexity for having
    a multi-threaded-capable event-loop-based function is not worth the extra
    performance gain.

    I think I will build off of libuv (or maybe even just select()??) and just
    write a completely single-threaded application.



Ok... well... shit.  I see that I need a better mechanism for freezing
write pipelines no matter what.  So here goes:


New and improved modular pipelining:
        __________                  _________                  ____________
       |          | READ --------> |         | READ --------> |            |
       |          | <--- READ_DONE |  SOME   | <--- READ_DONE |            |
       | UPSTREAM |                | GENERIC |                | DOWNSTREAM |
       |          | <------- WRITE |   NODE  | <------- WRITE |            |
       |__________| WRITE_DONE --> |_________| WRITE_DONE --> |____________|

    Details:
        After a node sends a READ or a WRITE, it can decide if it needs to
        freeze this context in this direction until further notice.  "Freeze"
        just means it doesn't process any more operations until it receives the
        relevant READ_DONE/WRITE_DONE trigger.

        After handling a READ or a WRITE, it is appropriate to send a READ_DONE
        or WRITE_DONE to the previous node.  These indicate that the read or
        write was processed, and another read or write is possible.

        When a node receives a WRITE_DONE/READ_DONE, it should try and generate
        another WRITE/READ right away, if possible.

    Additional abstractions needed:
        there needs to be a buf_get and a buf_release trigger for nodes to
        get/release buffers from their neighbor nodes


Error handling in the pipeline:
    the node furthest from the network is the node most capable of gracefully
    handling errors, therefore errors should be passed in that direction, and
    not the other way (IE, along read paths and write_done paths, but not
    read_done paths or write paths).

    An example:
      - read error talking to server, but only IMAP engine knows how to send
        a useful message via the other socket to the email client
    A contrapositive example:
      - IMAP engine decides there is an inconsistency and wants to abandon the
        connections.  The libuv thread doesn't give a fuck what the error was.

New bugs:
    how exactly do I determine the "ideal" number of buffers in a pool?
        I'm sure that there is a per-processor number that would be fine, and
        that a per-connection would be too much (in the many-imap-sessions
        case), however, I don't know what that right number would be.  Or there
        might be a per-connection component to it, if you know that every
        connection is always going to have at least one buffer being written to
        in the kernel, for instance.

    Wait... if the socket is closed all of the buffers allocated for cancelled
    reads will be passed to the read callback, right?
        I dunno.  The documentation seems to say no but I don't see how else
        you would free allocated buffers.  Smells like a recipe for a memory
        leak.

    I would like to have a more modular interace between engines.  In
    particular, there's no way to build out a fully-functional, TLS-capable
    fake imap client without borrowing the socket engine and the TLS engine.
    So I am going to just go ahead and engineer it the "right" way, meaning
    that all of the engines operate entirely independent of each other.  This
    will clean up a few parts of the code, such as:

      - the enormous, unweildy imap session context struct
      - poor testability of individual parts
      - overly complicated header dependencies

    READ_DONE and WRITE_DONE hooks are going away.  Instead, engines will
    request generic buffers for writing to from neighboring nodes, and will
    pass the buffers back in READ and WRITE

The NEW new and improved modular pipelining:

    __________                    _________                    ____________
   |          | GET_READ_BUF --> |         | GET_READ_BUF --> |            |
   |          | READ ----------> |  SOME   | READ ----------> |            |
   | UPSTREAM |                  | GENERIC |                  | DOWNSTREAM |
   |          | <- GET_WRITE_BUF |   NODE  | <- GET_WRITE_BUF |            |
   |__________| <--------- WRITE |_________| <--------- WRITE |____________|

    See more details in engine.h


Important hooks:
  - msgs EXPUNGE or folder DELETE should check file permissions to make sure
    command will succeed later
  - we'll need some sort of transaction log to track pending file operations
    in case of powerdown, so on reboot pending expunges,etc. are completed
  - When a box is opened, the recent flags need to be grabbed and put in the
    view of the box, but removed from the underlying box
  - when a box is closed, that is a trigger to check for file-deleting any
    expunged messages
      - Actually, what should really happen is there should be a update_check()
        hook called in several instances.  One such moment is after CHECK
        command.  Also, after receiving certain commands but before responding
        to them, it is allowed to change the message counts.  Those commands
        are any command except (FETCH or STORE or SEARCH).
  - on_message_file_deletion, queue EXPUNGE response for other sessions
  - on_append, queue EXISTS resposne for other sessions
  - on_flag_change queue FETCH responses for other sessions
  - when generating EXISTS, RECENT, or FLAGS responses, the counts for each
    should be calculated at resposne time by comparisons of real data

Deletions:
    I don't like how deletions are looking right now.  I dislike how there is
    a per-message reference count, and how opening a mailbox could mean walking
    through a hashmap of 10,000 or more messages to add references, and the
    same process again to close the mailbox.

    It makes more sense for the imap user session (ixu) to keep track of
    deleted files, and with each deleted file, a list of sessions that may are
    viewing it.  When a session closes the box, that triggers a garbage
    collection of the deletion list, removing session references that the
    deletion depends on.

Updates:
    I feel similarly about calculating updates as I do about deletions.  There
    are scalability issues with the performance of calculating updates on large
    boxes.  The number of updates is likely quite small compared to the size of
    an inbox, and it makes sense to keep some sort of much smaller data
    structure around to push short lists of messages-with-updates for each
    session.

    So a imap session (ixs) would have to keep a running count of recents, and
    number of messages (for EXISTS).  Ok... that is pretty easy.

    # exists (indicates new message in the selected mailbox, even after APPEND)
    # recent (indicates new message, which is marked recent)
    # expunge (indicates a message has been deleted)
    # fetch (flag updates; nothing else is allowed to change in a message)

Message sequences:
    Message indicies are per-session, so should be stored in imap session (view
    of a mailbox), and not in the user session (the "real" mailbox).

    We need to be able to map UID->index numbers, (for generating EXPUNGE
    response after messages are deleted by UID, and also able to dereference
    messages by either index or by UID.  Storing a list of imsg_view pointers
    which point to imsg_view's in a hashmap seems valid, but removing a message
    is expensive (caveat: if message deletion is expensive, it's not the end
    of the world).

        deref msg from UID -> const time
            hashmap lookup
        deref msg from IDX -> const time
            just index into array
        get IDX from UID -> log(N) time (*see below)
            binary search of our sorted UID array
        get UID from IDX -> const time
            hashmap lookup
        delete message -> N time (memmove. *If you wanted to also update a lot
                                  of cached seq numbers in the imsg_view_t's
                                  you would make get-IDX-from-UID const time)


    However, I see that if we modify our self-balancing binary tree to keep
    per-node counts of child nodes, it would be possible to dereference either
    by UID or by index, and easily go from one to the other.

        deref msg from UID -> log(N) time (const time at cost of extra hashmap)
            either walk the binary tree or look up via hashmap
        deref msg from IDX -> log(N) time
        get IDX from UID -> log(N) time
        get UID from IDX -> log(N) time
        delete message -> log(N) time

    In order to optimize for both best-worst-case performance and general
    coolness, I'm choosing option 2: modify jsw_atree to support indexing.

Overall IMAP sequence:
    init loop, tls engine, and imap engine
    loop starts listening for connections
    client A connects
    imap session A initialized
    outgoing connection for imap session (ixs) A created
    client A passes login info for a@splintermail
    login info validated
    user context (ixu) A initialized for a@splintermail
    outgoing connection for ixu A created (to watch the key box)
    now just service commands as you recieve them
    ...

    user context A makes 1 outgoing connection:
        which will watch for updates to the keys mailbox
    client A
        one will sit open watching for updates to the mailbox for syncing keys

    user context A connects remotely, validates credentials
