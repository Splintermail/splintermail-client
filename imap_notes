# IMAP COMMANDS
Status response codes (optional):
    ALERT:
        human readable text, that should be passed to the user
    BADCHARSET
        optionally followed by a parenthesized list of charsets.  A SEARCH
        failed because the given charset is not supported
    CAPABILITY
    PARSE:
        human readable error after trying to parse a message
    PERMANENTFLAGS:
        a list of flags that can be changed permananently (that is, that
        changes persist across sessions)
    READ-ONLY
        A mailbox is selected read-only
    READ-WRITE
    TRYCREATE
        An append or copy attempt is failing because the target mailbox does
        not exist
    UIDNEXT
        indicates the next unique identifier value
    UIDVALIDITY
        indicates the unique identifier validity value
    UNSEEN

OK [response code] human-readable text
  - if tagged, indicates successful completion of a command
  - if untagged, indicates information-only message

NO [response code] human-readable text
  - if tagged, indicates unsuccessful completion of a command
  - if untagged, indicates a warning-only message

BAD [response code] human-readable text
  - if tagged, indicates protocol-level error in client command
  - if untagged, indicates protocol-level error or server error, for which the
    associated client command cannot be determined.

PREAUTH [response code] human-readable text
  - always untagged, possible greeting, indicates no LOGIN is necessary

BYE [response code] human-readable text:
  - may occur as part of the normal logout process, or
  - as a panic shutdown announcement, or
  - as an announcement of an inactivity autologout, or
  - as a greeting, indicating the server is not willing to accept a connection
    from this client
  - In the normal logout case, a client should continue to process commands
    because it is possible the server has not finished processing all commands

(1 = command generates 1 response, * = command generates many responses)

1 CAPABILITY
* LIST (might be multiple)
* LSUB
1 STATUS
1 SEARCH
1 FLAGS (after SELECT or EXAMINE)

* EXISTS (SELECT, EXAMINE, or size change)
* RECENT (SELECT, EXAMINE, or size change)

* EXPUNGE
    Cannot be sent when no command is in progress nor while responding to a
    FEATCH, STORE, or SEARCH command, because client and server would enter a
    race condition where synchronization of message sequence numbers would be
    ambiguous.  If a UID command is in progress, EXPUNGE *can* be sent.

* FETCH
    can be response to a command or a unilateral server data (flag update)

Unilateral data:
    Bye
    untagged OK or NO
    exists (indicates new message in the selected mailbox, even after APPEND)
    recent (indicates new message, which is marked recent)
    expunge (indicates a message has been deleted)
    fetch (flag updates; nothing else is allowed to change in a message)

# parsing issue:

ALERT can be an atom.  So in bison's grammar it is impossible to distinguish
between between an ALERT or an atom, when atom might consist of a single ATOM.

Solutions:
  - add more states to scanner so that only specific keywords are passed at
    specific times
        I dislike this because it would complicate the scanner, and it would
        require a lot more states, and there would have to be synchronization
        between which states are possible in the grammar and in the scanner.
  - add an extra "mask-literal" step after the scanner, where illegal literals
    get turned into ATOMS or ASTR_ATOMS, according to the parser state
        This has the same synchronization problem but it does not complicate
        the scanner.  Also, this is a "whitelist" strategy, not a "blacklist"
        strategy, which should make this less error prone.

What about error handling and parser callbacks?
    Let's look at some different situations:
      - bug or NOMEM in parser code: result in closing the imap session
      - bug or NOMEM in a parser callback: result in closing the imap session
      - unallowed input in parser code: server responds BAD
      - unallowed action in parser callback: server responds NO or BAD

    Let's approach it from categories of error results:
      - Application-ending errors:
          - our listener socket got closed on us maybe?
          - out of sync between connection_cb() and uv_accept()
      - Session-ending errors:
          - Transient, don't notify the user:
              - out-of-memory errors
              - broken connections
          - Non-transient, notify the user:
              -
      - Respondable errors:
          - Invalid IMAP syntax
          -
functional programming to replace bison?

   non-reentrant plan:
    get_command:
        get_tag:
            get_command_name:
            switch(name):
                case FETCH: get_fetch
                ...

What about the reentrant version?
    swtich:
        get_command_mode:
            get_tag_mode
        get_tag_mode:
            scan(TAG, &more)
            if(!more):
                // advance buffer
                get_command_name(tag)
        get_command_name_mode:
            scan(COMMAND, &more)
            if(!more):
                // advance buffer
                switch(name):
                    case FETCH: get_fetch(tag, command)

    Hm.  It seems not undoable, but I think it would be really verbose, and on
    top of that, there's no way to get away from having to implement a stack
    of states.  Frankly, I'd really like to use Bison's implementation.

    Not to mention, the bison error handling seems so much better than anything
    I would want to have to implement.

# header dependencies
imap_scan is self-contained; it has token definitions and scan_mode_t
imap_parse.y calls needs to set parser->scan_mode

imap_expression needs the parser->keep and parser->error


# general DITM architecture for IMAP:

IMAP is not designed to be a request-response architecture but rather a fully
bidirectional pipeline of synchronization information.  Many server "responses"
are not strictly "responses" but rather synchronization info that might be sent
as a response to a client request or unilaterally.  Other "responses" truly are
responses and would have no meaning if sent unilaterally.

Responses which *must* be handled whether or not we requested them (also, it is
probably not worth handling them any other way):

  - Any untagged status response
  - EXISTS
  - RECENT
  - EXPUNGE
  - FETCH (in some cases)

Responses which make no sense if sent unilaterally:
  - Any tagged status response
  - LIST
  - LSUB
  - SEARCH
  - STATUS

Responses which could be treated either way:
  - CAPABILITY
  - SEARCH
  - FLAGS

Therefore, our general architecture will be the following:

                Server                Client
     ________  connections  _______  connections  ________
    |        | <---------- |       | <---------- |        |
    |  MAIL  | <---------- | LOCAL | <---------- | EMAIL  |
    | SERVER | <---------- | STORE |             | CLIENT |
    |________|             |_______|             |________|

With the following details:
  - There is one "unselected" server connection per user, through which all
    LIST and LSUB requests are passed.
  - There is one "selected" server connection per opened LOCAL STORE mailbox.
    If multiple email clients are accessing the same LOCAL STORE's INBOX, only
    one server connection has INBOX "selected".
  - There is one server connection per user which selects the DITM_KEYS mailbox
    DITM uses this box directly and does not let email clients see the box via
    LIST or LSUB or let them SELECT it or CREATE it or anything like that.
  - When a client connection calls an IMAP command, DITM first checks that
    the corresponding server connection has brought the relevant part of the
    LOCAL STORE up-to-date, and then the response is generated locally, not
    bounced off of the mail server, like is generally the case with POP.
  - Exceptions to the "response generated locally" include the LIST, LSUB,
    and (under some circumstances) STATUS commands, as well as any commands
    which alter the server state such as CREATE, DELETE, SUBSCRIBE, and
    UNSUBSCRIBE.
  - There is also another special mailbox, with a user-facing name, perhaps
    "DITM Alerts", which exists only on the LOCAL STORE, through which DITM
    can communicate security alerts or critical failures or whatnot.

data structures:
  - Only one in-memory copy of a user's folder hierarchy, on a per-user basis.
    A per-user data structure associated with a single imap connection keeps
    track of the folders.  Each folder will have to be associated with things
    like a list of pending deletions (which have to be coordinated between
    multiple possible sessions) and a list of pending deletions, stuff like that
  - A per-connection data structure keeps in memory the contents of a single
    folder which that connection is associated with.
  -


Order of work is:
    Make IMAP client capable of logging in
    Make IMAP client capable of mirroring a folder layout
    Make IMAP client capable of syncing a folder
    Make IMAP server, and all that entails...

Right now I'm going to work on:
    Parsing server responses correctly

# Socket / TLS / IMAP engines:

     (libuv)          (OpenSSL)                           (thread pool)
    ________          ________          ________           ___________
   |        | rawin  |        | decin  |        | imapin  |           |
   | SOCKET | -----> |  TLS   | -----> |  IMAP  | ------> |   IMAP    |
   | ENGINE | <----- | ENGINE | <----- | ENGINE | <------ | FUNCTIONS |
   |________| rawout |________| decout |________| imapout |___________|

Memory contexts:
    IMAP user context (ixu)
      - for each user who is connected, run an IDLE session for key updates
      - maintain a count of active connections, so we know when to disconnect
      - allocated when a new user is identified
      - cleaned up when the last of the user's sessions end
      - has list of keys

    IMAP session context: (ixs)
      - allocated by socket engine on connection, or key-related IDLE session
      - lists commands in flight
      - has rawin, rawout buffers (SSL memory buffer type)
      - has decin, decout (dstr_t type)

    IMAP command context: (ixc)
      - allocated by IMAP engine
      - tracks state of command / command-specific variables
      - has imapin, imapout buffers (dstr_t type)

Should the socket engine, the TLS engine, and the IMAP engine be separated?
    Pros:
      - Testability would be much better, as each section of the pipeline could
        be tested individually.
      - Nodes would be modular, and could potentially be strung together in
        different orders (for non-TLS connections, or a STARTTLS variety, for
        example)
    Cons:
      - each engine would need to be separated with a buffer pool, and some
        sort of event queue for passing messages, with checks for freezing
        inputs when a buffer pool is empty and callbacks for unfreezing it.
        There would be a small explosion of callback functions due to lots of
      - different situations arising (such as a allocate_before_accept()
        callback and a cleanup_after_failed_accept() callback, just in the
        libuv connection_cb function)
      - There's not a forseeable need for the improved modularity
      - Far greater complexity, primarily due to the distributed error handling
    Conclusion:
      - No, Socket Engine / TLS Engine / IMAP Engine should be one monolithic
        block, because the improved modularity wouldn't actually be useful and
        the improved testability argument is a wash compared to the massive
        increase in complexity.

buffer pool api
    bufp_init() : allocates memory and sets bufp_empty_cb and bufp_avail_cb
    bufp_get() : returns an available dstr_t*, or calls bufp_empty_cb with data
    bufp_release() : might call bufp_avail_cb with same data

Application write:
    Command lists its own PERCOM struct in the IMAP engine's queue
    IMAP engine takes PERCOM and does whatever conversion, writing to PERCLI
    IMAP engine lists PERCLI in TLS engine's queue
    TLS engine takes PERCLI and does encryption
    TLS engine lists PERCLI in libuv's queue
    Socket engine sends the packet when it can.

Read from socket: (don't know which PERCOM it goes to yet)
    Socket engine reads from a socket, puts data in PERCLI struct
    libuv puts PERCLI in TLS engine's queue, uv_read_stop()
    TLS engine takes PERCLI and does decryption, uv_read_start() and kick loop
    TLS engine puts PERCLI in IMAP Engine's queue
    IMAP Engine decides which PERCOM needs this data, decodes into that PERCOM
    Command handles the data that was read

cleaning of all resources:
    Our pipeline is bidirectional (unavoidable, since libuv and openssl require
    one thread to be in charge of reading and writing).  Therefore, our
    shutdown protol needs to be bidirectional as well, and no node shuts down
    until it has received a "quit" message in both directions:
                              ______
                     downin  |      | downout
                     ------> | some | ------>
                     <------ | node | <------
                      upout  |______|   upin

    One node's shutdown sequence:
      - recieve "quit" message from upstream, on "downin"
      - (downin queue now empty, since nothing more will be sent)
      - pass "quit" downstream, via "downout"
      - (downout queue gets emptied by next link in chain)
      - wait for "quit" to come back up from downstream, via "upin"
      - (upin queue now empty, downstream node has agreed to quit)
      - wait for all writes to be returned from upstream
      - pass "quit" back upstream via "upout"
      - (upout queue now empty, and we can shutdown)

    Overall sequence:
      - libuv node initiates shutdown
      - when libuv receives a "quit" again, that means everything else has
        down_ref()'ed all relevant contexts, and libuv only has to close
        sockets for any remaining contexts, down_ref() them, and exit the loop.

Cleaning up of one connection, at any time:
  - if you call uv_close, and free all associated contexts, you might have
    (for instance) a read callback in the middle of writing to one of those
    contexts.
  - So what you need to do is have each engine promise to ignore the contexts,
    and after each engine promises, then have a callback that frees the
    contexts:
      - socket engine stops reading, closes socket, (ongoing writes are
        cancelled),
      - TLS engine stops processing packets in either direction
      - IMAP stops procesing packets or dispatching functions
      - Wait for any ongoing IMAP functions to complete
      - The last-engine-to-forget could be the one that does the cleanup
      - Or there could be a separate cleaner-upper thread that handles that
    Or:
      - each engine could own the things that it reads from.  Actually, I don't
        think this would solve anything.  It would just suck.
    Or:
      - we could try and do the whole thing on one single thread, possibly with
        the IMAP functions still offloaded to a separate thread.  This might be
        less complex than the multi-engine setup I was thinking of.

Buffer Pools:
    Originally I thought I could use buffers in between nodes the way that I
    did it with POP; the producer always appends to the buffer and the
    consuming thread leaves unconsumed data in the buffer.

    buffer pools are necessary for overlapped IO, like using the Windows IOCP
    API.  That is, on the read side, the kernel needs access to a new read
    buffer *before* the application has a chance to use the read buffer that
    just got filled.  This is good for network throughput, because if the
    network is the slowest part of the system, the best thing you can do is
    make sure that the slowest part runs as fast as possible.  The write side
    is similar.

    Buffer pools are also necessary in between separate nodes of a pipeline.
    That is, if you have a thread pool handling IMAP functions, if you want to
    avoid gratuitous read-stop and read-start operations on the socket, you
    need a pool of buffers that the read pipeline can write to.

    However, what if you have a single, slow, read-heavy IMAP function (such
    as a large message decryption) in flight at the same time as a lot of fast,
    chatty IMAP functions?  The read socket behind the IMAP function could fill
    all of the read-side buffer pool with useless packets.

    Basically, a mechanism that freezes the input to a node is not good enough,
    you need to be able to freeze a whole pipeline upstream from a node, and
    intelligently and efficiently unfreeze it at a later time.  Also, my
    current model for unfreezing the sockets is a FIFO model, but you would
    need a different model for unfreezing specific sockets at will.

    Of course, if you went the select() route (meaning one thread for the whole
    application) then you would avoid all of this, because every read would be
    taken to completion and every write would be taken to completion, always,
    by the single thread.  The single threaded libuv solution would similary
    only require the read buffers and the write buffers, and one thread's slow
    reading would never affect another thread's read-side buffering because
    there would only be one thread.


    OK, I am starting to think that the amount of extra complexity for having
    a multi-threaded-capable event-loop-based function is not worth the extra
    performance gain.

    I think I will build off of libuv (or maybe even just select()??) and just
    write a completely single-threaded application.



Ok... well... shit.  I see that I need a better mechanism for freezing
write pipelines no matter what.  So here goes:


New and improved modular pipelining:

    (see engine.h)

Error handling in the pipeline:
    the node furthest from the network is the node most capable of gracefully
    handling errors, therefore errors should be passed in that direction, and
    not the other way (IE, along read paths and write_done paths, but not
    read_done paths or write paths).

    An example:
      - read error talking to server, but only IMAP engine knows how to send
        a useful message via the other socket to the email client
    A contrapositive example:
      - IMAP engine decides there is an inconsistency and wants to abandon the
        connections.  The libuv thread doesn't give a fuck what the error was.

New bugs:
    how exactly do I determine the "ideal" number of buffers in a pool?
        I'm sure that there is a per-processor number that would be fine, and
        that a per-connection would be too much (in the many-imap-sessions
        case), however, I don't know what that right number would be.  Or there
        might be a per-connection component to it, if you know that every
        connection is always going to have at least one buffer being written to
        in the kernel, for instance.

        Answer: by making buffers belong to engines and not to sessions, the
        answer is clearly a per-processor number; no number of extra sessions
        will make any engine of the pipeline capable of processing more events
        per second.

    Wait... if the socket is closed all of the buffers allocated for cancelled
    reads will be passed to the read callback, right?
        I dunno.  The documentation seems to say no but I don't see how else
        you would free allocated buffers.  Smells like a recipe for a memory
        leak.

    I would like to have a more modular interace between engines.  In
    particular, there's no way to build out a fully-functional, TLS-capable
    fake imap client without borrowing the socket engine and the TLS engine.
    So I am going to just go ahead and engineer it the *right* way, meaning
    that all of the engines operate entirely independent of each other.  This
    will clean up a few parts of the code, such as:

      - the enormous, unweildy imap session context struct
      - poor testability of individual parts
      - overly complicated header dependencies

Error handling during reads and writes:

    Principles of correct error handling:

      - Errors need to bubble to higher levels of the stack.  The IMAP engine
        knows how to tell the user there is a broken socket but the socket
        doesn't know shit about a broken IMAP session.
      - Errors are not allowed to skip layers.  Intermediate layers of the
        application should be capable to take corrective action, even if they
        normally just pass the error along.

    Note that errors from the read side of the pipeline are passed in the the
    READ step, but write errors are passed in the WRITE_DONE step.  As a
    result, there are some additional aspects we need to enforce on writes
    which happen automatically for reads:

      - Synchronous writes should be possible.  An application should be able
        to write, and not write again until it is sure that write succeeded.
      - Synchronous write should not be necessary.  An application capable of
        handling errors asynchronously should be able to write asynchronously.

    As a consequence of those requirements, it becomes possible to pipeline
    reads (each engine at each step can process it's data and pass that to the
    next engine) but writes must be stacked (layer A which receives a WRITE
    cannot respond with WRITE_DONE until *after* the layer below it passes back
    a WRITE_DONE):

        R = "read", D = "read done", W = "write", D = "write done"

    Engine | Timeline of read and write.  Read is pipelined, write is stacked
    -------+-----------------------------------------------------------------
    Socket | R                      D
    TLS    |   R D                W   D
    IMAP   |     R D            W       D
    ...    |       R D        W           D
    ...    |         R D    W               D

    Engine |  Correct error handling is maintained (E = "error")
    -------+----------------------------------------------------
    Socket | R                      E
    TLS    |   R D                W   E
    IMAP   |     R D            W       E
    ...    |       E D        W           E
    ...    |         E D    W               E

    In order to achieve the same processing parallelism, the write side has to
    contain many more buffers than the read side.  This requirement worsens the
    longer the pipe becomes.  See this table of equivalent-parallelism buffer
    counts:

                  number or bufs,   number of bufs,
        Engine:   pipeline case:    stack case:
        -------------------------------------------------
        socket:         0                 0
        TLS:            5                 5
        IMAP:           5                10
        EngineX:        5                15
        EngineY:        5                20

    Essentially, for the EngineY to saturate the entire pipe in the pipeline
    case, it would be able to write 20 times (the count of all buffers in all
    engines of the pipeline).  In the stacked case, that would require 20
    separate buffers at the EngineY level, 15 at the EngineY level, etc.

    The clear alternative to this strategy is to separate returning the write
    buffer from the error handling.  One easy way would be to allocate error
    messages on the heap when they are detected and pass them via a different
    channel.  However, I have gone through great lengths to avoid meta-error-
    handling, and I am not going to abandon that now.

    Another possibility would be to separate the write buffers and the write
    error checking.  We would still need more and more error checking elements
    as you went down the pipeline.  I think since we are realistically talking
    about less than a MB of memory this is really just a non-issue, and the
    stacked write buffers is acceptable.

References:
    The only thing that we count references for (right now) is a session.  A
    session primarily unifies a group of reads/writes as belonging to each
    other and stores data about them.  Each engine contributes its own
    self-contained child struct to a session, on which only it can operate.
    Since multiple engines might have references to the session at any time, a
    reference count is a natural solution.

    To keep things simple, we will consider that there are only mainly two
    kinds of references: references associated with an event_t (like a read or
    a write), and references associated with some engine specific data (like a
    socket that belongs to a session).  In the first case, session counting
    looks like this:

        EV_READ, EV_WRITE: upref session before passing event
        EV_READ_DONE: (carries no error) downref session after receiving event*
        EV_WRITE_DONE: (carries error) downref session after receiving event
        EV_QUIT_DOWN, EV_QUIT_UP: carries no session information

    (*The reason that EV_READ_DONE has session information is to enable the
    possibility of a per-session event limit.  For example, if at most two
    reads were allowed to be associated with a given session then you would
    need to know which session an EV_READ_DONE came from.  Otherwise it would
    make more sense to drop the session information before passing back the
    EV_READ_DONE event.)

    In the second case, for references associated with some engine specific
    data, a call to abort a session results in the session calling the
    engine-data-specific mechanism for freeing the engine data, which should
    eventually downref the session.  It may be a while before the session is
    downref'd, such as with the socket engine, where closing the loop_data
    triggers the event loop to close the socket on-thread, and then at a later
    time in a close_cb the reference finally gets downref'd.

Life of a per-session data struct:

    session_allocator() -- called from any thread
        The session is allocated.  All engine-specific getters (such as
        session_get_ssl_ctx() for the tls engine) are prepared and all engine
        data structs are zeroed before calling engine_data_start() for each
        engine data struct.  The reason for that is that so that each engine
        can lazily init its own engine data struct, as after the first
        engine_data_start() call events for this session can be passed at any
        time.  Each engine_data_start() can pass a SESSION_START message to
        its engine as needed, enabling on-thread initialization.

        There should be at least one engine which opens a standing reference to
        the session (otherwise there's not much point in having a session), so
        generally the session_allocator() starts the session with one reference
        but downrefs the session after each engine_data_start has been called.

    session_close() -- called from any thread
        Can be called anytime after session_allocator calls the first
        engine_data_start().  This will queue a SESSION_CLOSE event on each
        engine for that engine's session data, enable on-thread resource
        cleanup.

    SESSION_START event:
        This is like a reminder, in case no normal events come in, to trigger
        an engine to look at a session.  Things like starting TCP connections
        or starting TLS handshakes might only work with this trigger, though if
        any normal events for this session are received before this event,
        those events should trigger the engine data initialization and this
        event should be ignored.  Not all engine data init()s will generate a
        SESSION_START, but any pipeline should have at least one node which
        needs a SESSION_START event to guarantee correct behavior, because all
        sessions either need a connection accept()'ed or they need somebody to
        pass the first data packet.  Since those things likely have to happen
        on-thread, SESSION_START is the trigger required.

    SESSION_CLOSE event:
        Triggers an engine to mark the session as closed and to free all
        resources this engine's data struct.  After the last ref_down the
        whole session will be freed.  Normal events recieved after a
        SESSION_CLOSE should not be processed (since the engine doesn't have
        the state in memory required to process them anymore).

    normal events, associated with a session:
        guaranteed to come after the engine data was initialized with zeros,
        but *not* guaranteed to come before the SESSION_START event.  Also,
        it is possible to receive more normal events *after* a SESSION_CLOSE.

    Potential Gotcha:
        Right now, I don't have a solution to guarantee that SESSION_START
        doesn't triggers an eventual SESSION_CLOSE on a neighbor node before
        the SESSION_START is even passed to that same neighbor (for example, if
        a quick tlse_data_init() fails, calling session_close() which pushes
        a SESSION_CLOSE event to the IMAP engine before the session allocator
        can call imape_data_init() to push the SESSION_START).  That means
        that you could actually recieve SESSION_CLOSE *before* you get
        SESSION_START.  We should be careful to implement that correctly.

    Properties of certain calls in each engine:
        engine_data_start():
            Only called once, from off thread, in the session_alloc.  This
            function should only send a SESSION_START event to the engine
            thread (with a corresponding ref_up).
        engine_data_onthread_start():
            Called from on thread, the first time a packet is seen for the
            session.  It will often be after the SESSION_START packet.   The
            arguments which are needed from the session must use getters to
            avoid a race condition during initialization of multiple engines.
        engine_data_close():
            Called from off thread, the first time session_close() is called.
            session_close() guarantees it engine_data_close() will only be
            called once.  This function should only send a SESSION_CLOSE to
            the engine thread
        engine_data_onthread_close():
            Called from on thread, and must be robust to being called many
            times.  In particular, in error situations, the engine which calls
            session_close() will often immediately call its own
            engine_data_onthread_close() to ensure no more packets are handled
            for that engine.  Then later it will receive its SESSION_CLOSE
            message and call engine_data_onthread_close() again.

    Error handling during session initialization:
        How does the error get passed?
            It makes a lot of sense to pass it through SESSION_CLOSE, since
            there will already be a dedicated event for any engine that might
            need to see the error.
        Who needs to see the error?
            Probably just always hand it to the highest application layer
        How is the error handled?
            Errors from incoming connections are just logged; no other user
            session need to be bothered.  Errors from outgoing sessions should
            be handled by whatever thing asked for the session to be created.

Life of a session:
    Sessions can be started in one of two ways:
      - a connecting session is created when we are acting as the client
      - an accepting session is created when we are acting as the server

    Connecting session:
      - The allocation of the session will come from outside of the pipeline.
        Some thread will call a session_alloc_connect() and feed in the
        mgr_data field for the session_destroyed callback.
      - The loop engine will use a SESSION_START event to trigger trying to
        make a connection.  Any packets which the loop receives before making
        the connection are buffered up in preconnected writes and written upon
        making the connection.
      - The first packet in the pipeline will come from the tls engine.  The
        tls engine starts the handshake (client side starts) then the remote
        imap server will send its hello.
      - we may or may not know which user the connection belongs to.  If we do
        know we probably can expect that the credentials are valid, because the
        connection is one that we initiated.  Of course, we always have to
        handle either connection.

    Accepting session:
      - The connection_cb for the listening socket will make a callback which
        will allocate the session.


Important hooks:
  - msgs EXPUNGE or folder DELETE should check file permissions to make sure
    command will succeed later
  - we'll need some sort of transaction log to track pending file operations
    in case of powerdown, so on reboot pending expunges,etc. are completed
  - When a box is opened, the recent flags need to be grabbed and put in the
    view of the box, but removed from the underlying box
  - when a box is closed, that is a trigger to check for file-deleting any
    expunged messages
      - Actually, what should really happen is there should be a update_check()
        hook called in several instances.  One such moment is after CHECK
        command.  Also, after receiving certain commands but before responding
        to them, it is allowed to change the message counts.  Those commands
        are any command except (FETCH or STORE or SEARCH).
  - on_message_file_deletion, queue EXPUNGE response for other sessions
  - on_append, queue EXISTS resposne for other sessions
  - on_flag_change queue FETCH responses for other sessions
  - when generating EXISTS, RECENT, or FLAGS responses, the counts for each
    should be calculated at resposne time by comparisons of real data

Deletions:
    I don't like how deletions are looking right now.  I dislike how there is
    a per-message reference count, and how opening a mailbox could mean walking
    through a hashmap of 10,000 or more messages to add references, and the
    same process again to close the mailbox.

    It makes more sense for the imap user session (ixu) to keep track of
    deleted files, and with each deleted file, a list of sessions that may are
    viewing it.  When a session closes the box, that triggers a garbage
    collection of the deletion list, removing session references that the
    deletion depends on.

Updates:
    I feel similarly about calculating updates as I do about deletions.  There
    are scalability issues with the performance of calculating updates on large
    boxes.  The number of updates is likely quite small compared to the size of
    an inbox, and it makes sense to keep some sort of much smaller data
    structure around to push short lists of messages-with-updates for each
    session.

    So a imap session (ixs) would have to keep a running count of recents, and
    number of messages (for EXISTS).  Ok... that is pretty easy.

    # exists (indicates new message in the selected mailbox, even after APPEND)
    # recent (indicates new message, which is marked recent)
    # expunge (indicates a message has been deleted)
    # fetch (flag updates; nothing else is allowed to change in a message)

Message sequences:
    Message indicies are per-session, so should be stored in imap session (view
    of a mailbox), and not in the user session (the "real" mailbox).

    We need to be able to map UID->index numbers, (for generating EXPUNGE
    response after messages are deleted by UID, and also able to dereference
    messages by either index or by UID.  Storing a list of imsg_view pointers
    which point to imsg_view's in a hashmap seems valid, but removing a message
    is expensive (caveat: if message deletion is expensive, it's not the end
    of the world).

        deref msg from UID -> const time
            hashmap lookup
        deref msg from IDX -> const time
            just index into array
        get IDX from UID -> log(N) time (*see below)
            binary search of our sorted UID array
        get UID from IDX -> const time
            hashmap lookup
        delete message -> N time (memmove. *If you wanted to also update a lot
                                  of cached seq numbers in the imsg_view_t's
                                  you would make get-IDX-from-UID const time)


    However, I see that if we modify our self-balancing binary tree to keep
    per-node counts of child nodes, it would be possible to dereference either
    by UID or by index, and easily go from one to the other.

        deref msg from UID -> log(N) time (const time at cost of extra hashmap)
            either walk the binary tree or look up via hashmap
        deref msg from IDX -> log(N) time
        get IDX from UID -> log(N) time
        get UID from IDX -> log(N) time
        delete message -> log(N) time

    In order to optimize for both best-worst-case performance and general
    coolness, I'm choosing option 2: modify jsw_atree to support indexing.

Overall IMAP sequence:
    init loop, tls engine, and imap engine
    loop starts listening for connections
    client A connects
    imap session A initialized
    outgoing connection for imap session (ixs) A created
    client A passes login info for a@splintermail
    login info validated
    user context (ixu) A initialized for a@splintermail
    outgoing connection for ixu A created (to watch the key box)
    now just service commands as you recieve them
    ...

    user context A makes 1 outgoing connection:
        which will watch for updates to the keys mailbox
    client A
        one will sit open watching for updates to the mailbox for syncing keys

    user context A connects remotely, validates credentials

Separation between lower- and higher-level IMAP logic:

    The IMAP engine deals with the full range of IMAP hooks, but it can
    simplify the stream of events for a higher-level logic element.  A higher-
    level element might be the mutt synchronizer, or the DITM server.

    Low-level tasks that the IMAP engine can automate:
      (for upwards connections)
      - check for new folders
            returns a list of new folders
      - check folder for updates
            returns updated flags and list of new message UIDs
      - download message X
            returns updated flags and list of new message UIDs

      (for downwards connections)
      - just be an imap server
            higher-level logic receives hooks that server can't handle alone
      - deal with some updates in the folder you have selected
            no return, server just alerts its client when appropriate

imap_controller_t: the thing that tells imap_session what to do.
    For imap clients, this/these would be scoped to main().  Multiple clients
    could exist in a single application and would just own separate,
    non-interacting imap sessions.  For an imap server, this would be scoped to
    a listener_spec_t; multiple server applications could coexist on separate,
    non-interacting listening sockets.

    For clients or sessions with multiple, non-interacting bundles of imap
    sessions for multiple user accounts, the list of per-user structs should
    exist within the overall imap_controller_t logic.

imap_hooks_dn_t:
    Define the imap server behavior.  These should be the same for all servers;
    it is just "serve up the folder that we have downloaded and decrypted in
    an IMAP-compliant way".  These should be built into the imap_engine.

    The only special behaviors will be:
      - protecting the special ditm_keys folder

imap_hooks_up_t:
    This has two varieties:
      - a clone/decrypt variety
      - a watch-the-keys-folder variety
    It should be possible to define one set of hooks and have a few callbacks
    like "message_added" which are ignored by sessions which just do cloning
    (since the underlying maildir will handle them properly), but which trigger
    special actions in the watch-the-key-folder sessions.

Question:
    should imap_engine be it's own thread and separated from the
    higher-level logic?  Or should the higher level logic be on the same
    thread?

    Same thread pros:
      - no synchronization issues
      - no separate (complex) events for each high-level imap functionality

    own thread pros:
      - imap engine is free to choose to give more priority to surprise reads
      - session becomes the proper interface between the two levels
      - high-level logic doesn't need to integrate with session closing

      I like the own-thread pros better.  I like the cleaner architecture and
      the event passing between the two won't be a complete rewrite of the imap
      hooks because the higher level logic only needs a few actions (I think).

