#!/usr/bin/env python3
"""
gen.py: a recursive-descent parser generator, optimized for simplicity and
        fully in the public domain.

gen.py has embedded C and Python generators, but it will be possible in the
future to write a custom generator that is loaded dynamically by gen.py, so
any language should be supportable.

Both the embedded C parser and the embedded Python generate reentrant push
parsers.  gen.py's plugin architecture doesn't place requirements on the
properties of language generators, so custom language generators are allowed to
be simpler than the embedded generators, as desired.

gen.py is in alpha, and is still missing some key features.

TODO:
 - Python:
   - honor prematched to avoid certain duplicate token checks
   - reject roots which don't have a concrete ending token; the user api
     doesn't support it

 - C:
   - reject nested recovery blocks
   - reject roots which don't have a concrete ending token; the user api
     doesn't support it
   - header ordering vs body ordering is awkward.  E.g. you need some things
     to occur after header code but before body code (error handlers).
     Currently only solvable with extra-forward declarations (a hack) or with
     separate header and body generation (shouldn't be a hard requirement)

 - Grammar:
   - use source-order everywhere
   - configurable strictness for fallback-caused first/first, first/follow
   - support paramaterized expressions to reduce side-effect duplication:

       addr_spec = TEXT ARUBA TEXT;          # user@domain
       angle_addr = LANGLE addr_spec RANGLE; # <user@domain>
       named_addr = TEXT angle_addr;         # "Name" <user@domain>
       # conflict, TEXT might mean addr_spec or named_addr:
       mailbox = addr_spec | angle_addr | named_addr;

       # solved:
       _addr_spec (text) = ARUBA TEXT;
       angle_addr = LANGLE addr_spec RANGLE;
       _named_addr (text) = angle_addr;

       mailbox =
        | TEXT:t (
            | _addr_spec(t)
            | _named_addr(t)
          )
        | angle_addr
       ;

       # if you need the bare objects somewhere else:
       addr_spec = TEXT:t _addr_spec(t);
       named_addr = TEXT:t _named_addr(t)

 - Meta parser:
   - support an %include directive?
   - only create Snippets when the tag matches (no more generator-side filter)

 - gen.py cli:
   - support importing generators from external python files
   - allow for input templates, so you can have one file with an embedded grammar
   - configurable strictness for fallback-caused first/first, first/follow
   - come up with a better name than gen.py
"""

import abc
import argparse
import contextlib
import io
import re
import sys
import textwrap
import string

def render_location(loc, body, underline=True):
    lines = body.splitlines()

    # Note that values in loc are always 1-indexed.
    if loc.start.line == -1:
        start_line = len(lines)-1
        start_char = len(lines[-1])
        end_line = len(lines)
        end_char = len(lines[-1]) + 1
        out = "end-of-file:\n"
    else:
        start_line = loc.start.line - 1
        start_char = loc.start.char - 1
        end_line = loc.end.line
        end_char = loc.end.char
        out = "line=%d, char=%d:\n"%(loc.start.line, loc.start.char)

    for i in range(start_line, end_line):
        line = lines[i]
        out += line + "\n"
        if not underline:
            continue
        caret_start = start_char if i == start_line else 0
        caret_end = end_char if i+1 == end_line else len(line)
        out += " "*caret_start + "^"*(caret_end - caret_start) + "\n"

    return out.rstrip("\n")

class RenderedError(Exception):
    pass

class GrammarError(Exception):
    def __init__(self, text=None):
        self.text = text

    def __str__(self):
        if self.text is not None:
            return self.render(self.text)
        return super().__str__()

class FirstFirst(GrammarError):
    def __init__(self, parent, conflicts):
        self.parent = parent
        self.conflicts = conflicts
        super().__init__()

    def render(self, body):
        out = "FIRST/FIRST conflict detected in (%s):\n"%self.parent.summary()
        if self.parent.loc is not None:
            out += "at " + render_location(self.parent.loc, body) + "\n"

        for branchlist, tokens in self.conflicts.items():
            out += "token(s) [%s] "%(", ".join(tokens))
            out += "match multiple branches:\n"
            for b in branchlist:
                out += "  - " + b.summary() + "\n"

        return out.rstrip('\n')

class FirstFollow(GrammarError):
    def __init__(self, term, conflicts, due_to_fallbacks=False):
        self.term = term
        self.conflicts = conflicts
        self.due_to_fallbacks = due_to_fallbacks
        super().__init__()

    def __str__(self):
        if self.text is not None:
            return self.render(self.text)
        return super().__str__()

    def render(self, body):
        out = "FIRST/FOLLOW conflicts: [%s]\n"%(",".join(self.conflicts))
        if self.term.loc is not None:
            out += "at " + render_location(self.term.loc, body) + "\n"
        out += "at least one token that starts %s is disallowed by then."%(
            self.term.summary()
        )
        if self.due_to_fallbacks:
            out += "\n(note: this conflict only arises due to fallbacks)"
        return out

class InfiniteRecursion(Exception):
    def __init__(self, path, *args):
        self.path = path
        super().__init__(*args)

class PossiblyEmpty(GrammarError):
    def __init__(self, msg, summary, loc):
        self.msg = msg
        self.summary = summary
        self.loc = loc
        super().__init__()

    def render(self, body):
        out = "invalid %s on maybe-empty expression (%s)"%(
            self.msg, self.summary
        )
        if self.loc is not None:
            out += "\n" + render_location(self.loc, body)
        return out


class Fallbacks:
    """
    The result of all of the %fallback directives in a grammar.
    This is handled by the Grammar itself, rather than having every parser
    generator handle it.  This allows for conflict detection and it makes the
    generators a little easier to write.
    """
    def __init__(self, fallbackmap):
        self.fallbackmap = fallbackmap

    def __getitem__(self, value):
        """
        Return the set of all fallbacks that apply to a given token.
        """
        return self.fallbackmap[value]

    def get(self, key, default=None):
        return self.fallbackmap.get(key, default)

    def fallback_set(self, primary, exclude):
        """
        Return the fallbacks that apply to a given primary, in the presence
        of the other possible primary matches 'exclude'.

        Note that primary can appear in exclude without affecting the output.

        Also note that exclusions are hierarchical.

        Example: with the following fallback directives:

            %fallback LETTER A B C;
            %fallback WORD LETTER;

        EXPRESSION      PRIMARY  EXCLUDE         RESULT
        ----------------------------------------------------------
        WORD            WORD     [WORD]          {LETTER, A, B, C}

        WORD | A        WORD     [WORD, A]       {LETTER, B, C}
        WORD | A        A        [WORD, A]       {}

        WORD | LETTER   WORD     [WORD, LETTER]  {}
        WORD | LETTER   LETTER   [WORD, LETTER]  {A, B, C}
        """
        out = self.fallbackmap[primary]
        out = out.difference(set(exclude))
        for ex in exclude:
            if ex == primary:
                # allow primary to appear in exclude, but ignore it.
                continue
            if primary in self.fallbackmap[ex]:
                # general things cannot cause exclusions on specific things
                continue
            out = out.difference(self.fallbackmap[ex])
        return out

    def all_fallbacks(self, tokens, exclude=()):
        """
        Get a list of all fallbacks for some set of tokens.
        """
        out = set()
        # as with fallback_set(), ignore excluded primaries
        exclude = set(exclude).difference(set(tokens))
        for t in tokens:
            if t is not None:
                out = out.union(self.fallback_set(t, exclude))
        # primaries are never, ever fallbacks
        out = out.difference(set(tokens))
        return out


def extract_fallbacks(parsed_fallbacks, grammar, tag):
    fallbacklist = []
    to_types = set()
    fallback_keys = []
    for f in parsed_fallbacks:
        if f.tag not in (None, tag):
            continue
        assert f.to_type not in to_types, (
            "got duplicate %%fallback %s ..."%f.to_type
        )
        assert f.to_type in grammar.exprs, (
            "'%%fallback %s ...': %s is not a known token type"%(
                f.to_type, f.to_type
            )
        )
        assert isinstance(grammar.exprs[f.to_type], Token), (
            "'%%fallback %s ...': %s is an expression, not a token"%(
                f.to_type, f.to_type
            )
        )
        to_types.add(f.to_type)
        for from_type in f.from_types:
            assert from_type in grammar.exprs, (
                "'%%fallback %s %s': %s is not a known token type"%(
                    f.to_type, from_type, from_type
                )
            )
            assert isinstance(grammar.exprs[from_type], Token), (
                "'%%fallback %s %s': %s is an expression, not a token"%(
                    f.to_type, from_type, from_type
                )
            )
        fallbacklist.append((f.to_type, f.from_types))

    fallbackmap = {to: froms for to, froms in fallbacklist}
    for token_name, _ in grammar.sorted_tokens():
        # no missing entries in fallbackmap
        fallbackmap.setdefault(token_name, [])

    # fallbacks should be modeled as a tree, i.e. any type can only fallback
    # into one parent type (though that parent type might itself fallback to
    # another type)
    from_type_map = {}
    for to_type, from_types in fallbackmap.items():
        for from_type in from_types:
            from_type_map.setdefault(from_type, []).append(to_type)
    conflicts = {f: t for f, t in from_type_map.items() if len(t) > 1}
    if conflicts:
        msg = "Detected types which can %fallback to multiple other types:\n"
        for from_type, to_types in conflicts.items():
            msg += "  - [%s] <- %s\n"%(", ".join(to_types), from_type)
        raise RenderedError(msg.rstrip('\n'))

    def detect_circular_fallbacks(fallbackmap, to_type=None, path=None):
        if to_type is None:
            # entrypoint to recursion
            for to_type in fallbackmap:
                detect_circular_fallbacks(fallbackmap, to_type, (to_type,))
            return

        # inside of recursion
        for from_type in fallbackmap[to_type]:
            # to be the most helpful, only print when we have the exact loop
            if from_type == path[0]:
                path = (*path, from_type)
                raise RenderedError(
                    "detected circular %%fallback: %s"%(" <- ".join(path))
                )
            # but always quit the recursion after any loop
            if from_type in path:
                return
            detect_circular_fallbacks(fallbackmap, from_type, (*path, from_type))

    detect_circular_fallbacks(fallbackmap)

    # expand fallbacks, convert to sets
    def get_recursive(fallbackmap, value):
        from_types = fallbackmap[value]
        out = set(from_types)
        for from_type in from_types:
            out = out.union(get_recursive(fallbackmap, from_type))
        return out

    expanded = {t: get_recursive(fallbackmap, t) for t in fallbackmap}
    return expanded


def add_to_prev(prev, name, call):
    if prev is None:
        prev = []
    if name in prev:
        raise InfiniteRecursion(
            list(prev) + [name],
            "circular " + call + "() detected in " + name + ": " + str(prev)
        )
    return list(prev) + [name]

def cacheable(*args):
    if len(args) == 1 and callable(args[0]):
        # direct call case
        fn = args[0]
        prop = "_cached_" + fn.__name__
        def _fn(self, *args, **kwargs):
            if not hasattr(self, prop):
                setattr(self, prop, fn(self, *args, **kwargs))
            return getattr(self, prop)
        return _fn
    else:
        # parameterizable cache case
        # make sure you declare fn(self, ..., *, PARAM=None) or it won't work
        # inspect would be better but this works for our use-case
        def decorator(fn):
            keys = args
            prop = "_cached_" + fn.__name__
            def _fn(self, *args, **kwargs):
                if not hasattr(self, prop):
                    setattr(self, prop, {})
                cache = getattr(self, prop)
                cache_key = frozenset(kwargs.get(k) for k in keys)
                if cache_key not in cache:
                    cache[cache_key] = fn(self, *args, **kwargs)
                return cache[cache_key]
            return _fn
        return decorator


# When printing summaries, if you want to print something with a lower
# priority than your surrounding context, you need parens.
PRIORITY_UNAMBIGUOUS = 0
PRIORITY_BRANCHES = 1
PRIORITY_SEQUENCE = 2
PRIORITY_MULTIPLIER = 3


class Definition:
    """Only Tokens and Expressions are Definitions"""
    name: str


class Token(Definition):
    def __init__(self, name, typ=None, loc=None):
        self.name = name
        self.type = typ
        self.loc = loc


class Parsable(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        """Print a human-readable summary of this grammar element."""
        pass

    @abc.abstractmethod
    def state_map(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        """
        Return a dict mapping the Parsable you are currently parsing to a
        string representation of your parsing location.
        """
        pass

    @abc.abstractmethod
    def get_first(self, prev=None):
        pass

    @abc.abstractmethod
    def get_disallowed_after(self, prev=None):
        pass

    @abc.abstractmethod
    def check(self, *, fallbacks=None):
        pass


class Reference(Parsable):
    """
    Points to a Definition (either a Token or an Experiment).

    Token.loc and Expriment.loc point to definitions, but Reference.loc points
    to the location of a reference.
    """
    def __init__(self, defn, loc=None):
        assert isinstance(defn, (Token, Expression)), type(defn).__name__
        self.defn = defn
        self.loc = loc
        self.prematched = None

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return self.defn.name

    def state_map(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return {self: ". " + self.defn.name}

    @cacheable
    def get_first(self, prev=None):
        if isinstance(self.defn, Token):
            return {self.defn.name}
        return self.defn.seq.get_first(prev)

    @cacheable
    def get_disallowed_after(self, prev=None):
        if isinstance(self.defn, Token):
            return set()
        return self.defn.seq.get_disallowed_after(prev)

    @cacheable("fallbacks")
    def check(self, *, fallbacks=None):
        # The reference itself needs no checking.
        return


class Snippet:
    def __init__(self, text, tag=None, loc=None):
        self.text = text
        self.tag = tag
        self.loc = loc


class Repeat(Parsable):
    def __init__(self, name, rmin, rmax, loc=None):
        self.name = name
        if rmax is not None and rmin > rmax:
            raise ValueError(
                "rmin must not exceed rmax (rmin=%d, rmax=%d)"%(rmin, rmax)
            )
        if rmin < 0:
            raise ValueError(
                "rmin must be non-negative (rmin=%d)"%rmin
            )
        if rmax == 0:
            raise ValueError(
                "rmax must be greater than zero (rmax=%d)"%rmax
            )
        self.rmin = rmin
        self.rmax = rmax
        self.seq = Sequence(self.name + ".seq")
        self.loc = loc
        # When implementing generators, there are certain properties of
        # certain cases which make them easier to generate code for:
        if (rmin, rmax) == (0, 1):
            # maybe case: only branch logic, no repeat or counting
            self.type = "maybe"
        elif (rmin, rmax) == (0, None):
            # zero-or-more case: repeat and branch logic, but no counting
            self.type = "zero_or_more"
        else:
            # general case: repeat and branch and counting required
            self.type = "general"

        if (rmin, rmax) == (0, 1):
            self.err_op = "maybe operator"
        else:
            self.err_op = "repetition operator"
        self.prematched = None

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None, loc=None):
        self.seq.add_code(code, tag, loc)

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        if self.type == "maybe":
            return "[" + self.seq.summary(PRIORITY_UNAMBIGUOUS) + "]"
        if self.type == "zero_or_more":
            return "*" + self.seq.summary(PRIORITY_MULTIPLIER)
        if self.rmax is None:
            # an "N or more" statement
            return "%d*"%self.rmin + self.seq.summary(PRIORITY_MULTIPLIER)
        # an "N to M" statement
        return "%d*%d(%s)"%(
            self.rmin, self.rmax, self.seq.summary(PRIORITY_UNAMBIGUOUS)
        )

    def state_map(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        if self.type == "maybe":
            pre, post = "[", "]"
            parent_priority = PRIORITY_UNAMBIGUOUS
        elif self.type == "zero_or_more":
            pre, post = "*", ""
            parent_priority = PRIORITY_MULTIPLIER
        elif self.rmax is None:
            # an "N or more" statement
            pre, post = "%d*"%self.rmin, ""
            parent_priority = PRIORITY_MULTIPLIER
        else:
            # an "N to M" statement
            pre, post = "%d*%d"%(self.rmin, self.rmax), ""
            parent_priority = PRIORITY_MULTIPLIER
        out = {self: ". " + self.summary()}
        for k, v in self.seq.state_map(parent_priority).items():
            out[k] = pre + v + post
        return out

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        if None in first:
            raise PossiblyEmpty(self.err_op, self.seq.summary(), self.loc)
        if self.rmin == 0:
            # include None to indicate a maybe-empty expresssion
            first = first.union({None})
        return first

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        if None in disallowed:
            raise PossiblyEmpty(self.err_op, self.seq.summary(), self.loc)
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first(prev)
        if None in first:
            raise PossiblyEmpty(self.err_op, self.seq.summary(), self.loc)
        if self.rmin != self.rmax:
            # variable repeat: disallow our own first
            disallowed = disallowed.union(first)
        if self.rmin == 0:
            # include None to indicate a maybe-empty expresssion
            disallowed = disallowed.union({None})
        return disallowed

    @cacheable("fallbacks")
    def check(self, *, fallbacks=None):
        self.seq.check(fallbacks=fallbacks)


class Branches(Parsable):
    def __init__(self, name, loc=None):
        self.name = name
        self.branches = []
        self._branch = None
        self.loc = loc
        self.prematched = None

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        out = " | ".join(b.summary(PRIORITY_BRANCHES) for b in self.branches)
        if parent_priority >= PRIORITY_BRANCHES:
            out = "(" + out + ")"
        return out

    def state_map(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        out = {self: ". (" + self.summary() + ")"}
        for i, b in enumerate(self.branches):
            pre = " | ".join(bb.summary(PRIORITY_SEQUENCE) for bb in self.branches[:i])
            if pre:
                pre += " | "
            post = " | ".join(bb.summary(PRIORITY_SEQUENCE) for bb in self.branches[i+1:])
            if post:
                post = " | " + post

            if parent_priority >= PRIORITY_BRANCHES:
                pre = "(" + pre
                post = post + ")"

            for k, v in b.state_map(PRIORITY_BRANCHES).items():
                out[k] = pre + v + post
        return out

    def add_term(self, term, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_term(term, tag)

    def add_code(self, code, tag=None, loc=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_code(code, tag, loc)

    @contextlib.contextmanager
    def branch(self):
        assert self._branch is None, "can't nest calls to branch"
        self._branch = Sequence(self.name + '[' + str(len(self.branches)) + ']')
        yield
        self.branches.append(self._branch)
        self._branch = None

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for branch in self.branches:
            first = first.union(branch.get_first(list(prev)))

        return frozenset(first)

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        prev = list(prev) + [self.name]
        disallowed = set()
        for branch in self.branches:
            disallowed = disallowed.union(
                branch.get_disallowed_after(list(prev))
            )
        return frozenset(disallowed)

    @cacheable("fallbacks")
    def check(self, *, fallbacks=None):
        # if any tokens map to multiple branches, that's a first/first conflict
        token_to_branch = {}
        for branch in self.branches:
            branch.check(fallbacks=fallbacks)

            # no possibly-empty branches
            branch_first = branch.get_first()
            if None in branch_first:
                raise PossiblyEmpty("branch", branch.summary(), branch.loc)

            for t in branch_first:
                token_to_branch.setdefault(t, []).append(branch)

        conflicts= {}
        for t, branchlist in token_to_branch.items():
            if len(branchlist) > 1:
                conflicts.setdefault(tuple(branchlist), []).append(t)
        if conflicts:
            raise FirstFirst(self, conflicts)


class Recovery(Parsable):
    def __init__(self, name, code=None, tag=None, loc=None):
        self.name = name
        # allow defining the recovery code upfront
        self.code = [] if code is None else [Snippet(code, tag)]
        self.seq = Sequence(self.name + '.seq')
        self.after = None
        self.loc = loc
        self.prematched = None

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None, loc=None):
        self.seq.add_code(code, tag, loc)

    def add_recovery_code(self, code, tag=None):
        self.code.append(Snippet(code, tag))

    def set_after(self, after):
        assert self.after is None, "can't call set_after() twice!"
        self.after = after

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return "<" + self.seq.summary(PRIORITY_UNAMBIGUOUS) + " ? ... >"

    def state_map(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        out = {self,  "<" + self.seq.summary(PRIORITY_UNAMBIGUOUS) + " ? . >"}
        for k, v in self.seq.state_map(PRIORITY_UNAMBIGUOUS):
            out[k] = "<" + v + "? >"
        return out

    @cacheable
    def get_first(self, prev=None):
        return self.seq.get_first(prev)

    @cacheable
    def get_disallowed_after(self, prev):
        return self.seq.get_disallowed_after(prev)

    @cacheable("fallbacks")
    def check(self, *, fallbacks=None):
        self.seq.check(fallbacks=fallbacks)


class Sequence(Parsable):
    """
    A sequence of terms which must be matched consecutively.
    """
    def __init__(self, name, loc=None):
        self.name = name
        self.scopes = [self]
        self.precode = []
        # tuples of (Parsable, tag=None, code=[])
        self.terms = []
        self.loc = loc
        self.prematched = None

    def add_term(self, term, tag=None):
        self.terms.append((term, tag, []))

    def add_code(self, code, tag=None, loc=None):
        if not self.terms:
            self.precode.append(Snippet(code, tag, loc))
        else:
            self.terms[-1][2].append(Snippet(code, tag, loc))

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        if len(self.terms) == 1:
            return self.terms[0][0].summary(parent_priority)
        out = " ".join(t.summary(PRIORITY_SEQUENCE) for t, _, _ in self.terms)
        if parent_priority > PRIORITY_SEQUENCE:
            out = "(" + out + ")"
        return out

    def state_map(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        out = {self: ". " + self.summary(PRIORITY_SEQUENCE)}
        if len(self.terms) == 1:
            out.update(self.terms[0][0].state_map())
            return out
        for i, (t, _, _) in enumerate(self.terms):
            # calculate error strings
            pre = " ".join(tt.summary(PRIORITY_SEQUENCE) for tt,_,_ in self.terms[:i])
            if pre:
                pre += " "
            post = " ".join(tt.summary(PRIORITY_SEQUENCE) for tt,_,_ in self.terms[i+1:])
            if post:
                post = " " + post
            if parent_priority > PRIORITY_SEQUENCE:
                pre = "(" + pre
                post = post + ")"

            for k, v in t.state_map(PRIORITY_SEQUENCE).items():
                out[k] = pre + v + post
        return out

    def get_first_ex(self, start, prev):
        """
        Get the list of all tokens that this expression could start with.

        Ex: at runtime, when are processing a .maybe(), we need to be able to
        look ahead one token and know if that token can be the first token
        of this expression.
        """
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for term, _, _ in self.terms[start:]:
            term_first = term.get_first(list(prev))
            # detect conflicts in .check(), not here.
            first = first.union(term_first)
            # if None is not in the set, we have our answer.
            if None not in first:
                break
            # if None is in the set, check the next term too.
            first.remove(None)
        else:
            # restore the None from the final term_first
            first.add(None)

        return frozenset(first)

    @cacheable
    def get_first(self, prev=None):
        return self.get_first_ex(0, prev)

    @cacheable
    def get_disallowed_after(self, prev=None):
        """
        Get the list of all tokens which would cause ambiguities after this
        expression.

        Ex. a maybe(EOL) term could not be followed by a match(EOL) term; it
        would be ambiguous if an EOL matched to the maybe() or the match().
        """
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = set()
        for term, _, _ in reversed(self.terms):
            term_disallowed = term.get_disallowed_after(list(prev))
            disallowed = disallowed.union(term_disallowed)
            # if None is not in this term's disallowed, we have our answer
            if None not in disallowed:
                break
            # if None is in the set, check the next term too
            disallowed.remove(None)
        else:
            # restore the None from the final term_disallowed
            disallowed.add(None)

        return frozenset(disallowed)

    @cacheable("fallbacks")
    def check(self, *, fallbacks=None):
        """
        At generation time, we need to ensure that for every Term in the
        expression, there is never a point where the lookahead token could
        possibly match the next token.
        """
        prev = {self.name}

        for term, _, _ in self.terms:
            term.check(fallbacks=fallbacks)

        disallowed = set()
        for i, (term, _, _) in enumerate(self.terms):
            term_first = term.get_first(list(prev))
            # Ignore None when checking for conflicts
            disallowed = disallowed.difference({None})
            conflicts = disallowed.intersection(term_first)
            if conflicts:
                raise FirstFollow(term, conflicts)
            if fallbacks is not None:
                # Predict where fallbacks would expand either the disallowed
                # set or the term_first set to cause conflicts.
                def expand(tokens):
                    out = set(tokens)
                    for t in tokens:
                        out = out.union(fallbacks.get(t, set()))
                    return out
                disallowed_ex = expand(disallowed)
                term_first_ex = expand(term_first)
                conflicts = disallowed_ex.intersection(term_first_ex)
                if conflicts:
                    raise FirstFollow(term, conflicts, due_to_fallbacks=True)
            term_disallowed = term.get_disallowed_after(list(prev))
            if None in term_disallowed:
                # disallowed should grow
                disallowed = disallowed.union(term_disallowed)
            else:
                # disallowed is reset
                disallowed = term_disallowed

            # Special case: check Recovery terms and call their set_after().
            if isinstance(term, Recovery):
                after_recovery = self.get_first_ex(i+1, None)
                if None in after_recovery:
                    raise ValueError(
                        "a Recovery must be followed by a never-empty match "
                        "within the same Sequence"
                    )
                term.set_after(after_recovery)


class Expression(Definition):
    """
    Each Expression will generate one function in the parser.
    """
    def __init__(self, name, typ=None, loc=None, body_loc=None):
        self.name = name
        self.type = typ
        self.seq = Sequence(self.name + ".seq", loc=body_loc)
        self.scopes = [self.seq]
        self.nrpt = 0
        self.nbranches = 0
        self.nrec = 0
        self.loc = loc

    def __call__(self, fn):
        """Provide a definition for a forwards-declared expression"""
        assert fn.__name__ == self.name, fn.__name__ + " != " + self.name
        fn(self)
        return self

    def match(self, term, tag=None, loc=None):
        self.scopes[-1].add_term(Reference(term, loc), tag)
        return term

    @contextlib.contextmanager
    def repeat(self, rmin, rmax, loc=None):
        term = Repeat(self.name + '.repeat' + str(self.nrpt), rmin, rmax, loc)
        self.nrpt += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def branches(self, loc=None):
        term = Branches(self.name + '.br' + str(self.nbranches), loc)
        self.nbranches += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def recovery(self, code=None, tag=None, loc=None):
        term = Recovery(
            self.name + ".recovery" + str(self.nrec), code, tag, loc
        )
        self.nrec += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    def exec(self, code, tag=None, loc=None):
        self.scopes[-1].add_code(code, tag, loc)

    def recurses(self, prev=None):
        """
        Return None if parsing this root includes any recursion at all, or
        the path by which it does recurse.
        """
        if prev is None:
            # top-level: user call, return None or a recursion path
            try:
                self.recurses([])
            except InfiniteRecursion as e:
                return e.path
        else:
            # inner-level: either raise an error or don't
            prev = add_to_prev(prev, self.name, "recurses")

            def _recurses(obj):
                if isinstance(obj, Reference):
                    if isinstance(obj.defn, Expression):
                        return obj.defn.recurses(prev)
                    elif isinstance(obj.defn, Token):
                        return
                    raise ValueError(
                        "unknown defn type: %s"%type(obj.defn).__name__
                    )
                elif isinstance(obj, (Repeat, Recovery)):
                    return _recurses(obj.seq)
                elif isinstance(obj, Branches):
                    for b in obj.branches:
                        _recurses(b)
                    return
                elif isinstance(obj, Sequence):
                    for term, _, _ in obj.terms:
                        _recurses(term)
                    return
                raise ValueError(
                    "unknown term type: %s"%type(term).__name__
                )

            _recurses(self.seq)

class Grammar:
    def __init__(self, text=None):
        self.exprs = {}
        self.text = text

    def get_name(self, name):
        """
        Creates a name if it doesn't yet exist.  Mostly for the meta parser.
        """
        if name in self.exprs:
            return self.exprs[name]
        if name[0].upper() == name[0]:
            # starts uppercase, we guess it's a token
            e = self.token(name)
        else:
            # starts lowercase, we guess it's a expression
            e = self.expr(name)
        return e

    def expr(self, val):
        if isinstance(val, str):
            # Forward declaration case:
            #     @g.expr
            #     def my_expr(e):
            #         e.match(...)
            fn = None
            name = val
        else:
            # Declaration/definition case.
            #     my_expr = g.expr("my_expr")
            #
            #     ...
            #
            #     @my_expr
            #     def my_expr(e):
            #         e.match(...)
            fn = val
            name = fn.__name__

        assert name not in self.exprs, name
        e = Expression(name)
        self.exprs[name] = e
        if fn is not None:
            fn(e)
        return e

    def token(self, name, typ=None, loc=None):
        assert name not in self.exprs
        e = Token(name, typ, loc)
        self.exprs[name] = e
        return e

    def sorted_tokens(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Token):
                yield name, expr

    def sorted_expressions(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Expression):
                yield name, expr

    def check(self, *, fallbacks=None):
        # first make sure everything is defined, or the checks become confusing
        undefined = set()
        for name, expr in self.sorted_expressions():
            if not expr.seq.terms:
                undefined.add(name)
        if undefined:
            # TODO: report these with locations to where they're referenced.
            raise RenderedError(
                "found referenced expression(s) with no definition: %s"%(
                    str(sorted(undefined))
                )
            )

        for name, expr in self.sorted_expressions():
            try:
                expr.seq.check(fallbacks=fallbacks)
            except GrammarError as e:
                # inject the full text of the grammar, if we have it
                e.text = self.text
                raise
            # detect auto-created expressions with no definition
            if not expr.seq.terms:
                raise RenderedError(
                    "expression %s referenced but never defined"%name
                )

            # trigger the prematched checks
            self._set_prematched(expr.seq, False)

    def _set_prematched(self, obj, prematched):
        if isinstance(obj, Reference):
            obj.prematched = prematched
            return
        if isinstance(obj, Repeat):
            # only maybe-types (not real repeats) can ever be prematched
            obj.prematched = obj.type == "maybe" and prematched
            # but contents of the repeat are always prematched
            self._set_prematched(obj.seq, True)
            return
        if isinstance(obj, Recovery):
            obj.prematched = prematched
            self._set_prematched(obj.seq, prematched)
            return
        if isinstance(obj, Branches):
            obj.prematched = prematched
            for b in obj.branches:
                # every branch is always prematched
                self._set_prematched(b, True)
            return
        if isinstance(obj, Sequence):
            obj.prematched = prematched
            # the first term may prematched, but only if it is concrete
            if None in obj.terms[0][0].get_first():
                prematched = False
            for t, _, _ in obj.terms:
                self._set_prematched(t, prematched)
                # Subsequent terms are never prematched
                prematched = False
            return
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

class Types:
    """The result of all of the %type directives in a grammar."""
    def __init__(self, types):
        self.types = types
        self.typemap = {t.name: t for t in types}

    def __getitem__(self, value):
        return self.typemap[value]

    def source_ordered(self):
        yield from iter(self.types)


def extract_types(parsed_types, grammar, tag):
    typelist = []
    type_names = set()
    for t in parsed_types:
        if t.tag not in (None, tag):
            continue
        assert t.name not in type_names, (
            "got multiple '%%type %s' directives"%t.name
        )
        typelist.append(t)
        type_names.add(t.name)
    detected = set(e.type for e in grammar.exprs.values() if e.type)
    missing_defs = detected.difference(type_names)
    extra_defs = type_names.difference(detected)
    if missing_defs or extra_defs:
        raise RenderedError(
            "detected missing %%types [%s] and unused %%types [%s]"%(
                ','.join(missing_defs), ','.join(extra_defs)
            )
        )
    return Types(typelist)


class GeneratorContext:
    """
    A parameter given to a generator class.  Append-only for easy forwards
    compatibility of generator implementations.
    """
    def __init__(
        self,
        grammar,
        roots,
        prefix,
        file,
        src,
        dst,
        dst_offset,
        precode,
        postcode,
        types,
        fallbacks,
        params,
    ):
        self.grammar = grammar
        self.roots = roots
        self.src = src
        self.dst = dst
        self.dst_offset = dst_offset
        self.file = file
        self.prefix = prefix
        self.precode = precode
        self.postcode = postcode
        self.types = types
        self.fallbacks = fallbacks
        self.params = params


class C:
    """
    An object with all the methods for generating C code.
    """
    code_tag = "c"

    class Variables:
        def __init__(self, fprint_fn, semloc):
            # a list of lists of available variables
            self.scopes = [[]]
            self.semloc = bool(semloc)
            self.fprint = fprint_fn

        def new_scope(self):
            self.scopes.append([])

        def define(self, tag, typ, pos):
            for scope in self.scopes:
                assert tag not in scope, "tag " + tag + " shadows another tag"
            suffix = "OUT" if tag == "$" else "_" + tag
            if typ is not None:
                self.fprint(
                    "    #define YYVAL%s "
                    "(p->semstack[call->stack + %d].val.%s)"%(
                        suffix, pos, typ
                    ),
                )
            if self.semloc and tag != "$":
                self.fprint(
                    "    #define YYLOC" + suffix
                    + " (p->semstack[call->stack + %d].loc)"%pos,
                )
            self.scopes[-1].append((tag, typ))

        def define_final_loc(self):
            """
            We delay defining the final location because it isn't set until
            just before the last code block of an expression, so it wouldn't
            be sensible to read or write it before then.
            """
            self.fprint(
                "    #define YYLOCOUT (p->semstack[call->stack + 0].loc)"
            )

        def pop_scope(self):
            popped = self.scopes.pop()
            for tag, typ in popped:
                suffix = "OUT" if tag == "$" else "_" + tag
                if typ is not None:
                    self.fprint("    #undef YYVAL" + suffix)
                if self.semloc:
                    self.fprint("    #undef YYLOC" + suffix)

    def __init__(
        self,
        ctx,
        *,
        semloc_type=None,
        span_fn=None,
        zero_loc_fn=None,
        error_fn=None,
    ):
        self.grammar = ctx.grammar
        self.file = ctx.file
        assert ctx.roots, "you must specify at least one %root"
        self.roots = ctx.roots
        self.precode = ctx.precode
        self.postcode = ctx.postcode
        self.types = ctx.types
        self.fallbacks = ctx.fallbacks
        self.prefix = ctx.prefix
        self.params = ctx.params
        self.src = ctx.src
        self.dst = ctx.dst

        self.prefix_ = (self.prefix + "_") if self.prefix else ""
        assert bool(semloc_type)+bool(span_fn)+bool(zero_loc_fn) in (0, 3), (
            "semloc_type, span_fn, and zero_loc_fn must be all set or all unset"
        )
        self.semloc_type = semloc_type
        self.span_fn = span_fn
        self.zero_loc_fn = zero_loc_fn
        self.error_fn = error_fn

        # build a default mapping
        params_str = ""
        args_str = ""
        voids_str = ""
        for p in self.params:
            params_str += ",\n    %s %s"%(p.type.strip(), p.name)
            args_str += ", " + p.name
            voids_str += " (void)%s;"%p.name
        self.varmap = {
            "prefix": self.prefix,
            "prefix_": self.prefix_,
            "PREFIX_": self.prefix_.upper(),
            "params": params_str,
            "args": args_str,
            "voids": voids_str,
            "semloc_type": self.semloc_type,
            "span_fn": self.span_fn,
            "zero_loc_fn": self.zero_loc_fn,
            "error_fn": self.error_fn or "%shandle_syntax_error"%self.prefix_,
            "parse_fn_args": (
                "\n    %scall_t *call,"
                "\n    %sparser_t *p"
                "%s,"
                "\n    %stoken_e *token,"
                "\n    %ssem_t sem"
                "\n"
            )%(self.prefix_, self.prefix_, params_str, self.prefix_, self.prefix_),
        }

        self._extra_vars = {}

        self.tokens_have_value = any(
            t.type is not None for _, t in self.grammar.sorted_tokens()
        )

        self.lines = ctx.dst_offset or 0
        self.ncounters = 0
        self.state_map = None

        self.semstackmaxes = {}
        self.callstackmaxes = {}

    def sub(self, s):
        return string.Template(s).substitute(**self.varmap, **self._extra_vars)

    def fprint(self, s, **kwargs):
        text = self.sub(s)
        print(text, file=self.file, **kwargs)
        self.lines += text.count("\n") + kwargs.get("end", "\n").count("\n")

    def fprint_multi(self, s):
        text = self.sub(textwrap.dedent(s).strip("\n"))
        print(text, file=self.file)
        self.lines += text.count("\n") + 1

    @contextlib.contextmanager
    def extra_vars(self, **mapping):
        old_vars = dict(**self._extra_vars)
        self._extra_vars.update(mapping)
        try:
            yield
        finally:
            self._extra_vars = old_vars

    def print_code(self, code, i):
        if code is None:
            return
        for c in code:
            if c.tag in (None, "c"):
                self.fprint(i + "// USER CODE")
                break
        for c in code:
            if c.tag in (None, "c"):
                with self.line_hints(c.loc):
                    # self.fprint(textwrap.indent(c.text, i))
                    text = re.sub("\\$\\$", "YYVALOUT", c.text)
                    text = re.sub("@\\$", "YYLOCOUT", text)
                    text = re.sub("\\$([a-zA-Z_][a-zA-Z0-9_]*)", "YYVAL_\\1", text)
                    text = re.sub("@([a-zA-Z_][a-zA-Z0-9_]*)", "YYLOC_\\1", text)
                    # don't use fprint; we shouldn't be subbing in internal vars
                    text = textwrap.indent(text, i)
                    print(text, file=self.file)
                    self.lines += text.count("\n") + 1

    @contextlib.contextmanager
    def line_hints(self, loc):
        if loc and self.src and self.dst:
            self.fprint('#line %d "%s"'%(loc.start.line, self.src))
            yield
            self.fprint('#line %d "%s"'%(self.lines+2, self.dst))
        else:
            yield

    def print_destructor(self, code, lhv, i):
        if code is None:
            return
        for c in code:
            if c.tag in (None, "c"):
                self.fprint(i + "// USER CODE")
                break
        for c in code:
            if c.tag in (None, "c"):
                with self.line_hints(c.loc):
                    text = re.sub("\\$\\$", lhv, c.text)
                    # don't use fprint; we shouldn't be subbing in internal vars
                    text = textwrap.indent(text, i)
                    print(text, file=self.file)
                    self.lines += text.count("\n") + 1

    def _token_list_args(self, t, f):
        out = ", ".join(map(lambda x: "${PREFIX_}"+x, t))
        if f:
            out += ", /*fallback*/ " + ", ".join(map(lambda x: "${PREFIX_}"+x, f))
        return len(t) + len(f), out

    def token_check(self, tokens):
        # primary tokens
        t = sorted(t for t in tokens if t is not None)
        # fallback tokens
        f = self.fallbacks and sorted(self.fallbacks.all_fallbacks(t)) or []
        nargs, args = self._token_list_args(t, f)
        return "${prefix_}token_in(*token, %d, %s)"%(nargs, args)

    def mask_set(self, tokens):
        # primary tokens
        t = sorted(t for t in tokens if t is not None)
        # fallback tokens
        f = self.fallbacks and sorted(self.fallbacks.all_fallbacks(t)) or []
        nargs, args = self._token_list_args(t, f)
        return "${prefix_}mask_set(p->mask, %d, %s)"%(nargs, args)

    def stackpersist(self, obj):
        """Return the persistent created during a match to obj."""
        if isinstance(obj, Reference):
            return 1
        return 0

    def stackmax(self, obj):
        """Return the maximum stack size during a match to obj."""
        if isinstance(obj, Reference):
            return 1
        if isinstance(obj, Repeat):
            # when we need to count repeats, we do that on the semstack
            extra = 1 if obj.type == "general" else 0
            return self.stackmax(obj.seq) + extra
        if isinstance(obj, Recovery):
            return self.stackmax(obj.seq)
        if isinstance(obj, Branches):
            return max(self.stackmax(b) for b in obj.branches)
        if isinstance(obj, Sequence):
            persist = 0
            peak = 0
            for t, _, _ in obj.terms:
                peak = max(peak, persist + self.stackmax(t))
                persist += self.stackpersist(t)
            return peak
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def callstackmax(self, root):
        """
        Calculate the maximum total depth of calls that could be required when
        parsing a root.  Returns None when root contains a recursion.
        """
        if root.name in self.callstackmaxes:
            return self.callstackmaxes[root.name]

        if root.recurses() is not None:
            self.semstackmaxes[root.name] = None
            self.callstackmaxes[root.name] = None
            return None

        def _callstackmax(obj):
            if isinstance(obj, Reference):
                if isinstance(obj.defn, Expression):
                    return self.callstackmax(obj.defn)
                elif isinstance(obj.defn, Token):
                    return 0
                raise ValueError(
                    "unknown term type: %s"%type(term).__name__
                )
            elif isinstance(obj, (Repeat, Recovery)):
                return _callstackmax(obj.seq)
            elif isinstance(obj, Branches):
                return max(_callstackmax(b) for b in obj.branches)
            elif isinstance(obj, Sequence):
                return max(_callstackmax(term) for term, _, _ in obj.terms)
            raise ValueError(
                "unknown term type: %s"%type(term).__name__
            )

        # Count the root expression as well.
        out = 1 + _callstackmax(root.seq)
        self.callstackmaxes[root.name] = out
        return out

    def semstackmax(self, root):
        """
        Calculate the maximum total depth of sems that could be required when
        parsing a root.  Returns None when root contains a recursion.
        """
        if root.name in self.semstackmaxes:
            return self.semstackmaxes[root.name]

        if root.recurses() is not None:
            self.semstackmaxes[root.name] = None
            self.callstackmaxes[root.name] = None
            return None

        def _semstackmax(obj):
            if isinstance(obj, Reference):
                if isinstance(obj.defn, Expression):
                    return self.semstackmax(obj.defn)
                elif isinstance(obj.defn, Token):
                    return 1
                raise ValueError(
                    "unknown term type: %s"%type(term).__name__
                )
            elif isinstance(obj, Repeat):
                # when we need to count repeats, we do that on the semstack
                extra = 1 if obj.type == "general" else 0
                return _semstackmax(obj.seq) + extra
            elif isinstance(obj, Recovery):
                return _semstackmax(obj.seq)
            elif isinstance(obj, Branches):
                return max(_semstackmax(b) for b in obj.branches)
            elif isinstance(obj, Sequence):
                persist = 0
                peak = 0
                for t, _, _ in obj.terms:
                    peak = max(peak, persist + _semstackmax(t))
                    persist += self.stackpersist(t)
                return peak
            raise ValueError(
                "unknown term type: %s"%type(term).__name__
            )

        # Count the root's $$ as well.
        out = 1 + _semstackmax(root.seq)
        self.semstackmaxes[root.name] = out
        return out

    @contextlib.contextmanager
    def delay_output(self):
        old_file = self.file
        self.file = io.StringIO()

        try:
            yield self.file
        finally:
            self.file = old_file

    def nstates(self, obj):
        # Hack: logic here mirrors gen, but without generating anything
        if isinstance(obj, Reference):
            if isinstance(obj.defn, Token):
                return int(not obj.prematched)
            if isinstance(obj.defn, Expression):
                return 1
            raise RuntimeError("unrecognized reference type: " + type(obj.defn).__name__)
        if isinstance(obj, Repeat):
            if obj.type == "maybe":
                return int(not obj.prematched) + self.nstates(obj.seq)
            if obj.type == "zero_or_more":
                return 1 + self.nstates(obj.seq)
            return 1 + self.nstates(obj.seq)
        if isinstance(obj, Branches):
            return (
                int(not obj.prematched)
                + sum(self.nstates(b) for b in obj.branches)
            )
        if isinstance(obj, Recovery):
            return 1 + self.nstates(obj.seq)
        if isinstance(obj, Sequence):
            return sum(self.nstates(t) for t,_,_ in obj.terms)

    def gen(self, obj, state, stack, var, tag=None, want_final_loc=False):
        """Generate code to match an object."""
        if isinstance(obj, Reference):
            if isinstance(obj.defn, Token):
                if obj.prematched:
                    self.fprint("    // prematched: " + obj.summary())
                else:
                    self.fprint("    // " + obj.summary())
                    self.fprint("    call->state = " + str(state) + ";")
                    self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
                    self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
                    self.fprint("yy" + str(state) + ":")
                    self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
                    self.fprint("        " + self.mask_set(obj.get_first()) + ";")
                    self.fprint("        ${PREFIX_}SYNTAX_ERROR(*token, sem);")
                    self.fprint("    }")
                    state += 1
                self.fprint("    p->semstack[call->stack + " + str(stack) + "] = sem;")
                self.fprint("#ifdef ${PREFIX_}HAVE_SEMLOC")
                self.fprint("    ${prefix_}process_matched_loc(p, call, sem.loc);")
                self.fprint("#endif")
                self.fprint("    ${PREFIX_}CONSUME_TOKEN;")
                self.fprint("    ${prefix_}mask_clear(p->mask);")
                return state, stack + 1

            if isinstance(obj.defn, Expression):
                self.fprint("    // " + obj.summary())
                self.fprint("    call->state = " + str(state) + ";")
                self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
                self.fprint("    ${PREFIX_}CALL(%s, %d);"%(self.parse_fn(obj.defn), stack))
                self.fprint("yy" + str(state) + ":")
                self.fprint("#ifdef ${PREFIX_}HAVE_SEMLOC")
                self.fprint("    ${prefix_}process_matched_loc(p,")
                self.fprint("        call, p->semstack[call->stack + %d].loc"%stack)
                self.fprint("    );")
                self.fprint("#endif")
                return state + 1, stack + 1

            raise RuntimeError("unrecognized reference type: " + type(obj.defn).__name__)

        if isinstance(obj, Repeat):
            if obj.type == "maybe":
                # no looping or counting required
                if obj.prematched:
                    self.fprint("    // prematched: " + obj.summary())
                    tstate = state
                else:
                    self.fprint("    // " + obj.summary())
                    self.fprint("    call->state = " + str(state) + ";")
                    self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
                    self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
                    self.fprint("yy" + str(state) + ":")
                    self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
                    self.fprint("        " + self.mask_set(obj.get_first()) + ";")
                    self.fprint("        goto yy" + str(state) + "_done;")
                    self.fprint("    }")
                    tstate = state + 1
                tstate, _ = self.gen(obj.seq, tstate, stack, var)
                if not obj.prematched:
                    self.fprint("yy" + str(state) + "_done:")
                return tstate, stack

            if obj.type == "zero_or_more":
                # looping, but no counting required
                # ignore prematched since we always need a check
                self.fprint("    // " + obj.summary())
                self.fprint("yy" + str(state) + ":")
                self.fprint("    call->state = " + str(state) + ";")
                self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
                self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
                self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
                self.fprint("        " + self.mask_set(obj.get_first()) + ";")
                self.fprint("        goto yy" + str(state) + "_done;")
                self.fprint("    }")
                self.fprint("")
                tstate, _ = self.gen(obj.seq, state + 1, stack, var)
                self.fprint("    // try for another " + obj.summary())
                self.fprint("    goto yy" + str(state) + ";")
                self.fprint("yy" + str(state) + "_done:")
                return tstate, stack

            # arbitrary repeat; looping and counting required.
            # This means we need an extra semantic value.
            # ignore prematched since we always need a check
            counter = "p->semstack[call->stack + %d].val._yy_counter"%stack
            COUNTER = "COUNTER_%d"%self.ncounters
            self.ncounters += 1
            with self.extra_vars(
                state=state,
                counter=counter,
                COUNTER=COUNTER,
                rmin=obj.rmin,
                rmax=obj.rmax
            ):
                self.fprint("    // " + obj.summary())
                self.fprint("    #define $COUNTER ($counter)")
                self.fprint("    $COUNTER = 0;")
                self.fprint("yy$state:")
                self.fprint("    call->state = $state;")
                self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
                self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
                self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
                self.fprint("        " + self.mask_set(obj.get_first()) + ";")
                if obj.rmin > 0:
                    self.fprint("        if($COUNTER < $rmin){")
                    self.fprint("            // minimum repetition not met")
                    self.fprint("            ${PREFIX_}SYNTAX_ERROR(*token, sem);")
                    self.fprint("        }")
                self.fprint("        goto yy${state}_done;")
                self.fprint("    }")
                self.fprint("    $COUNTER++;")
                self.fprint("")
                tstate, _ = self.gen(obj.seq, state + 1, stack + 1, var)
                if obj.rmax is None:
                    self.fprint("    // try for another " + obj.summary())
                    self.fprint("    goto yy$state;")
                else:
                    self.fprint("    if($COUNTER < $rmax){")
                    self.fprint("        // try for another " + obj.summary())
                    self.fprint("        goto yy$state;")
                    self.fprint("    }")
                self.fprint("yy${state}_done:")
                self.fprint("    // clear our counter")
                self.fprint("    $COUNTER = 0;")
                self.fprint("    #undef $COUNTER")
                return tstate, stack

        if isinstance(obj, Branches):
            if obj.prematched:
                # if empty branches are ever allowed, remember to fix this
                assert None not in obj.get_first(), "fixme"
                self.fprint("    // prematched: " + obj.summary())
                bstate = state
            else:
                self.fprint("    // " + obj.summary())
                self.fprint("    call->state = " + str(state) + ";")
                self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
                self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
                self.fprint("yy" + str(state) + ":")
                bstate = state + 1
            self.fprint("    switch(*token){")
            # detect primary types
            primaries = set(
                t for b in obj.branches for t in b.get_first()
            )
            # emit case statements
            for i, b in enumerate(obj.branches):
                for t in b.get_first():
                    label = "yy%d_br_%d"%(state, i)
                    fallbacks = set()
                    if self.fallbacks:
                        # calculate fallback types
                        fallbacks = self.fallbacks.fallback_set(t, primaries)
                    for f in sorted(fallbacks):
                        self.fprint("        case ${PREFIX_}%s: // fallback"%f)
                    self.fprint("        case ${PREFIX_}%s: goto %s;"%(t, label))
            self.fprint("        default:")
            self.fprint("            " + self.mask_set(obj.get_first()) + ";")
            self.fprint("            ${PREFIX_}SYNTAX_ERROR(*token, sem);")
            self.fprint("    }")
            self.fprint("")
            # branch parsing
            for i, b in enumerate(obj.branches):
                self.fprint("yy" + str(state) + "_br_" + str(i) + ":")
                # bstate advances between branches, but stack resets each time
                bstate, _ = self.gen(b, bstate, stack, var)
                self.fprint("    goto yy" + str(state) + "_done;")
                self.fprint("")
            self.fprint("yy"+ str(state) + "_done:")
            return bstate, stack

        if isinstance(obj, Recovery):
            assert obj.after is not None, ".set_after() was never called"
            self.fprint("    // " + obj.summary())
            self.fprint("    call->recover = " + str(state) + ";")
            self.fprint("")
            tstate, _ = self.gen(obj.seq, state + 1, stack, var)
            self.fprint("")
            self.fprint("    call->recover = 0;")
            self.fprint("    goto yy" + str(state) + "_done;")
            self.fprint("yy"+ str(state) + ":")
            self.fprint("    // recovery: consume all tokens until a valid one")
            self.fprint("    call->summary = \"" + self.state_map[obj] + "\";")
            self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
            self.fprint("    if(!" + self.token_check(obj.after) + "){")
            self.fprint("        ${PREFIX_}CONSUME_TOKEN;")
            self.fprint("        return 0;")
            self.fprint("    }")
            self.print_code(obj.code, "    ")
            self.fprint("yy"+ str(state) + "_done:")
            return tstate, stack

        if isinstance(obj, Sequence):
            oldstack = stack
            var.new_scope()
            self.print_code(obj.precode, "    ")
            for i, (t, tag, code) in enumerate(obj.terms):
                if tag is not None:
                    assert isinstance(t, Reference), "wrong type for tag: " + type(t).__name__
                    var.define(tag, t.defn.type, stack)
                state, stack = self.gen(t, state, stack, var, tag)
                final = i+1 == len(obj.terms)
                # calculate the overall location before the final code block
                if final and want_final_loc:
                    self.fprint_multi(r"""
                    #ifdef ${PREFIX_}HAVE_SEMLOC
                        // calculate the final location before the final user action
                        if(call->have_loc){
                            p->semstack[call->stack].loc = $span_fn(
                                call->loc_start, call->loc_end
                            );
                        }else{
                            p->semstack[call->stack].loc = $zero_loc_fn(p->prev);
                        }
                    #endif
                    """)
                    var.define_final_loc()
                self.print_code(code, "    ")
                if not final:
                    self.fprint("")
            var.pop_scope()
            self.fprint("    ${prefix_}free_sems(p, call->stack+%d);"%oldstack)
            return state, stack

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def parse_fn(self, expr):
        """Get the generated function name for an Expression."""
        return "_${prefix_}parse_" + expr.name

    def declare_fn(self, expr):
        self.fprint(
            "static ${prefix_}status_e %s($parse_fn_args);"%self.parse_fn(expr)
        )

    def define_fn(self, expr):
        """Generate the function definition for an Expression."""
        self.ncounters = 0
        self.state_map = expr.seq.state_map()

        exp_nstates = 1 + self.nstates(expr.seq)

        self.fprint("// %s = %s;"%(expr.name, expr.seq.summary()))
        self.fprint(
            "static ${prefix_}status_e %s($parse_fn_args){"%self.parse_fn(expr)
        )
        self.fprint("(void)token; (void)sem;$voids")
        # Jump to state.
        self.fprint("    switch(call->state){")
        self.fprint("        case " + str(0) + ": break;")
        for n in range(exp_nstates-1):
            self.fprint("        case " + str(n+1) + ": goto yy" + str(n+1) + ";")
        self.fprint("    }")
        # Ensure we have enough stack space to operate.
        x = 1 + self.stackmax(expr.seq)
        self.fprint("    if(!${prefix_}sems_available(p, call->stack + %d)){"%x)
        self.fprint("        return ${PREFIX_}STATUS_SEMSTACK_OVERFLOW;")
        self.fprint("    }")
        self.fprint("    call->nsems = %d;"%x)

        var = C.Variables(self.fprint, semloc=self.semloc_type)
        if expr.type is not None:
            var.define("$", expr.type, 0)
            self.fprint("    p->semstack[call->stack].type = ${PREFIX_}SEMTYPE_%s;"%expr.type)
        self.fprint("")

        # state starts at 1, since 0 is only the first time we're called
        # stack starts at 1, since we always allocate the output
        nstates, _ = self.gen(expr.seq, 1, 1, var, want_final_loc=True)
        # make sure self.nstates() agrees with self.gen()
        assert nstates == exp_nstates, "%d != %d"%(nstates, exp_nstates)

        self.fprint("")
        var.pop_scope()

        self.fprint("    p->callslen--;");
        self.fprint("    return 0;")
        self.fprint("}")
        self.fprint("")

    def define_user_api(self, root, header=False):
        type_name = self.grammar.exprs[root].type
        type_spec = None
        if type_name:
            type_spec = self.types[type_name].spec

        with self.extra_vars(
            root=root, type_name=type_name, type_spec=type_spec
        ):
            self.fprint("${prefix_}status_e ${prefix_}parse_$root(")
            self.fprint("    ${prefix_}parser_t *p", end="")
            self.fprint("$params", end="")
            self.fprint(",\n    ${prefix_}token_e token", end="")
            if self.tokens_have_value:
                self.fprint(",\n    ${prefix_}val_u val", end="")
            if self.semloc_type:
                self.fprint(",\n    $semloc_type loc", end="")
            if type_spec:
                self.fprint(",\n    $type_spec *val_out", end="")
            if self.semloc_type:
                self.fprint(",\n    $semloc_type *loc_out", end="")
            self.fprint("")
            if header:
                self.fprint(");")
                return
            self.fprint("){")
            self.fprint("    ${prefix_}sem_t sem = {")
            if self.tokens_have_value:
                self.fprint("        .type = _${prefix_}token_types[token],")
                self.fprint("        .val = val,")
            if self.semloc_type:
                self.fprint("        .loc = loc,")
            if not self.tokens_have_value and not self.semloc_type:
                self.fprint("        0")
            self.fprint("    };")
            self.fprint("    ${prefix_}sem_t sem_out = {0};")
            self.fprint("    ${prefix_}status_e status = _${prefix_}do_parse(")
            self.fprint("        p, _${prefix_}parse_$root$args, token, sem, &sem_out")
            self.fprint("    );")
            if type_spec or self.semloc_type:
                self.fprint("    if(status != ${PREFIX_}STATUS_DONE) return status;")
            if self.semloc_type:
                self.fprint("    if(loc_out){")
                self.fprint("        *loc_out = sem_out.loc;")
                self.fprint("    }")
            if type_spec:
                self.fprint("    if(val_out){")
                self.fprint("        *val_out = sem_out.val.$type_name;")
                self.fprint("    }else{")
                self.fprint("        ${prefix_}sem_free(&sem_out);")
                self.fprint("    }")
            self.fprint("    return status;")
            self.fprint("}")

    def gen_header(self):
        """
        Generate a .h file exposing types and functions that are meaningful to
        external modules.
        """
        if self.semloc_type:
            self.fprint("#define ${PREFIX_}HAVE_SEMLOC")
            self.fprint("")

        self.fprint("typedef enum {")
        self.fprint("    _${PREFIX_}NOT_A_TOKEN = 0,")
        for _, token in self.grammar.sorted_tokens():
            self.fprint("    ${PREFIX_}%s,"%token.name)
        self.fprint("} ${prefix_}token_e;")

        self.fprint("")
        self.fprint("typedef enum {")
        self.fprint("    ${PREFIX_}NO_SEMTYPE = 0,")
        for t in self.types.source_ordered():
            self.fprint("    ${PREFIX_}SEMTYPE_%s,"%t.name)
        self.fprint("} ${prefix_}sem_type_e;")

        self.fprint("")
        self.fprint("// we need ${PREFIX_}MASKLEN chars to have one bit per token")
        masklen = (len(list(self.grammar.sorted_tokens())) + 7) // 8
        self.fprint("#define ${PREFIX_}MASKLEN %d"%masklen)

        self.fprint("")
        self.fprint_multi(r"""
            typedef enum {
                ${PREFIX_}STATUS_OK = 0,
                ${PREFIX_}STATUS_DONE,
                ${PREFIX_}STATUS_SYNTAX_ERROR,
                ${PREFIX_}STATUS_SEMSTACK_OVERFLOW,
                ${PREFIX_}STATUS_CALLSTACK_OVERFLOW,
            } ${prefix_}status_e;

            struct ${prefix_}call_t;
            typedef struct ${prefix_}call_t ${prefix_}call_t;
            struct ${prefix_}parser_t;
            typedef struct ${prefix_}parser_t ${prefix_}parser_t;
            struct ${prefix_}sem_t;
            typedef struct ${prefix_}sem_t ${prefix_}sem_t;

            typedef ${prefix_}status_e (*${prefix_}parse_f)($parse_fn_args);

            struct ${prefix_}call_t {
                // a function to call
                ${prefix_}parse_f fn;
                // our starting position in the semstack
                size_t stack;
                // a count of how many sems we'll need
                size_t nsems;
                // our position in the call
                size_t state;
                // always set when we pause execution
                const char *summary;
                // the state to enter if we have to recover
                size_t recover;
            #ifdef ${PREFIX_}HAVE_SEMLOC
                // if we have matched anything yet
                int have_loc;
                // the first location that matches this call
                $semloc_type loc_start;
                // the last location that matches this call
                $semloc_type loc_end;
            #endif
            };

            struct ${prefix_}parser_t {
                // stack of calls
                ${prefix_}call_t *callstack;
                size_t callslen;
                size_t callsmax;
                // stack of semvals
                ${prefix_}sem_t *semstack;
                size_t semsmax;
                // a mask of all possible tokens
                unsigned char mask[${PREFIX_}MASKLEN];
            #ifdef ${PREFIX_}HAVE_SEMLOC
                // the last location we accepted
                $semloc_type last;
                // either NULL or points to .last
                $semloc_type *prev;
            #endif
            };
        """)

        self.fprint("")
        self.fprint("typedef union {")
        self.fprint("    int _yy_counter;")
        for t in self.types.source_ordered():
            self.fprint("    %s %s;"%(t.spec, t.name))
        self.fprint("} ${prefix_}val_u;")

        self.fprint("")
        self.fprint_multi(r"""
            struct ${prefix_}sem_t {
                ${prefix_}sem_type_e type;
                // semantic value
                ${prefix_}val_u val;

            #ifdef ${PREFIX_}HAVE_SEMLOC
                // semantic location
                $semloc_type loc;
            #endif
            };

            // USER API

            // get a (read-only) string with the name of a token
            const char *${prefix_}token_name(${prefix_}token_e t);

            // allocate a new fixed-size parser, backed by heap memory
            ${prefix_}parser_t *${prefix_}parser_new(size_t callsmax, size_t semsmax);

            // free all memory in a heap-backed parser.
            void ${prefix_}parser_free(${prefix_}parser_t **pp);

            // free all semantic values in the parser state and reset callstack
            void ${prefix_}parser_reset(${prefix_}parser_t *p);

            // define a fixed-size parser, backed by stack memory
            #define ${PREFIX_}ONSTACK_PARSER(_name, _callsmax, _semsmax) \
                ${prefix_}call_t _name##calls[_callsmax] = {0}; \
                ${prefix_}sem_t _name##sems[_semsmax] = {0}; \
                ${prefix_}parser_t _name = (${prefix_}parser_t){ \
                    .callstack = _name##calls, \
                    .callsmax = _callsmax, \
                    .semstack = _name##sems, \
                    .semsmax = _semsmax, \
                }

            // during a syntax error, print the parsing stack
            int ${prefix_}snprint_stack(char *buf, size_t limit, ${prefix_}parser_t *p);
            int ${prefix_}fprint_stack(FILE *f, ${prefix_}parser_t *p);

            // during a syntax error, print the expected tokens
            int ${prefix_}snprint_mask(
                char *buf, size_t limit, const unsigned char *mask, const char *joiner
            );
            int ${prefix_}fprint_mask(
                FILE *f, const unsigned char *mask, const char *joiner
            );

            // ROOT PARSERS
        """)

        # define user apis
        for root in self.roots:
            self.fprint("")
            # Check for statically-determined stack sizes.
            expr = self.grammar.exprs[root]
            if not expr.recurses():
                cmax = self.callstackmax(expr)
                smax = self.semstackmax(expr)
                with self.extra_vars(
                    root=root, ROOT=root.upper(), cmax=cmax, smax=smax
                ):
                    self.fprint(
                        "// the definition of $root is non-recursive, and so you can"
                    )
                    self.fprint(
                        "// use the following values without risk of stack overflows"
                    )
                    self.fprint("#define ${PREFIX_}${ROOT}_MAX_CALLSTACK $cmax")
                    self.fprint("#define ${PREFIX_}${ROOT}_MAX_SEMSTACK $smax")
            self.define_user_api(root, header=True)

    def gen_body(self):
        """
        Generate a .c file which will not compile without the header file being
        included in a code block or template of some sort.
        """
        self.gen_file(embed_header=False)

    def gen_file(self, embed_header=True):
        """
        Generate a standalone file with the headerfile and the body of
        definitions in the same file (not #include necessary).
        """
        self.print_code(self.precode, "")

        self.fprint_multi("""
            #include <stdio.h>
            #include <stdlib.h>
            #include <stdarg.h>
            #include <stdint.h>
            #include <string.h>
            #include <limits.h>
        """)

        if embed_header:
            self.fprint("")
            self.gen_header()

        if self.tokens_have_value:
            self.fprint("")
            self.fprint("static ${prefix_}sem_type_e _${prefix_}token_types[] = {")
            self.fprint("   /* _${PREFIX_}NOT_A_TOKEN */  ${PREFIX_}NO_SEMTYPE,")
            for _, token in self.grammar.sorted_tokens():
                if token.type:
                    typ = "${PREFIX_}SEMTYPE_%s"%token.type
                else:
                    typ = "${PREFIX_}NO_SEMTYPE"
                self.fprint("   /* %s */  %s,"%(token.name, typ))
            self.fprint("};")

        self.fprint("")
        self.fprint("static void ${prefix_}sem_free(${prefix_}sem_t *sem){")
        self.fprint("    switch(sem->type){")
        self.fprint("        case ${PREFIX_}NO_SEMTYPE: break;")
        for t in self.types.source_ordered():
            self.fprint("        case ${PREFIX_}SEMTYPE_%s: {"%t.name)
            self.print_destructor(
                t.destructor, lhv="(sem->val.%s)"%t.name, i="           "
            )
            self.fprint("        } break;")
        self.fprint("    }")
        self.fprint("    *sem = (${prefix_}sem_t){0};")
        self.fprint("}")

        self.fprint("")
        self.fprint_multi(r"""
            static void ${prefix_}mask_clear(unsigned char *mask){
                memset(mask, 0, ${PREFIX_}MASKLEN);
            }

            // accepts `n` token types
            static void ${prefix_}mask_set(unsigned char *mask, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                for(size_t i = 0; i < n; i++){
                    ${prefix_}token_e t = va_arg(ap, ${prefix_}token_e);
                    mask[t/8] |= (unsigned char)(1 << (t%8));
                }

                va_end(ap);
            }

            int ${prefix_}snprint_mask(
                char *buf, size_t limit, const unsigned char *mask, const char *joiner
            ){
                int allowed = limit > INT_MAX ? INT_MAX : (int)limit;
                int written = 0;
                int first = 1;
                for(int i = 0; i < ${PREFIX_}MASKLEN; i++){
                    for(int j = 0; j < 8; j++){
                        if(!(mask[i] & ((unsigned char)1)<<j)) continue;
                        int w = snprintf(
                            buf + written,
                            (size_t)(allowed - written),
                            "%s%s",
                            first ? "": joiner,
                            ${prefix_}token_name((${prefix_}token_e)(i*8+j))
                        );
                        if(w < 0){
                            if(limit) buf[limit-1] = '\0';
                            return w;
                        }
                        written += w;
                        first = 0;
                    }
                }
                if(limit) buf[limit-1] = '\0';
                return written;
            }

            int ${prefix_}fprint_mask(
                FILE *f, const unsigned char *mask, const char *joiner
            ){
                int written = 0;
                int first = 1;
                for(int i = 0; i < ${PREFIX_}MASKLEN; i++){
                    for(int j = 0; j < 8; j++){
                        if(!(mask[i] & ((unsigned char)1)<<j)) continue;
                        int w = fprintf(
                            f, "%s%s",
                            first ? "": joiner,
                            ${prefix_}token_name((${prefix_}token_e)(i*8+j))
                        );
                        if(w < 0) return w;
                        written += w;
                        first = 0;
                    }
                }
                return written;
            }

            static ${prefix_}call_t *${prefix_}calls_append(${prefix_}parser_t *p){
                if(p->callslen == p->callsmax) return NULL;
                return &p->callstack[p->callslen++];
            }

            // returns 0 or 1
            static int ${prefix_}sems_available(${prefix_}parser_t *p, size_t n){
                return n <= p->semsmax;
            }

            static void ${prefix_}free_sems(${prefix_}parser_t *p, size_t first){
                // free all possible sems, according to the last call
                if(p->callslen == 0) return;
                ${prefix_}call_t clast = p->callstack[p->callslen-1];
                size_t end = clast.stack + clast.nsems;
                for(size_t i = first; i < end; i++){
                    ${prefix_}sem_free(&p->semstack[i]);
                }
            }

            void ${prefix_}parser_reset(${prefix_}parser_t *p){
                ${prefix_}free_sems(p, 0);
                p->callslen = 0;
                ${prefix_}mask_clear(p->mask);
            }

            void ${prefix_}parser_free(${prefix_}parser_t **pp){
                ${prefix_}parser_t *p = *pp;
                if(!p) return;
                ${prefix_}parser_reset(p);
                free(p->callstack);
                free(p->semstack);
                free(p);
                *pp = NULL;
            }

            ${prefix_}parser_t *${prefix_}parser_new(size_t callsmax, size_t semsmax){
                ${prefix_}parser_t *p = malloc(sizeof(*p));
                *p = (${prefix_}parser_t){
                    .callstack = malloc(sizeof(p->callstack[0]) * callsmax),
                    .callsmax = callsmax,
                    .semstack = malloc(sizeof(p->semstack[0]) * callsmax),
                    .semsmax = semsmax,
                };

                if(!p->callstack || !p->semstack){
                    ${prefix_}parser_free(&p);
                }else{
                    memset(p->callstack, 0, sizeof(p->callstack[0]) * callsmax);
                    memset(p->semstack, 0, sizeof(p->semstack[0]) * callsmax);
                }

                return p;
            }

            static const char *_${prefix_}fn_name(${prefix_}parse_f fn);

            int ${prefix_}snprint_stack(char *buf, size_t limit, ${prefix_}parser_t *p){
                int allowed = limit > INT_MAX ? INT_MAX : (int)limit;
                int written = 0;
                for(size_t i = 0; i < p->callslen; i++){
                    ${prefix_}call_t call = p->callstack[i];
                    int w = snprintf(
                        buf + written,
                        (size_t)(allowed - written),
                        "parsing %s (%s)\n",
                        _${prefix_}fn_name(call.fn), call.summary
                    );
                    if(w < 0){
                        if(limit) buf[limit-1] = '\0';
                        return w;
                    }
                    written += w;
                }
                if(limit) buf[limit-1] = '\0';
                return written;
            }

            int ${prefix_}fprint_stack(FILE *f, ${prefix_}parser_t *p){
                int written = 0;
                for(size_t i = 0; i < p->callslen; i++){
                    ${prefix_}call_t call = p->callstack[i];
                    int w = fprintf(
                        f, "parsing %s (%s)\n",
                        _${prefix_}fn_name(call.fn), call.summary
                    );
                    if(w < 0) return w;
                }
                return written;
            }

            #define ${PREFIX_}CALL(_fn, _stack) do { \
                ${prefix_}call_t *subcall = ${prefix_}calls_append(p); \
                if(!subcall) return ${PREFIX_}STATUS_CALLSTACK_OVERFLOW; \
                *subcall = (${prefix_}call_t){ \
                    .fn = _fn, \
                    .stack = call->stack + _stack, \
                }; \
                return 0; \
            } while(0)

            #define ${PREFIX_}AWAIT_TOKEN \
                if(!*token) return 0

            #define ${PREFIX_}CONSUME_TOKEN \
                *token = _${PREFIX_}NOT_A_TOKEN

            // accepts `n` token types returns 0 or 1
            static int ${prefix_}token_in(${prefix_}token_e token, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                int out = 0;
                for(size_t i = 0; i < n; i++){
                    if(token != va_arg(ap, ${prefix_}token_e)) continue;
                    out = 1;
                    break;
                }

                va_end(ap);
                return out;
            }

            #define ${PREFIX_}SYNTAX_ERROR(_token, _sem) do { \
                $error_fn( \
                    p$args, \
                    _token, \
                    _sem, \
                    (const unsigned char*)p->mask, \
                    (const char*)call->summary \
                ); \
                return ${PREFIX_}STATUS_SYNTAX_ERROR; \
            } while(0)

            #ifdef ${PREFIX_}HAVE_SEMLOC
            static void ${prefix_}process_matched_loc(
                ${prefix_}parser_t *p, ${prefix_}call_t *call, $semloc_type matched
            ){
                if(!call->have_loc){
                    call->loc_start = matched;
                    call->have_loc = 1;
                }
                call->loc_end = matched;
                p->last = matched;
                p->prev = &p->last;
            }
            #endif

            static ${prefix_}status_e _${prefix_}do_parse(
                ${prefix_}parser_t *p,
                ${prefix_}parse_f entrypoint$params,
                ${prefix_}token_e token,
                ${prefix_}sem_t sem,
                ${prefix_}sem_t *sem_out
            ){
                if(p->callslen == 0){
                    // first token, initialize output at semstack[0]
                    if(!${prefix_}sems_available(p, 1)){
                        return ${PREFIX_}STATUS_SEMSTACK_OVERFLOW;
                    }
            #ifdef ${PREFIX_}HAVE_SEMLOC
                    // initialize the location tracking
                    p->prev = NULL;
            #endif
                    // initialize the first call
                    ${prefix_}call_t *call = ${prefix_}calls_append(p);
                    if(!call) return ${PREFIX_}STATUS_CALLSTACK_OVERFLOW;
                    *call = (${prefix_}call_t){ .fn = entrypoint, .stack = 0 };
                }

                while(p->callslen){
                    size_t last = p->callslen;
                    ${prefix_}call_t *call = &p->callstack[p->callslen-1];
                    ${prefix_}status_e status = call->fn(call, p$args, &token, sem);
                    if(status > ${PREFIX_}STATUS_SYNTAX_ERROR){
                        // CALLSTACK_OVERFLOW or SEMSTACK_OVERFLOW
                        return status;
                    }
                    if(!status){
                        if(p->callslen == last){
                            // pause until the next token
                            return ${PREFIX_}STATUS_OK;
                        }
                        continue;
                    }
                    // syntax error; pop from the stack until we can recover
                    int recovered = 0;
                    while(p->callslen){
                        ${prefix_}call_t *call = &p->callstack[p->callslen-1];
                        if(call->recover){
                            // this call has a recovery state we can enter
                            call->state = call->recover;
                            call->recover = 0;
                            recovered = 1;
                            break;
                        }else{
                            ${prefix_}free_sems(p, call->stack);
                            p->callslen--;
                        }
                    }
                    if(!recovered){
                        return ${PREFIX_}STATUS_SYNTAX_ERROR;
                    }
                }

                // parsed everything!
                *sem_out = p->semstack[0];
                p->semstack[0] = (${prefix_}sem_t){0};
                ${prefix_}parser_reset(p);
                return ${PREFIX_}STATUS_DONE;
            }
        """)

        if self.error_fn is None:
            # default error function
            self.fprint("")
            self.fprint_multi(r"""
                static void $error_fn(
                    ${prefix_}parser_t *p$params,
                    ${prefix_}token_e token,
                    ${prefix_}sem_t sem,
                    const unsigned char *expected_mask,
                    const char *loc_summary
                ){
                    (void)sem; (void)loc_summary;$voids
                    fprintf(stderr, "syntax error!\n");
                    ${prefix_}fprint_stack(stderr, p);
                    fprintf(stderr, "expected one of: {");
                    ${prefix_}fprint_mask(stderr, expected_mask, ",");
                    fprintf(stderr, "} but got %s\n", ${prefix_}token_name(token));
                }
            """)

        # declare functions
        for _, expr in self.grammar.sorted_expressions():
            self.declare_fn(expr)
        self.fprint("")

        self.fprint("const char *${prefix_}token_name(${prefix_}token_e t){")
        self.fprint("    switch(t){")
        for name, _ in self.grammar.sorted_tokens():
            self.fprint("        case ${PREFIX_}" + name + ": return \"" +name+ "\";")
        self.fprint("        default: return \"unknown\";")
        self.fprint("    }")
        self.fprint("}")
        self.fprint("")

        # get function names
        self.fprint(
            "static const char *_${prefix_}fn_name(${prefix_}parse_f fn){"
        )
        for _, expr in self.grammar.sorted_expressions():
            self.fprint(
                "    if(fn == " + self.parse_fn(expr) + ")"
                + " return \""+expr.name+"\";"
            )
        self.fprint("    return \"unknown\";")
        self.fprint("}")
        self.fprint("")

        # define functions
        for _, expr in self.grammar.sorted_expressions():
            self.define_fn(expr)

        # define user apis
        for root in self.roots:
            self.define_user_api(root)
            self.fprint("")

        self.print_code(self.postcode, "")


class Python:
    """
    An object with all the methods for generating Python code.
    """
    code_tag = "py"

    def __init__(
        self,
        ctx,
        *,
        span_fn=None,
        zero_loc_fn=None,
    ):
        self.grammar = ctx.grammar
        self.file = ctx.file
        assert ctx.roots, "you must specify at least one %root"
        self.roots = ctx.roots
        self.precode = ctx.precode
        self.postcode = ctx.postcode
        self.types = ctx.types
        self.fallbacks = ctx.fallbacks
        self.prefix = ctx.prefix
        self.params = ctx.params

        if list(self.types.source_ordered()):
            print(
                "warning: %type directives are ignored by the python generator",
                file=sys.stderr
            )

        self.prefix_ = (self.prefix + "_") if self.prefix else ""
        assert bool(span_fn) + bool(zero_loc_fn) in (0, 2), (
            "span_fn and zero_loc_fn must be both set or both unset"
        )
        self.span_fn = span_fn
        self.zero_loc_fn = zero_loc_fn
        if any(p.type for p in self.params):
            print(
                "warning: the type argument of %param directive is ignored "
                "by the python generator",
                file=sys.stderr
            )
        self.u = [p.name for p in self.params]

        # build a default mapping
        args_str = ", ".join(self.u)
        self.varmap = {
            "prefix": self.prefix,
            "prefix_": self.prefix_,
            "u": args_str,
            "_u": "" if not args_str else ", " + args_str,
            "u_": "" if not args_str else args_str + ", ",
            "span_fn": self.span_fn,
            "zero_loc_fn": self.zero_loc_fn,
        }

        self._extra_vars = {}

        # give every counter a unique name
        self.ncounters = 0

    def sub(self, s):
        return string.Template(s).substitute(**self.varmap, **self._extra_vars)

    def fprint(self, s):
        print(self.sub(s), file=self.file)

    def fprint_multi(self, s, i=""):
        print(
            self.sub(textwrap.indent(textwrap.dedent(s).strip("\n"), i)),
            file=self.file,
        )

    @contextlib.contextmanager
    def extra_vars(self, **mapping):
        old_vars = dict(**self._extra_vars)
        self._extra_vars.update(mapping)
        try:
            yield
        finally:
            self._extra_vars = old_vars

    def print_code(self, code, i):
        if code is None:
            return
        for c in code:
            if c.tag in (None, self.code_tag):
                self.fprint(i + "# USER CODE")
                break
        for c in code:
            if c.tag in (None, self.code_tag):
                text = re.sub("\\$\\$", "(out.val)", c.text)
                if self.span_fn:
                    text = re.sub("@\\$", "(out.loc)", text)
                text = re.sub("\\$([a-zA-Z_][a-zA-Z0-9_]*)", "(yy_\\1.val)", text)
                if self.span_fn:
                    text = re.sub("@([a-zA-Z_][a-zA-Z0-9_]*)", "(yy_\\1.loc)", text)
                # don't use fprint; we shouldn't be subbing in internal vars
                print(
                    textwrap.indent(textwrap.dedent(text).strip("\n"), i),
                    file=self.file,
                )

    def syntax_error(self, loc, i):
        args = "" if not self.span_fn else ("loc=" + loc)
        self.fprint(i + "raise ${prefix}SyntaxError("+ args + ")")

    def token_list(self, tokens, fallbacks=()):
        assert tokens
        tokens = sorted(t for t in tokens if t is not None) + sorted(fallbacks)
        if len(tokens) > 1:
            return "(" + ", ".join(tokens) + ")"
        else:
            return "(" + tokens[0] + ",)"

    def if_token_in(self, tokens, fallbacks, i, pos=0):
        token_list = self.token_list(tokens, fallbacks)
        lead = "if" if pos == 0 else "elif"
        with self.extra_vars(token_list=token_list, lead=lead):
            self.fprint(i + "$lead (yield from tokens.peek()) in $token_list:")

    def if_token_not_in(self, tokens, fallbacks, i, pos=0):
        token_list = self.token_list(tokens, fallbacks)
        lead = "if" if pos == 0 else "elif"
        with self.extra_vars(token_list=token_list, lead=lead):
            self.fprint(i + "$lead (yield from tokens.peek()) not in $token_list:")

    def while_token_in(self, tokens, fallbacks, i, pre=None):
        token_list = self.token_list(tokens, fallbacks)
        pre = pre and (pre+" or ") or ""
        with self.extra_vars(token_list=token_list, pre=pre):
            self.fprint(i + "while $pre(yield from tokens.peek()) in $token_list:")

    def while_token_not_in(self, tokens, fallbacks, i):
        token_list = self.token_list(tokens, fallbacks)
        with self.extra_vars(token_list=token_list):
            self.fprint(i + "while (yield from tokens.peek()) not in $token_list:")

    def gen(self, obj, tag, i, want_final_loc=False):
        try:
            name = obj.defn.name if isinstance(obj, Reference) else None
            with self.extra_vars(
                name=name,
                tag="" if not tag else "yy_%s"%tag,
            ):
                self._gen(obj, tag, i, want_final_loc)
        except Exception as e:
            raise RuntimeError("generating for " + obj.summary()) from e

    def _gen(self, obj, tag, i, want_final_loc):
        if isinstance(obj, Reference):
            if isinstance(obj.defn, Token):
                tokens = {obj.defn.name}.union(self.fallbacks[obj.defn.name])
                token_list = self.token_list(tokens)
                self.fprint(i + "# " + obj.summary())
                self.fprint(i + "val = (yield from next(tokens))")
                self.fprint(i + "if val.type not in %s:"%token_list)
                self.syntax_error("val.loc", i + "    ")
                if self.span_fn:
                    self.fprint(i + "out.loc = $span_fn(out.loc, val.loc)")
                if tag:
                    self.fprint(i + "$tag = val")
                return

            if isinstance(obj.defn, Expression):
                self.fprint(i + "# " + obj.summary())
                self.fprint(
                    i + "val = (yield from _${prefix_}parse_$name(tokens${_u}))"
                )
                if self.span_fn:
                    self.fprint(i + "out.loc = $span_fn(out.loc, val.loc)")
                if tag:
                    self.fprint(i + "$tag = val")
                return

            raise RuntimeError("unrecognized reference type: " + type(obj.defn).__name__)

        if isinstance(obj, Repeat):
            if obj.type == "maybe":
                self.fprint(i + "# " + obj.summary())
                t = obj.get_first()
                f = self.fallbacks.all_fallbacks(t)
                self.if_token_in(t, f, i)
                self.gen(obj.seq, None, i+"    ")
                return

            if obj.type == "zero_or_more":
                self.fprint(i + "# " + obj.summary())
                t = obj.get_first()
                f = self.fallbacks.all_fallbacks(t)
                self.while_token_in(t, f, i)
                self.gen(obj.seq, None, i+"    ")
                return

            # general case
            counter = "count%d"%self.ncounters
            self.ncounters += 1
            with self.extra_vars(
                counter=counter, rmin=obj.rmin, rmax=obj.rmax
            ):
                self.fprint(i + "# " + obj.summary())
                self.fprint(i + "$counter = 0")
                if obj.rmin:
                    pre = "%s < %d"%(counter, obj.rmin)
                else:
                    pre = None
                t = obj.get_first()
                f = self.fallbacks.all_fallbacks(t)
                self.while_token_in(t, f, i, pre)
                self.fprint(i + "    $counter += 1")
                self.gen(obj.seq, None, i+"    ")
            return

        if isinstance(obj, Branches):
            self.fprint(i + "# " + obj.summary())
            exclude = obj.get_first()
            for n, seq in enumerate(obj.branches):
                t = seq.get_first()
                f = self.fallbacks.all_fallbacks(t, exclude)
                self.if_token_in(t, f, i, pos=n)
                self.gen(seq, None, i+"    ")
            self.fprint(i + "else:")
            self.syntax_error("(yield from next(tokens)).loc", i + "    ")
            return

        if isinstance(obj, Recovery):
            self.fprint(i + "# " + obj.summary())
            self.fprint(i + "try:")
            self.gen(obj.seq, None, i+"    ")
            t = obj.after
            f = self.fallbacks.all_fallbacks(t)
            self.if_token_not_in(t, f, i+"    ")
            self.fprint(i + "        raise ${prefix}SyntaxError()")
            self.fprint(i + "except ${prefix}SyntaxError:")
            self.while_token_not_in(t, f, i+"    ")
            self.fprint(i + "        yield from next(tokens)")
            self.print_code(obj.code, i+"    ")
            return

        if isinstance(obj, Sequence):
            self.print_code(obj.precode, i)
            for term, tag, code in obj.terms:
                self.gen(term, tag, i)
                self.print_code(code, i)
            return

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def define_fn(self, expr):
        # reset counter naming
        self.ncounters = 0
        with self.extra_vars(name=expr.name, summary=expr.seq.summary()):
            self.fprint("# $name = $summary")
            self.fprint("def _${prefix_}parse_$name(tokens${_u}):")
            self.fprint("    out = ${prefix}Expression()")
            if self.span_fn:
                self.fprint("    out.loc = $zero_loc_fn(tokens.last_loc)")
            self.gen(expr.seq, None, "    ")
            self.fprint("    return out")

    def define_user_api(self, root):
        with self.extra_vars(root=root):
            self.fprint_multi("""
            class ${prefix}${root}Parser:
                def __init__(self${_u}, repeat=False):
                    self.tokens = ${prefix}TokenStream()
                    self.out = None
                    self.args = ($u_)
                    self.gen = self.mkgen()
                    self.repeat = repeat
                    self.done = False

                def mkgen(self):
                    def _gen():
                        self.out = yield from _${prefix_}parse_$root(self.tokens, *self.args)
                    gen = _gen()
                    next(gen)
                    return gen

                def feed(self, t):
                    if self.done:
                        raise RuntimeError("parser is already done!")
                    try:
                        return self.gen.send(t)
                    except StopIteration:
                        self.gen = None
                        out = self.out
                        self.out = None
                        self.gen = self.mkgen()
                        if not self.repeat:
                            self.done = True
                        return out
            """)

    def gen_file(self):
        self.print_code(self.precode, "")
        if self.precode:
            self.fprint("")

        self.fprint_multi(r"""
            class ${prefix}SyntaxError(Exception):
                def __init__(self, msg=None, loc=None):
                    self.loc = loc

                def __repr__(self):
                    return str(msg) + ": " + str(self.loc)

            class ${prefix}Expression:
                def __init__(self, val=None, loc=None):
                    self.val = val
                    self.loc = loc

                def __repr__(self):
                    return "${prefix}Expression(val=%s)"%(str(self.val))

            class ${prefix}Token:
                def __init__(self, typ, val=None, loc=None):
                    self.type = typ
                    self.val = val
                    self.loc = loc

                def __repr__(self):
                    return "${prefix}Token(%s, %s)"%(str(self.type), str(self.val))

            class ${prefix}TokenStream:
                def __init__(self):
                    self.first = None
                    self.last_loc = None
                    self.count = 0

                def peek(self):
                    if self.first is None:
                        self.first = yield
                    return self.first.type

                def __next__(self):
                    if self.first:
                        out = self.first
                        self.first = None
                    else:
                        out = yield
                    self.count += 1
                    self.last_loc = out.loc
                    return out
        """)
        self.fprint("")

        for i, (name, _) in enumerate(self.grammar.sorted_tokens()):
            self.fprint(name + " = " + str(i))
        self.fprint("")

        for name, expr in self.grammar.sorted_expressions():
            self.define_fn(expr)
            self.fprint("")

        for root in self.roots:
            self.define_user_api(root)
            self.fprint("")

        self.print_code(self.postcode, "")


### Meta Parser ###

class ParsedName:
    def __init__(self, name, loc):
        self.name = name
        self.tag = None
        self.loc = loc

    def process(self, g, e):
        x = g.get_name(self.name)
        if x.loc is None:
            # implicit initialization, set a location
            x.loc = self.loc
        e.match(x, self.tag, self.loc)

class ParsedSnippet:
    def __init__(self, text, loc):
        self.text = text
        self.tag = None
        self.loc = loc

    def process(self, g, e):
        e.exec(self.text, self.tag, self.loc)

class ParsedBranches:
    def __init__(self):
        self.branches = []
        self.loc = None

    def process(self, g, e):
        # Collapse single-branch expressions.
        if len(self.branches) == 1:
            self.branches[0].process(g, e)
            return

        with e.branches(self.loc) as b:
            for parsed_branch in self.branches:
                with b.branch():
                    parsed_branch.process(g, e)

class ParsedRecovery:
    def __init__(self, sub):
        self.sub = sub
        self.code = []
        self.loc = None

    def process(self, g, e):
        with e.recovery(loc=self.loc) as r:
            self.sub.process(g, e)
            for c in self.code:
                r.add_recovery_code(c.text, c.tag)

class ParsedRepeat:
    def __init__(self):
        self.term = None
        self.rmin = 0
        self.rmax = None
        self.loc = None

    def process(self, g, e):
        with e.repeat(self.rmin, self.rmax):
            self.term.process(g, e)

class ParsedSequence:
    def __init__(self):
        self.elems = []
        self.loc = None

    def process(self, g, e):
        for elem in self.elems:
            elem.process(g, e)

class ParsedGenerator:
    def __init__(self, name):
        self.name = name
        self.loc = None

class ParsedKwarg:
    def __init__(self):
        self.key = None
        self.value = None
        self.tag = None
        self.loc = None

class ParsedRoot:
    def __init__(self):
        self.name = None
        self.tag = None
        self.loc = None

class ParsedFallback:
    def __init__(self):
        self.to_type = None
        self.tag = None
        self.from_types = []
        self.loc = None

class ParsedParam:
    def __init__(self):
        self.tag = None
        self.name = None
        self.type = None
        self.loc = None

class ParsedPrefix:
    def __init__(self):
        self.tag = None
        self.prefix = None
        self.loc = None

class ParsedType:
    def __init__(self):
        self.name = None
        self.code = None
        self.destructor = None
        self.tag = None
        self.loc = None

class ParsedDoc:
    def __init__(self):
        self.precode = []
        self.defs = []
        self.postcode = []
        # directive handling
        self.generator = None
        self.prefixes = []
        self.kwargs = []
        self.roots = []
        self.types = []
        self.fallbacks = []
        self.params = []

    def add_stmt(self, stmt):
        if isinstance(stmt, ParsedGenerator):
            if self.generator is not None:
                raise RenderedError("received multiple %generator directives")
            self.generator = stmt.name
            return
        if isinstance(stmt, ParsedKwarg):
            self.kwargs.append(stmt)
            return
        if isinstance(stmt, ParsedType):
            self.types.append(stmt)
            return
        if isinstance(stmt, ParsedRoot):
            self.roots.append(stmt)
            return
        if isinstance(stmt, ParsedFallback):
            self.fallbacks.append(stmt)
            return
        if isinstance(stmt, ParsedParam):
            self.params.append(stmt)
            return
        if isinstance(stmt, ParsedPrefix):
            self.prefixes.append(stmt)
            return
        if isinstance(stmt, tuple):
            self.defs.append(stmt)
            return
        raise RuntimeError("unknown statement of type %s"%type(stmt).__name__)

    def build_grammar(self, text=None):
        g = Grammar(text=text)

        # first declare all tokens and expressions
        for name, val in self.defs:
            if val is None:
                _ = g.token(name.name, name.tag, name.loc)
            else:
                e = g.expr(name.name)
                e.type = name.tag
                e.loc = text_span(name.loc, val.loc)

        # then define all the expressions
        for name, val in self.defs:
            if val is None:
                continue
            e = g.get_name(name.name)
            val.process(g, e)

        return g

# BEGIN GENERATED CODE
# END GENERATED CODE

class TextLocation:
    def __init__(self, line, char):
        self.line = line
        self.char = char

    def __repr__(self):
        return "(line=%d, char=%d)"%(self.line, self.char)

class TextSpan:
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __repr__(self):
        return "(start=%s, end=%s)"%(self.start, self.end)

def text_span(loc, token_loc):
    return TextSpan(loc.start, token_loc.end)

def text_zero_loc(prev):
    if prev is None:
        pos = TextLocation(0, 0)
    else:
        pos = TextLocation(prev.end.line, prev.end.char+1)
    return TextSpan(pos, pos)

class CharStream:
    def __init__(self):
        self._lookahead = []
        # We start in line 1, char 1.
        self.line = 1
        # char is a position within this line
        self.char = 1

    def peek(self, index=0):
        while len(self._lookahead) <= index:
            temp = yield
            self._lookahead.append(temp)
        return self._lookahead[index]

    def __next__(self):
        if self._lookahead:
            c = self._lookahead.pop(0)
        else:
            c = yield
        # newlines are considered the end of this current line
        loc = TextLocation(self.line, self.char)
        if c == '\n':
            self.line += 1
            self.char = 1
        else:
            self.char += 1
        return c, loc

    def empty(self):
        return not self._lookahead

class Discard:
    pass

class Tokenizer:
    def __init__(self):
        self.cs = CharStream()
        self.out = []

        def _gen():
            yield from self._tokenize_many()

        self.gen = _gen()
        next(self.gen)

    def feed(self, c):
        if self.gen is None:
            raise RuntimeError("tokenizer is already done!")

        if c is None:
            try:
                # This should finish the generator.
                self.gen.send(None)
                raise RuntimeError("tokenizer failed to exit after None")
            except StopIteration:
                return self.out

        self.gen.send(c)
        out = self.out
        self.out = []
        return out

    def _tokenize_many(self):
        while True:
            token = yield from self._tokenize()
            if token is None:
                break
            if isinstance(token, Discard):
                continue
            self.out.append(token)

    def _tokenize(self):
        c = yield from self.cs.peek()

        if c is None:
            return None

        if c in " \r\n\t":
            return (yield from self._ignore_whitespace())

        if c in "#":
            return (yield from self._ignore_comment())

        singles = {
            "*": ASTERISK,
            ":": COLON,
            "=": EQ,
            "<": LANGLE,
            "[": LBRACKET,
            "(": LPAREN,
            "%": PERCENT,
            "|": PIPE,
            "?": QUESTION,
            ">": RANGLE,
            "]": RBRACKET,
            ")": RPAREN,
            ";": SEMI,
        }
        if c in singles:
            _, loc = yield from next(self.cs)
            return MetaToken(singles[c], c, TextSpan(loc, loc))

        if c == "{":
            return (yield from self._tokenize_code())

        if c in "0123456789":
            return (yield from self._tokenize_num())

        if c in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_":
            return (yield from self._tokenize_text())

        _, loc = yield from next(self.cs)
        msg = "invalid character in tokenizer: " + str(c.encode('utf8'))
        raise MetaSyntaxError(msg, TextSpan(loc, loc))

    def _ignore_whitespace(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c not in " \r\n\t":
                break
            yield from next(self.cs)
        return Discard()

    def _ignore_comment(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c in "\r\n":
                break
            yield from next(self.cs)
        return Discard()

    def _tokenize_code(self):
        # Some number of leading '{' chars which we discard...
        count = 0
        start = None
        while True:
            c = yield from self.cs.peek()
            if c != "{":
                break
            _, loc = yield from next(self.cs)
            if start is None:
                start = loc
            count += 1

        # ... and code body, which we keep...
        text = ""
        while True:
            c = yield from self.cs.peek()
            if c is None:
                raise RenderedError("unterminated code block:\n" + text)
            if c != "}":
                c, loc = yield from next(self.cs)
                text += c
                continue

            # ... and a matching number of closing '}' chars which we discard.
            matched = 1
            while matched < count:
                c = yield from self.cs.peek(matched)
                if c != "}":
                    break
                matched += 1

            if matched != count:
                # oops, these were actually part of the code body
                for _ in range(matched):
                    c, _ = yield from next(self.cs)
                    text += c
                continue

            # discard these, they're not part of the code body
            for _ in range(matched):
                _, loc = yield from next(self.cs)
            return MetaToken(CODE, text, TextSpan(start, loc))

    def _tokenize_num(self):
        text = ""
        start = None
        loc = None
        while True:
            c = yield from self.cs.peek()
            if c is None:
                break
            if c not in "0123456789":
                break
            c, loc = yield from next(self.cs)
            text += c
            if start is None:
                start = loc
        return MetaToken(NUM, int(text), TextSpan(start, loc))

    def _tokenize_text(self):
        text = ""
        start = None
        loc = None
        while True:
            c = yield from self.cs.peek()
            if c is None:
                break
            if c not in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_0123456789":
                break
            c, loc = yield from next(self.cs)
            text += c
            if start is None:
                start = loc
        keywords = {
            "generator": GENERATOR,
            "kwarg": KWARG,
            "type": TYPE,
            "root": ROOT,
            "fallback": FALLBACK,
            "param": PARAM,
            "prefix": PREFIX,
        }
        typ = keywords.get(text, TEXT)
        return MetaToken(typ, text, TextSpan(start, loc))

    def iter(self, chars):
        for c in chars:
            tokens = self.feed(c)
            if tokens is None:
                break
            yield from tokens

        eof_loc = TextLocation(line=-1, char=-1)
        yield MetaToken(EOF, "", TextSpan(eof_loc, eof_loc))


@contextlib.contextmanager
def read_template(template, tag="GENERATED CODE", file=None):
    if template is None:
        yield 0
        return

    with open(template) as f:
        lines = iter(enumerate(f))
        start_offset = 0
        # passtru pre-tag lines
        for i, line in lines:
            print(line, end="", file=file)
            if "BEGIN " + tag in line:
                start_offset = i + 1
                break
        else:
            print('did not find "BEGIN ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # discard inter-tag lines
        for _, line in lines:
            if "END " in line:
                yield  # generated code gets printed here
                print(line, end="", file=file)
                break
        else:
            print('did not find "END ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # passtru post-tag lines
        for _, line in lines:
            print(line, end="", file=file)


def parse_doc(grammar_text):
    parser = MetadocParser()

    try:
        for t in Tokenizer().iter(grammar_text):
            # the last time this is called it should return non-None
            parsed_doc = parser.feed(t)
    except MetaSyntaxError as e:
        msg = "syntax error at " + render_location(e.loc, grammar_text)
        raise RenderedError(msg) from e

    assert parsed_doc, "didn't get a doc!"

    return parsed_doc.val

def inspect_def(g, name, body):
    if name not in g.exprs:
        return "no object in grammar by name of '%s'"%name
    obj = g.exprs[name]
    if isinstance(obj, Token):
        out = "token %s:\n"%name
        if obj.loc is not None:
            out += textwrap.indent(
                "defined at " + render_location(obj.loc, body), "    "
            )
        return out
    out = "expression %s = %s\n"%(name, obj.seq.summary())
    out += "    first: [%s]\n"%(",".join(str(f) for f in obj.seq.get_first()))
    out += "    disallowed_after: [%s]\n"%(
        ",".join(str(f) for f in obj.seq.get_disallowed_after())
    )
    out += "    recursion path: %s\n"%str(obj.recurses())
    if obj.loc is not None:
        out += textwrap.indent(
            "defined at " + render_location(obj.loc, body, underline=False),
            "    ",
        )
    return out

def gen(
    grammar_text,
    generator,
    out,
    template=None,
    template_tag=None,
    action="gen_file",
    cli_kwargs=(),
    inspect=(),
    src=None,
    dst=None,
):
    with read_template(template, template_tag, out) as start_offset:
        parsed_doc = parse_doc(grammar_text)

        g = parsed_doc.build_grammar(grammar_text)

        for i in inspect:
            print(inspect_def(g, i, grammar_text), file=sys.stderr)
        if inspect:
            print(file=sys.stderr)

        try:
            g.check()
        except GrammarError as e:
            msg = e.render(grammar_text)
            raise RenderedError(msg) from e

        # TODO: support importing a user-defined generator at runtime
        generator_name = parsed_doc.generator
        if generator:
            # cli --generator overrides %generator directive
            generator_name = generator
        assert generator_name is not None, "neither --generator nor %generator was found"
        generator_cls = {"py": Python, "c": C}[generator_name]

        kwargs = {}
        # extract %kwarg directives
        for kwarg in parsed_doc.kwargs:
            if kwarg.tag not in (None, generator_cls.code_tag):
                continue
            assert kwarg.key not in kwargs, "got multiple %%kwarg for key '%s'"%kwarg.key
            kwargs[kwarg.key] = kwarg.value

        # cli kwargs override %kwarg directives
        for kwarg in cli_kwargs:
            assert "=" in kwarg, "kwarg '%s' is not of the form KEY=VALUE"%kwarg
            k, v = kwarg.split("=", 1)
            kwargs[k] = v

        # extract roots
        roots = []
        root_names = set()
        for r in parsed_doc.roots:
            if r.tag not in (None, generator_cls.code_tag):
                continue
            assert r.name not in root_names, (
                "got multiple '%%root %s' directives"%r.name
            )
            assert r.name in g.exprs, (
                "'%%root %s': %s is not a known expression"%(r.name, r.name)
            )
            assert isinstance(g.exprs[r.name], Expression), (
                "'%%root %s': %s is a token, not an expression"%(
                    r.name, r.name
                )
            )
            roots.append(r.name)
            root_names.add(r.name)

        # extract params
        params = []
        names = set()
        for p in parsed_doc.params:
            if p.tag not in (None, generator_cls.code_tag):
                continue
            assert p.name not in names, "got multiple '%%param %s' directives"%p.name
            names.add(p.name)
            params.append(p)

        # extract types
        types = extract_types(parsed_doc.types, g, generator_cls.code_tag)

        # extract fallbacks
        fallbackmap = extract_fallbacks(
            parsed_doc.fallbacks, g, generator_cls.code_tag
        )
        fallbacks = Fallbacks(fallbackmap)

        # extract prefix
        prefix = None
        for p in parsed_doc.prefixes:
            if p.tag not in (None, generator_cls.code_tag):
                continue
            assert prefix is None, "got multiple %prefix directives"
            prefix = p.prefix

        ctx = GeneratorContext(
            grammar=g,
            roots=roots,
            prefix=prefix or "",
            src=src,
            dst=dst,
            dst_offset=start_offset,
            file=out,
            precode=parsed_doc.precode,
            postcode=parsed_doc.postcode,
            types=types,
            fallbacks=fallbacks,
            params=params,
        )

        generator = generator_cls(ctx, **kwargs)

        try:
            action = getattr(generator, action)
        except AttributeError:
            print(
                "error: generator class '%s' has no method matching the given "
                "--action ('%s')"%(generator_cls.__name__, action),
                file=sys.stderr
            )
            sys.exit(1)
        action()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("input")
    parser.add_argument("-t", "--template")
    parser.add_argument("-T", "--template-tag", default="GENERATED CODE")
    parser.add_argument("-o", "--output")
    parser.add_argument("-c", "--check", action="store_true")
    parser.add_argument("-g", "--generator")
    parser.add_argument("-a", "--action", default="gen_file")
    parser.add_argument("-I", "--inspect", action="append", default=[])
    parser.add_argument("kwarg", nargs="*")
    args = parser.parse_args()

    assert not args.check or args.output, "--check requires --output"

    if args.input is None:
        text = sys.stdin.read()
    else:
        with open(args.input) as f:
            text = f.read()

    out = io.StringIO()

    try:
        gen(
            grammar_text=text,
            generator=args.generator,
            out=out,
            template=args.template,
            template_tag=args.template_tag,
            action=args.action,
            cli_kwargs=args.kwarg,
            inspect=args.inspect,
            src=args.input,
            dst=args.output,
        )
    except RenderedError as e:
        print(e, file=sys.stderr)
        sys.exit(1)

    if args.check:
        with open(args.output) as f:
            old = f.read()
        if out.getvalue() == old:
            sys.exit(0)
        print(args.output + " would be modified", file=sys.stderr)
        sys.exit(1)

    if args.output is not None:
        with open(args.output, "w") as f:
            f.write(out.getvalue())
    else:
        print(out.getvalue(), end="", flush=True)
