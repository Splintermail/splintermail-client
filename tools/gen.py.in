#!/usr/bin/env python3
"""
gen.py: a recursive-descent parser generator, optimized for simplicity and
        fully in the public domain.

gen.py has embedded C and Python generators, but it will be possible in the
future to write a custom generator that is loaded dynamically by gen.py, so
any language should be supportable.

Both the embedded C parser and the embedded Python generate reentrant push
parsers.  gen.py's plugin architecture doesn't place requirements on the
properties of language generators, so custom language generators are allowed to
be simpler than the embedded generators, as desired.

gen.py is in alpha, and is still missing some key features.

TODO:
 - Python:
   - location tracking for expressions (right now only the tokenizer has it)
   - some state tracking, to avoid duplicate token checks
   - introduce a lookahead variable, to improve readability of generated code
   - reject roots which don't have a concrete ending token; the user api
     doesn't support it
   - support fallback

 - C:
   - give compiler source line hints
   - some state tracking, to avoid duplicate token checks
   - reject nested recovery blocks
   - reject roots which don't have a concrete ending token; the user api
     doesn't support it
   - set @$ before the final codeblock

 - Grammar:
   - use source-order everywhere
   - support 1* in addition to *

 - Meta parser:
   - location tracking (for compiler source line hints)
   - support an %include directive?

 - gen.py cli:
   - support importing generators from external python files
   - allow for input templates, so you can have one file with an embedded grammar
"""

import abc
import argparse
import contextlib
import io
import re
import sys
import textwrap
import string


class FirstFirst(Exception):
    pass

class FirstFollow(Exception):
    pass

class InfiniteRecursion(Exception):
    pass

def add_to_prev(prev, name, call):
    if prev is None:
        prev = []
    if name in prev:
        raise InfiniteRecursion(
            "circular " + call + "() detected in " + name + ": " + str(prev)
        )
    return list(prev) + [name]

def cacheable(fn):
    cache = None
    prop = "_cached_" + fn.__name__
    def _fn(self, *args, **kwargs):
        if not hasattr(self, prop):
            setattr(self, prop, fn(self, *args, **kwargs))
        return getattr(self, prop)
    return _fn


# When printing summaries, if you want to print something with a lower
# priority than your surrounding context, you need parens.
PRIORITY_UNAMBIGUOUS = 0
PRIORITY_BRANCHES = 1
PRIORITY_SEQUENCE = 2
PRIORITY_MULTIPLIER = 3


class Parsable(metaclass=abc.ABCMeta):
    @abc.abstractmethod
    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        pass

    @abc.abstractmethod
    def get_first(self, prev=None):
        pass

    @abc.abstractmethod
    def get_disallowed_after(self, prev=None):
        pass

    @abc.abstractmethod
    def check(self):
        pass


class Token(Parsable):
    def __init__(self, name, typ=None):
        self.name = name
        self.type = typ

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return self.name

    @cacheable
    def get_first(self, prev=None):
        return {self.name}

    @cacheable
    def get_disallowed_after(self, prev=None):
        return set()

    @cacheable
    def check(self):
        pass


class Snippet:
    def __init__(self, text, tag=None):
        self.text = text
        self.tag = tag


class Maybe(Parsable):
    def __init__(self, name):
        self.name = name
        self.seq = Sequence(self.name + ".seq")

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return "[" + self.seq.summary(PRIORITY_UNAMBIGUOUS) + "]"

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        # TODO: is this necessary at all?
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        return first.union({None})

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        assert None not in disallowed, "invalid .maybe() on maybe-empty expression"
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first()
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        # always include None, since this term may be missing
        return disallowed.union(first).union({None})

    @cacheable
    def check(self):
        self.seq.check()


class ZeroOrMore(Parsable):
    def __init__(self, name):
        self.name = name
        self.seq = Sequence(self.name + '.seq')

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return "*" + self.seq.summary(PRIORITY_MULTIPLIER)

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = self.seq.get_first(prev)
        assert None not in first, (
            "invalid .zero_or_more() on maybe-empty expression: "
            + self.seq.summary()
        )
        return first.union({None})

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = self.seq.get_disallowed_after(prev)
        assert None not in disallowed, "invalid .maybe() on maybe-empty expression"
        # since this might not exist, we disallow the this expression's first as well
        first = self.seq.get_first(prev)
        assert None not in first, "invalid .maybe() on maybe-empty expression"
        # always include None, since this term may be missing
        return disallowed.union(first).union({None})

    @cacheable
    def check(self):
        self.seq.check()


class Branches(Parsable):
    def __init__(self, name):
        self.name = name
        self.branches = []
        self._branch = None

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        out = " | ".join(b.summary(PRIORITY_BRANCHES) for b in self.branches)
        if parent_priority >= PRIORITY_BRANCHES:
            out = "(" + out + ")"
        return out

    def add_term(self, term, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_term(term, tag)

    def add_code(self, code, tag=None):
        assert self._branch is not None, (
            "inside a branches() context, but not any branch() subcontext!"
        )
        self._branch.add_code(code, tag)

    @contextlib.contextmanager
    def branch(self):
        assert self._branch is None, "can't nest calls to branch"
        self._branch = Sequence(self.name + '[' + str(len(self.branches)) + ']')
        yield
        self.branches.append(self._branch)
        self._branch = None

    @cacheable
    def get_first(self, prev=None):
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for branch in self.branches:
            first = first.union(branch.get_first(list(prev)))

        return frozenset(first)

    @cacheable
    def get_disallowed_after(self, prev):
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        prev = list(prev) + [self.name]
        disallowed = set()
        for branch in self.branches:
            disallowed = disallowed.union(branch.get_disallowed_after(list(prev)))

        return frozenset(disallowed)

    @cacheable
    def check(self):
        first = set()
        for branch in self.branches:
            branch.check()

            # no possibly-empty branches
            branch_first = branch.get_first()
            assert None not in branch_first

            # Ensure that no branches begin with the same tokens
            conflicts = first.intersection(branch_first)
            assert not conflicts, (
                "FIRST/FIRST conflicts for tokens [%s]"%(",".join(conflicts))
            )

            first = first.union(branch_first)


class Recovery(Parsable):
    def __init__(self, name, code=None, tag=None):
        self.name = name
        # allow defining the recovery code upfront
        self.code = [] if code is None else [Snippet(code, tag)]
        self.seq = Sequence(self.name + '.seq')
        self.after = None

    def add_term(self, term, tag=None):
        self.seq.add_term(term, tag)

    def add_code(self, code, tag=None):
        self.seq.add_code(code, tag)

    def add_recovery_code(self, code, tag=None):
        self.code.append(Snippet(code, tag))

    def set_after(self, after):
        assert self.after is None, "can't call set_after() twice!"
        self.after = after

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return "<" + self.seq.summary(PRIORITY_UNAMBIGUOUS) + " ? ... >"

    @cacheable
    def get_first(self, prev=None):
        return self.seq.get_first(prev)

    @cacheable
    def get_disallowed_after(self, prev):
        return self.seq.get_disallowed_after(prev)

    @cacheable
    def check(self):
        self.seq.check()


class Sequence(Parsable):
    """
    A sequence of terms which must be matched consecutively.
    """
    def __init__(self, name):
        self.name = name
        self.scopes = [self]
        self.precode = []
        # tuples of (Parsable, tag=None, code=[])
        self.terms = []

    def add_term(self, term, tag=None):
        self.terms.append((term, tag, []))

    def add_code(self, code, tag=None):
        if not self.terms:
            self.precode.append(Snippet(code, tag))
        else:
            self.terms[-1][2].append(Snippet(code, tag))

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        if len(self.terms) == 1:
            return self.terms[0][0].summary(parent_priority)
        out = " ".join(t.summary(PRIORITY_SEQUENCE) for t, _, _ in self.terms)
        if parent_priority > PRIORITY_SEQUENCE:
            out = "(" + out + ")"
        return out

    def get_first_ex(self, start, prev):
        """
        Get the list of all tokens that this expression could start with.

        Ex: at runtime, when are processing a .maybe(), we need to be able to
        look ahead one token and know if that token can be the first token
        of this expression.
        """
        prev = add_to_prev(prev, self.name, "get_first")
        first = set()
        for term, _, _ in self.terms[start:]:
            term_first = term.get_first(list(prev))
            # detect conflicts in .check(), not here.
            first = first.union(term_first)
            # if None is not in the set, we have our answer.
            if None not in first:
                break
            # if None is in the set, check the next term too.
            first.remove(None)
        else:
            # restore the None from the final term_first
            first.add(None)

        return frozenset(first)

    @cacheable
    def get_first(self, prev=None):
        return self.get_first_ex(0, prev)

    @cacheable
    def get_disallowed_after(self, prev=None):
        """
        Get the list of all tokens which would cause ambiguities after this
        expression.

        Ex. a maybe(EOL) term could not be followed by a match(EOL) term; it
        would be ambiguous if an EOL matched to the maybe() or the match().
        """
        prev = add_to_prev(prev, self.name, "get_disallowed_after")
        disallowed = set()
        for term, _, _ in reversed(self.terms):
            term_disallowed = term.get_disallowed_after(list(prev))
            disallowed = disallowed.union(term_disallowed)
            # if None is not in this term's disallowed, we have our answer
            if None not in disallowed:
                break
            # if None is in the set, check the next term too
            disallowed.remove(None)
        else:
            # restore the None from the final term_disallowed
            disallowed.add(None)

        return frozenset(disallowed)

    @cacheable
    def check(self):
        """
        At generation time, we need to ensure that for every Term in the
        expression, there is never a point where the lookahead token could
        possibly match the next token.
        """
        prev = {self.name}

        for term, _, _ in self.terms:
            # don't allow recursion
            if not isinstance(term, Expression):
                term.check()

        disallowed = set()
        for i, (term, _, _) in enumerate(self.terms):
            term_first = term.get_first(list(prev))
            # Ignore None when checking for conflicts
            disallowed = disallowed.difference({None})
            conflicts = disallowed.intersection(term_first)
            if conflicts:
                raise FirstFollow(
                    "FIRST/FOLLOW conflicts: [%s]\n"%(",".join(conflicts)) +
                    "found while checking expression: %s\n"%self.summary() +
                    "at least one token that starts term[%d] (%s)\n"%(
                        i, term.summary()
                    ) + "is disallowed by that point."
                )
            term_disallowed = term.get_disallowed_after(list(prev))
            if None in term_disallowed:
                # disallowed should grow
                disallowed = disallowed.union(term_disallowed)
            else:
                # disallowed is reset
                disallowed = term_disallowed

            # Special case: check Recovery terms and call their set_after().
            if isinstance(term, Recovery):
                after_recovery = self.get_first_ex(i+1, None)
                if None in after_recovery:
                    raise ValueError(
                        "a Recovery must be followed by a never-empty match "
                        "within the same Sequence"
                    )
                term.set_after(after_recovery)


class Expression(Parsable):
    """
    Each Expression will generate one function in the parser.
    """
    def __init__(self, name, typ=None):
        self.name = name
        self.type = typ
        self.seq = Sequence(self.name + ".seq")
        self.scopes = [self.seq]
        self.nmaybe = 0
        self.nzom = 0
        self.nbranches = 0
        self.nrec = 0

    # API for defining the grammar.

    def __call__(self, fn):
        """Provide a definition for a fowards-declared expression"""
        assert fn.__name__ == self.name, fn.__name__ + " != " + self.name
        fn(self)
        return self

    def match(self, term, tag=None):
        self.scopes[-1].add_term(term, tag)
        return term

    @contextlib.contextmanager
    def maybe(self):
        term = Maybe(self.name + '.maybe' + str(self.nmaybe))
        self.nmaybe += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def zero_or_more(self):
        term = ZeroOrMore(self.name + '.zom' + str(self.nzom))
        self.nzom += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def branches(self):
        term = Branches(self.name + '.br' + str(self.nbranches))
        self.nbranches += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    @contextlib.contextmanager
    def recovery(self, code=None, tag=None):
        term = Recovery(self.name + ".recovery" + str(self.nrec), code, tag)
        self.nrec += 1
        self.scopes.append(term)
        yield term
        self.scopes.pop()
        self.scopes[-1].add_term(term)

    def exec(self, code, tag=None):
        self.scopes[-1].add_code(code, tag)

    # Non-user-facing methods.

    def summary(self, parent_priority=PRIORITY_UNAMBIGUOUS):
        return self.name

    def get_first(self, prev=None):
        # Refer to our Sequence.
        return self.seq.get_first(prev)

    def get_disallowed_after(self, prev=None):
        # Refer to our Sequence.
        return self.seq.get_disallowed_after(prev)

    def check(self):
        return self.seq.check()


class Grammar:
    def __init__(self):
        self.exprs = {}

    def get_name(self, name):
        """
        Creates a name if it doesn't yet exist.  Mostly for the meta parser.
        """
        if name in self.exprs:
            return self.exprs[name]
        if name[0].upper() == name[0]:
            # starts uppercase, we guess it's a token
            e = self.token(name)
        else:
            # starts lowercase, we guess it's a expression
            e = self.expr(name)
        return e

    def expr(self, val):
        if isinstance(val, str):
            # Forward declaration case:
            #     @g.expr
            #     def my_expr(e):
            #         e.match(...)
            fn = None
            name = val
        else:
            # Declaration/definition case.
            #     my_expr = g.expr("my_expr")
            #
            #     ...
            #
            #     @my_expr
            #     def my_expr(e):
            #         e.match(...)
            fn = val
            name = fn.__name__

        assert name not in self.exprs
        e = Expression(name)
        self.exprs[name] = e
        if fn is not None:
            fn(e)
        return e

    def token(self, name, typ=None):
        assert name not in self.exprs
        e = Token(name, typ)
        self.exprs[name] = e
        return e

    def sorted_tokens(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Token):
                yield name, expr

    def sorted_expressions(self):
        for name in sorted(self.exprs.keys()):
            expr = self.exprs[name]
            if isinstance(expr, Expression):
                yield name, expr

    def check(self):
        for _, expr in self.sorted_expressions():
            expr.check()


class C:
    """
    An object with all the methods for generating C code.
    """
    code_tag = "c"

    class Variables:
        def __init__(self, fprint_fn, semloc):
            # a list of lists of available variables
            self.scopes = [[]]
            self.semloc = bool(semloc)
            self.fprint = fprint_fn

        def new_scope(self):
            self.scopes.append([])

        def define(self, tag, typ, pos):
            for scope in self.scopes:
                assert tag not in scope, "tag " + tag + " shadows another tag"
            suffix = "OUT" if tag == "$" else "_" + tag
            if typ is not None:
                self.fprint(
                    "    #define YYVAL%s "
                    "(p->semstack[call->stack + %d].val.%s)"%(
                        suffix, pos, typ
                    ),
                )
            if self.semloc:
                self.fprint(
                    "    #define YYLOC" + suffix
                    + " (p->semstack[call->stack + %d].loc)"%pos,
                )
            self.scopes[-1].append((tag, typ))

        def pop_scope(self):
            popped = self.scopes.pop()
            for tag, typ in popped:
                suffix = "OUT" if tag == "$" else "_" + tag
                if typ is not None:
                    self.fprint("    #undef YYVAL" + suffix)
                if self.semloc:
                    self.fprint("    #undef YYLOC" + suffix)

    def __init__(
        self,
        grammar,
        roots,
        file,
        *,
        precode=None,
        postcode=None,
        types=(),
        fallbacks=(),
        prefix="",
        user_args=(),
        semloc_type=None,
        span_fn=None,
    ):
        self.grammar = grammar
        self.file = file

        assert roots, "you must specify at least one %root"
        self.roots = roots

        self.precode = precode
        self.postcode = postcode

        all_detected_types = set(e.type for e in self.grammar.exprs.values() if e.type)
        all_defined_types = set(t[0] for t in types)
        missing_defs = all_detected_types.difference(all_defined_types)
        extra_defs = all_defined_types.difference(all_detected_types)
        if missing_defs or extra_defs:
            raise RuntimeError(
                "detected missing %%types [%s] and unused %%types [%s]"%(
                    ','.join(missing_defs), ','.join(extra_defs)
                )
            )
        self.types = types
        self.typemap = {t[0]: (t[1], t[2]) for t in types}

        self.fallbacks = fallbacks
        self.fallbackmap = {to: froms for to, froms in fallbacks}
        for token_name, _ in self.grammar.sorted_tokens():
            # no missing entries in fallbackmap
            self.fallbackmap.setdefault(token_name, [])

        self.prefix = prefix or ""

        self.u = (
            user_args.split(",") if isinstance(user_args, str) else user_args
        )

        self.prefix_ = (prefix + "_") if prefix else ""
        assert bool(semloc_type) + bool(span_fn) in (0, 2), (
            "semloc_type and span_fn must be both set or both unset"
        )
        self.semloc_type=semloc_type
        self.span_fn=span_fn

        # build a default mapping
        args_str = ", ".join(self.u)
        self.varmap = {
            "prefix": self.prefix,
            "prefix_": self.prefix_,
            "PREFIX_": self.prefix_.upper(),
            "user_args": args_str,
            "_user_args": "" if not args_str else ", " + args_str,
            "semloc_type": self.semloc_type,
            "span_fn": self.span_fn,
            "PARSE_FN_ARGS": self.prefix_.upper() + "PARSE_FN_ARGS",
        }

        self._extra_vars = {}

        self.tokens_have_value = any(
            t.type is not None for _, t in self.grammar.sorted_tokens()
        )


    def sub(self, s):
        return string.Template(s).substitute(**self.varmap, **self._extra_vars)

    def fprint(self, s):
        print(self.sub(s), file=self.file)

    def fprint_multi(self, s):
        print(textwrap.dedent(self.sub(s)).strip("\n"), file=self.file)

    @contextlib.contextmanager
    def extra_vars(self, **mapping):
        old_vars = dict(**self._extra_vars)
        self._extra_vars.update(mapping)
        try:
            yield
        finally:
            self._extra_vars = old_vars

    def print_code(self, code, i):
        if code is None:
            return
        for c in code:
            if c.tag in (None, "c"):
                self.fprint(i + "// USER CODE")
                break
        for c in code:
            if c.tag in (None, "c"):
                # self.fprint(textwrap.indent(c.text, i))
                text = re.sub("\\$\\$", "YYVALOUT", c.text)
                text = re.sub("\\$([a-zA-Z_][a-zA-Z0-9_]*)", "YYVAL_\\1", text)
                # actually, we don't support reading the location of the output
                # because it's not clear to user code when it's ready
                ## text = re.sub("@@", "YYLOCOUT", text)
                text = re.sub("@([a-zA-Z_][a-zA-Z0-9_]*)", "YYLOC_\\1", text)
                self.fprint(textwrap.indent(text, i))

    def print_destructor(self, code, lhv, i):
        if code is None:
            return
        for c in code:
            if c.tag in (None, "c"):
                self.fprint(i + "// USER CODE")
                break
        for c in code:
            if c.tag in (None, "c"):
                # self.fprint(textwrap.indent(c.text, i))
                text = re.sub("\\$\\$", lhv, c.text)
                self.fprint(textwrap.indent(text, i))

    def _token_list_args(self, t, f):
        out = ", ".join(t)
        if f:
            out += ", /*fallback*/ " + ", ".join(f)
        return len(t) + len(f), out

    def token_check(self, tokens):
        # primary tokens
        t = sorted(t for t in tokens if t is not None)
        # fallback tokens
        f = sorted(
            i for tt in t for i in self.fallbackmap[tt] if i not in t
        )
        nargs, args = self._token_list_args(t, f)
        return "${prefix_}token_in(*token, %d, %s)"%(nargs, args)

    def mask_set(self, tokens):
        # primary tokens
        t = sorted(t for t in tokens if t is not None)
        # fallback tokens
        f = sorted(
            i for tt in t for i in self.fallbackmap[tt] if i not in t
        )
        nargs, args = self._token_list_args(t, f)
        return "${prefix_}mask_set(p->mask, %d, %s)"%(nargs, args)

    def stackpersist(self, obj):
        """Return the persistent created during a match to obj."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Expression):
            return 1
        return 0

    def stackmax(self, obj):
        """Return the maximum stack size during a match to obj."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Expression):
            return 1
        if isinstance(obj, Maybe):
            return self.stackmax(obj.seq)
        if isinstance(obj, ZeroOrMore):
            return self.stackmax(obj.seq)
        if isinstance(obj, Branches):
            return max(self.stackmax(b) for b in obj.branches)
        if isinstance(obj, Recovery):
            return self.stackmax(obj.seq)
        if isinstance(obj, Sequence):
            persist = 0
            peak = 0
            for t, _, _ in obj.terms:
                peak = max(peak, persist + self.stackmax(t))
                persist += self.stackpersist(t)
            return peak
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def nstates(self, obj):
        """Count how many states we need for every entry."""
        if isinstance(obj, Token):
            return 1
        if isinstance(obj, Maybe):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, ZeroOrMore):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, Branches):
            return sum(self.nstates(b) for b in obj.branches) + 1
        if isinstance(obj, Recovery):
            return self.nstates(obj.seq) + 1
        if isinstance(obj, Sequence):
            return sum(self.nstates(t) for t, _, _ in obj.terms)
        if isinstance(obj, Expression):
            return 1
        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def gen(self, obj, state, stack, var, tag=None):
        """Generate code to match an object."""
        if isinstance(obj, Token):
            self.fprint("    // " + obj.summary())
            self.fprint("    call->state = " + str(state) + ";")
            self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
            self.fprint("yy" + str(state) + ":")
            self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
            self.fprint("        " + self.mask_set(obj.get_first()) + ";")
            self.fprint("        SYNTAX_ERROR;")
            self.fprint("    }")
            self.fprint("    p->semstack[call->stack + " + str(stack) + "] = sem;")
            self.fprint("    ${PREFIX_}CONSUME_TOKEN;")
            self.fprint("    ${prefix_}mask_clear(p->mask);")
            if self.semloc_type:
                if self.want_first_loc:
                    self.want_first_loc = False
                    self.fprint("    call->loc_start = sem.loc;")
                self.fprint("    call->loc_end = sem.loc;")

            return state + self.nstates(obj), stack + 1

        if isinstance(obj, Maybe):
            self.fprint("    // " + obj.summary())
            self.fprint("    call->state = " + str(state) + ";")
            self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
            self.fprint("yy" + str(state) + ":")
            self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
            self.fprint("        " + self.mask_set(obj.get_first()) + ";")
            self.fprint("        goto yy" + str(state) + "_done;")
            self.fprint("    }")
            self.gen(obj.seq, state+1, stack, var)
            self.fprint("yy" + str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, ZeroOrMore):
            self.fprint("    // " + obj.summary())
            self.fprint("yy" + str(state) + ":")
            self.fprint("    call->state = " + str(state) + ";")
            self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
            self.fprint("    if(!" + self.token_check(obj.get_first()) + "){")
            self.fprint("        " + self.mask_set(obj.get_first()) + ";")
            self.fprint("        goto yy" + str(state) + "_done;")
            self.fprint("    }")
            self.fprint("")
            self.gen(obj.seq, state + 1, stack, var)
            self.fprint("    // try for another " + obj.summary())
            self.fprint("    goto yy" + str(state) + ";")
            self.fprint("yy" + str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, Branches):
            self.fprint("    // " + obj.summary())
            self.fprint("    call->state = " + str(state) + ";")
            self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
            self.fprint("yy" + str(state) + ":")
            self.fprint("    switch(*token){")
            bstate = state + 1
            # detect primary types
            primary_types = set(
                t for b in obj.branches for t in b.get_first()
            )
            # detect fallback types
            fallback_types = {}
            for b in obj.branches:
                for t in b.get_first():
                    fallback_types[t] = [
                        f for f in self.fallbackmap[t]
                        if f not in primary_types
                    ]
            # emit case statements
            for i, b in enumerate(obj.branches):
                for t in b.get_first():
                    primary_types.add(t)
                    label = "yy%d_br_%d"%(state, i)
                    for f in fallback_types.get(t, []):
                        self.fprint("        case %s: // fallback"%f)
                    self.fprint("        case %s: goto %s;"%(t, label))
                bstate += self.nstates(b)
            self.fprint("        default:")
            self.fprint("            " + self.mask_set(obj.get_first()) + ";")
            self.fprint("            SYNTAX_ERROR;")
            self.fprint("    }")
            self.fprint("")
            # branch parsing
            bstate = state + 1
            for i, b in enumerate(obj.branches):
                self.fprint("yy" + str(state) + "_br_" + str(i) + ":")
                bstate, stack = self.gen(b, bstate, stack, var)
                self.fprint("    goto yy" + str(state) + "_done;")
                self.fprint("")
            self.fprint("yy"+ str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, Recovery):
            assert obj.after is not None, ".set_after() was never called"
            self.fprint("    // " + obj.summary())
            self.fprint("    call->recover = " + str(state) + ";")
            self.fprint("")
            self.gen(obj.seq, state + 1, stack, var)
            self.fprint("")
            self.fprint("    call->recover = 0;")
            self.fprint("    goto yy" + str(state) + "_done;")
            self.fprint("yy"+ str(state) + ":")
            self.fprint("    // recovery: consume all tokens until a valid one")
            self.fprint("    ${PREFIX_}AWAIT_TOKEN;")
            self.fprint("    if(!" + self.token_check(obj.after) + "){")
            self.fprint("        ${PREFIX_}CONSUME_TOKEN;")
            self.fprint("        return 0;")
            self.fprint("    }")
            self.print_code(obj.code, "    ")
            self.fprint("yy"+ str(state) + "_done:")
            return state + self.nstates(obj), stack

        if isinstance(obj, Sequence):
            oldstack = stack
            var.new_scope()
            self.print_code(obj.precode, "    ")
            for i, (t, tag, code) in enumerate(obj.terms):
                if tag is not None:
                    assert isinstance(t, (Token, Expression)), "wrong type for tag: " + type(t).__name__
                    var.define(tag, t.type, stack)
                state, stack = self.gen(t, state, stack, var, tag)
                self.print_code(code, "    ")
                if i+1 != len(obj.terms):
                    self.fprint("")
            var.pop_scope()
            self.fprint("    ${prefix_}free_sems(p, call->stack+%d);"%oldstack)
            return state, stack

        if isinstance(obj, Expression):
            # code generated when calling this function
            self.fprint("    // " + obj.summary())
            self.fprint("    call->state = " + str(state) + ";")
            self.fprint("    ${PREFIX_}CALL(%s, %d);"%(self.parse_fn(obj), stack))
            self.fprint("yy" + str(state) + ":")
            if self.semloc_type:
                if self.want_first_loc:
                    self.want_first_loc = False
                    self.fprint("    call->loc_start = p->semstack[%d].loc;"%stack)
                self.fprint("    call->loc_end = p->semstack[%d].loc;"%stack)
            return state + self.nstates(obj), stack + 1

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def parse_fn(self, expr):
        """Get the generated function name for an Expression."""
        return "_parse_" + expr.name

    def declare_fn(self, expr):
        self.fprint("static int " + self.parse_fn(expr) + "($PARSE_FN_ARGS);")

    def define_fn(self, expr):
        """Generate the function definition for an Expression."""
        var = []
        self.fprint("// %s = %s;"%(expr.name, expr.seq.summary()))
        self.fprint("static int " + self.parse_fn(expr) + "($PARSE_FN_ARGS){")
        # Jump to state.
        self.fprint("    switch(call->state){")
        self.fprint("        case " + str(0) + ": break;")
        for n in range(self.nstates(expr.seq)):
            self.fprint("        case " + str(n+1) + ": goto yy" + str(n+1) + ";")
        self.fprint("    }")
        # Ensure we have enough stack space to operate.
        x = 1 + self.stackmax(expr.seq)
        self.fprint("    if(!${prefix_}sems_available(p, call->stack + %d)){"%x)
        self.fprint("        return ${PREFIX_}STATUS_SEMSTACK_OVERFLOW;")
        self.fprint("    }")
        self.fprint("    call->nsems = %d;"%x)
        var = C.Variables(self.fprint, semloc=self.semloc_type)
        if expr.type is not None:
            var.define("$", expr.type, 0)
            self.fprint("    p->semstack[call->stack].type = ${PREFIX_}SEMTYPE_%s;"%expr.type)
        if self.semloc_type:
            self.want_first_loc = True
        self.fprint("")

        # state starts at 1, since 0 is only the first time we're called
        # stack starts at 1, since we always allocate the output
        self.gen(expr.seq, 1, 1, var)

        self.fprint("")
        if self.semloc_type:
            # calculate the overall location
            self.fprint("    p->semstack[call->stack].loc = $span_fn(")
            self.fprint("        call->loc_start, call->loc_end")
            self.fprint("    );")
        var.pop_scope()
        self.fprint("    p->callslen--;");
        self.fprint("    return 0;")
        self.fprint("}")
        self.fprint("")

    def define_user_api(self, root):
        type_name = self.grammar.exprs[root].type
        type_spec = None
        if type_name:
            type_spec = self.typemap[type_name][0]

        with self.extra_vars(
            root=root, type_name=type_name, type_spec=type_spec
        ):
            self.fprint("${prefix_}status_t ${prefix_}parse_$root(")
            params = []
            params.append("    ${prefix_}parser_t *p")
            params.append("    ${prefix_}token_e token")
            if self.tokens_have_value:
                params.append("    ${prefix_}val_u val")
            if self.semloc_type:
                params.append("    $semloc_type loc")
            if type_spec:
                params.append("    $type_spec *val_out")
            if self.semloc_type:
                params.append("    $semloc_type *loc_out")
            for p in params[:-1]:
                self.fprint(p + ",")
            self.fprint(params[-1])
            self.fprint("){")
            self.fprint("    ${prefix_}sem_t sem = {")
            self.fprint("        .type = _${prefix_}token_types[token],")
            if self.tokens_have_value:
                self.fprint("        .val = val,")
            if self.semloc_type:
                self.fprint("        .loc = loc,")
            self.fprint("    };")
            self.fprint("    ${prefix_}sem_t sem_out = {0};")
            self.fprint("    ${prefix_}status_t status = ${prefix_}do_parse(")
            self.fprint("        p, _parse_$root, token, sem, &sem_out")
            self.fprint("    );")
            if type_spec or self.semloc_type:
                self.fprint("    if(status != ${PREFIX_}STATUS_DONE) return status;")
            if self.semloc_type:
                self.fprint("    if(loc_out){")
                self.fprint("        *loc_out = sem_out.loc;")
                self.fprint("    }")
            if type_spec:
                self.fprint("    if(val_out){")
                self.fprint("        *val_out = sem_out.val.$type_name;")
                self.fprint("    }else{")
                self.fprint("        ${prefix_}sem_free(&sem_out);")
                self.fprint("    }")
            self.fprint("    return status;")
            self.fprint("}")

    def gen_file(self):
        self.print_code(self.precode, "")
        self.fprint_multi("""
            #include <stdio.h>
            #include <stdbool.h>
            #include <stdlib.h>
            #include <stdarg.h>
            #include <stdint.h>
            #include <string.h>
        """)

        self.fprint("typedef enum {")
        self.fprint("    _${PREFIX_}NOT_A_TOKEN = 0,")
        for _, token in self.grammar.sorted_tokens():
            self.fprint("    %s,"%token.name)
        self.fprint("} ${prefix_}token_e;")

        self.fprint("")
        self.fprint("typedef enum {")
        self.fprint("    ${PREFIX_}NO_SEMTYPE = 0,")
        for name, _, __ in self.types:
            self.fprint("    ${PREFIX_}SEMTYPE_%s,"%name)
        self.fprint("} ${prefix_}sem_type_e;")

        if self.tokens_have_value:
            self.fprint("")
            self.fprint("static ${prefix_}sem_type_e _${prefix_}token_types[] = {")
            self.fprint("   /* _${PREFIX_}NOT_A_TOKEN */  ${PREFIX_}NO_SEMTYPE,")
            for _, token in self.grammar.sorted_tokens():
                if token.type:
                    typ = "${PREFIX_}SEMTYPE_%s"%token.type
                else:
                    typ = "${PREFIX_}NO_SEMTYPE"
                self.fprint("   /* %s */  %s,"%(token.name, typ))
            self.fprint("};")

        self.fprint("")
        self.fprint_multi(r"""
            typedef enum {
                ${PREFIX_}STATUS_OK = 0,
                ${PREFIX_}STATUS_DONE,
                ${PREFIX_}STATUS_SYNTAX_ERROR,
                ${PREFIX_}STATUS_SEMSTACK_OVERFLOW,
                ${PREFIX_}STATUS_CALLSTACK_OVERFLOW,
            } ${prefix_}status_t;

            struct ${prefix_}parser_t;
            typedef struct ${prefix_}parser_t ${prefix_}parser_t;
            struct ${prefix_}call_t;
            typedef struct ${prefix_}call_t ${prefix_}call_t;
            struct ${prefix_}sem_t;
            typedef struct ${prefix_}sem_t ${prefix_}sem_t;

            #define $PARSE_FN_ARGS \
                ${prefix_}call_t *call, \
                ${prefix_}parser_t *p, \
                ${prefix_}token_e *token, \
                ${prefix_}sem_t sem

            typedef int (*${prefix_}parse_f)($PARSE_FN_ARGS);

            struct ${prefix_}call_t {
                // a function to call
                ${prefix_}parse_f fn;

                // our starting position in the semstack
                int stack;

                // a count of how many sems we'll need
                int nsems;

                // our position in the call
                int state;

                // the state to enter if we have to recover
                int recover;
        """)
        if self.semloc_type:
            self.fprint("")
            self.fprint("    // the first location that matches this call")
            self.fprint("    $semloc_type loc_start;")
            self.fprint("")
            self.fprint("    // the last location that matches this call")
            self.fprint("    $semloc_type loc_end;")
        self.fprint("};")

        self.fprint("")
        self.fprint("typedef union {")
        for name, spec, _ in self.types:
            self.fprint("    %s %s;"%(spec, name))
        self.fprint("} ${prefix_}val_u;")

        self.fprint("")
        self.fprint("struct ${prefix_}sem_t {")
        self.fprint("    ${prefix_}sem_type_e type;")
        self.fprint("    // semantic value")
        self.fprint("    ${prefix_}val_u val;")

        if self.semloc_type:
            self.fprint("    // semantic location")
            self.fprint("    " + self.semloc_type + " loc;")
        else:
            self.fprint("    // no semantic location; semloc_type not configured")
        self.fprint("};")

        self.fprint("")
        self.fprint("void ${prefix_}sem_free(${prefix_}sem_t *sem){")
        self.fprint("    switch(sem->type){")
        self.fprint("        case ${PREFIX_}NO_SEMTYPE: break;")
        for name, spec, destructor in self.types:
            self.fprint("        case ${PREFIX_}SEMTYPE_%s: {"%name)
            self.print_destructor(
                destructor, lhv="(sem->val.%s)"%name, i="           "
            )
            self.fprint("        } break;")
        self.fprint("    }")
        self.fprint("    *sem = (${prefix_}sem_t){0};")
        self.fprint("}")

        masklen = (len([t for t in self.grammar.sorted_tokens()]) + 7) // 8
        self.fprint("// we need ${PREFIX_}MASKLEN chars to have one bit per token")
        self.fprint("#define ${PREFIX_}MASKLEN %d"%masklen)

        self.fprint("")
        self.fprint_multi(r"""
            static void ${prefix_}mask_clear(unsigned char *mask){
                memset(mask, 0, ${PREFIX_}MASKLEN);
            }

            // accepts `n` ints, representing token values
            static void ${prefix_}mask_set(unsigned char *mask, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                for(size_t i = 0; i < n; i++){
                    int t = va_arg(ap, int);
                    mask[t/8] |= ((unsigned char)1) << (t%8);
                }

                va_end(ap);
            }

            static const char *${prefix_}token_name(int t);

            static void ${prefix_}mask_print(unsigned char *mask, char *joiner){
                bool first = true;
                for(int i = 0; i < ${PREFIX_}MASKLEN; i++){
                    for(int j = 0; j < 8; j++){
                        if(!(mask[i] & ((unsigned char)1)<<j)) continue;
                        printf(
                            "%s%s",
                            first ? (first=false,"") : joiner,
                            ${prefix_}token_name(i*8+j)
                        );
                    }
                }
            }

            struct ${prefix_}parser_t {
                // stack of calls
                ${prefix_}call_t *callstack;
                size_t callslen;
                size_t callsmax;
                // stack of semvals
                ${prefix_}sem_t *semstack;
                size_t semsmax;
                // a mask of all possible tokens
                unsigned char mask[${PREFIX_}MASKLEN];
            };

            ${prefix_}call_t *${prefix_}calls_append(${prefix_}parser_t *p){
                if(p->callslen == p->callsmax) return NULL;
                return &p->callstack[p->callslen++];
            }

            bool ${prefix_}sems_available(${prefix_}parser_t *p, size_t n){
                return n <= p->semsmax;
            }

            void ${prefix_}free_sems(${prefix_}parser_t *p, size_t first){
                // free all possible sems, according to the last call
                if(p->callslen == 0) return;
                ${prefix_}call_t clast = p->callstack[p->callslen-1];
                size_t end = clast.stack + clast.nsems;
                for(size_t i = first; i < end; i++){
                    ${prefix_}sem_free(&p->semstack[i]);
                }
            }

            #define ${PREFIX_}ONSTACK_PARSER(name, _callsmax, _semsmax) \
                ${prefix_}call_t name##calls[_callsmax] = {0}; \
                ${prefix_}sem_t name##sems[_semsmax] = {0}; \
                ${prefix_}parser_t name = (${prefix_}parser_t){ \
                    .callstack = name##calls, \
                    .callsmax = _callsmax, \
                    .semstack = name##sems, \
                    .semsmax = _semsmax, \
                }

            void ${prefix_}parser_reset(${prefix_}parser_t *p){
                ${prefix_}free_sems(p, 0);
                p->callslen = 0;
                ${prefix_}mask_clear(p->mask);
            }

            void ${prefix_}parser_free(${prefix_}parser_t **pp){
                ${prefix_}parser_t *p = *pp;
                if(!p) return;
                ${prefix_}parser_reset(p);
                free(p->callstack);
                free(p->semstack);
                free(p);
                *pp = NULL;
            }

            ${prefix_}parser_t *${prefix_}parser_new(size_t callsmax, size_t semsmax){
                ${prefix_}parser_t *p = malloc(sizeof(*p));
                *p = (${prefix_}parser_t){
                    .callstack = malloc(sizeof(p->callstack[0]) * callsmax),
                    .callsmax = callsmax,
                    .semstack = malloc(sizeof(p->semstack[0]) * callsmax),
                    .semsmax = semsmax,
                };

                if(!p->callstack || !p->semstack){
                    ${prefix_}parser_free(&p);
                }else{
                    memset(p->callstack, 0, sizeof(p->callstack[0]) * callsmax);
                    memset(p->semstack, 0, sizeof(p->semstack[0]) * callsmax);
                }

                return p;
            }

            static const char *_${prefix_}fn_name(${prefix_}parse_f fn);

            static void print_stack(${prefix_}parser_t *p){
                for(size_t i = 0; i < p->callslen; i++){
                    ${prefix_}call_t call = p->callstack[i];
                    printf("parsing %s (%d)\n", _${prefix_}fn_name(call.fn), call.state);
                }

            }

            #define ${PREFIX_}CALL(_fn, _stack) do { \
                ${prefix_}call_t *subcall = ${prefix_}calls_append(p); \
                if(!subcall) return ${PREFIX_}STATUS_CALLSTACK_OVERFLOW; \
                *subcall = (${prefix_}call_t){ .fn = _fn,  .stack = call->stack + _stack }; \
                return 0; \
            } while(0)

            #define ${PREFIX_}AWAIT_TOKEN \
                if(!*token) return 0

            #define ${PREFIX_}CONSUME_TOKEN \
                *token = _${PREFIX_}NOT_A_TOKEN

            bool ${prefix_}token_in(${prefix_}token_e token, size_t n, ...){
                va_list ap;
                va_start(ap, n);

                bool out = false;
                for(size_t i = 0; i < n; i++){
                    if(token != va_arg(ap, int)) continue;
                    out = true;
                    break;
                }

                va_end(ap);
                return out;
            }

            #define SYNTAX_ERROR do { \
                printf("syntax error!\n"); \
                print_stack(p); \
                printf("expected one of: {"); \
                ${prefix_}mask_print(p->mask, ","); \
                printf("} but got %s\n", ${prefix_}token_name(*token)); \
                return 1; \
            } while(0)

            ${prefix_}status_t ${prefix_}do_parse(
                ${prefix_}parser_t *p,
                ${prefix_}parse_f entrypoint,
                ${prefix_}token_e token,
                ${prefix_}sem_t sem,
                ${prefix_}sem_t *sem_out
            ){
                if(p->callslen == 0){
                    // first token, initialize output at semstack[0]
                    if(!${prefix_}sems_available(p, 1)){
                        return ${PREFIX_}STATUS_SEMSTACK_OVERFLOW;
                    }
                    // initialize the first call
                    ${prefix_}call_t *call = ${prefix_}calls_append(p);
                    if(!call) return ${PREFIX_}STATUS_CALLSTACK_OVERFLOW;
                    *call = (${prefix_}call_t){ .fn = entrypoint, .stack = 0 };
                }

                while(p->callslen){
                    size_t last = p->callslen;
                    ${prefix_}call_t *call = &p->callstack[p->callslen-1];
                    ${prefix_}status_t status = call->fn(call, p, &token, sem);
                    if(status > ${PREFIX_}STATUS_SYNTAX_ERROR){
                        // CALLSTACK_OVERFLOW or SEMSTACK_OVERFLOW
                        return status;
                    }
                    if(!status){
                        if(p->callslen == last){
                            // pause until the next token
                            return ${PREFIX_}STATUS_OK;
                        }
                        continue;
                    }
                    // syntax error; pop from the stack until we can recover
                    bool recovered = false;
                    while(p->callslen){
                        ${prefix_}call_t *call = &p->callstack[p->callslen-1];
                        if(call->recover){
                            // this call has a recovery state we can enter
                            call->state = call->recover;
                            call->recover = 0;
                            recovered = true;
                            break;
                        }else{
                            ${prefix_}free_sems(p, call->stack);
                            p->callslen--;
                        }
                    }
                    if(!recovered){
                        return ${PREFIX_}STATUS_SYNTAX_ERROR;
                    }
                }

                // parsed everything!
                *sem_out = p->semstack[0];
                p->semstack[0] = (${prefix_}sem_t){};
                ${prefix_}parser_reset(p);
                return ${PREFIX_}STATUS_DONE;
            }
        """)

        # declare functions
        for _, expr in self.grammar.sorted_expressions():
            self.declare_fn(expr)
        self.fprint("")

        self.fprint("static const char *${prefix_}token_name(int t){")
        self.fprint("    switch(t){")
        for name, _ in self.grammar.sorted_tokens():
            self.fprint("        case " + name + ": return \"" +name+ "\";")
        self.fprint("        default: return \"unknown\";")
        self.fprint("    }")
        self.fprint("}")
        self.fprint("")

        # get function names
        self.fprint(self.sub("static const char *_${prefix_}fn_name(${prefix_}parse_f fn){"))
        for _, expr in self.grammar.sorted_expressions():
            self.fprint(
                "    if(fn == " + self.parse_fn(expr) + ")"
                + " return \""+self.parse_fn(expr)+"\";"
            )
        self.fprint("    return \"unknown\";")
        self.fprint("}")
        self.fprint("")

        # define functions
        for _, expr in self.grammar.sorted_expressions():
            self.define_fn(expr)

        # define user apis
        for root in self.roots:
            self.define_user_api(root)
            self.fprint("")

        self.print_code(self.postcode, "")


class Python:
    """
    An object with all the methods for generating Python code.
    """
    code_tag = "py"

    def __init__(
        self,
        grammar,
        roots,
        file,
        *,
        precode=None,
        postcode=None,
        types=(),
        fallbacks=(),
        prefix="",
        user_args=(),
        span_fn=None,
    ):
        self.grammar = grammar
        self.file = file
        self.roots = roots
        self.precode = precode
        self.postcode = postcode
        if types:
            print(
                "warning: %type directives are ignored by the python generator",
                file=sys.stderr
            )
        if fallbacks:
            print(
                "warning: %fallback directives are ignored by the python generator",
                file=sys.stderr
            )
        self.prefix = prefix
        self.prefix_ = (prefix + "_") if prefix else ""
        self.span_fn = span_fn
        self.u = user_args.split(",") if isinstance(user_args, str) else user_args

        # build a default mapping
        args_str = ", ".join(self.u)
        self.varmap = {
            "prefix": self.prefix,
            "prefix_": self.prefix_,
            "u": args_str,
            "_u": "" if not args_str else ", " + args_str,
            "u_": "" if not args_str else args_str + ", ",
        }
        if span_fn is not None:
            self.varmap["span_fn"] = span_fn

        self._extra_vars = {}

    def sub(self, s):
        return string.Template(s).substitute(**self.varmap, **self._extra_vars)

    def fprint(self, s):
        print(self.sub(s), file=self.file)

    def fprint_multi(self, s, i=""):
        print(
            textwrap.indent(textwrap.dedent(self.sub(s)).strip("\n"), i),
            file=self.file,
        )

    @contextlib.contextmanager
    def extra_vars(self, **mapping):
        old_vars = dict(**self._extra_vars)
        self._extra_vars.update(mapping)
        try:
            yield
        finally:
            self._extra_vars = old_vars

    def print_code(self, code, i):
        if code is None:
            return
        for c in code:
            if c.tag in (None, self.code_tag):
                self.fprint(i + "# USER CODE")
                break
        for c in code:
            if c.tag in (None, self.code_tag):
                text = re.sub("\\$\\$", "(out.val)", c.text)
                text = re.sub("\\$([a-zA-Z_][a-zA-Z0-9_]*)", "(yy_\\1.val)", text)
                # actually, we don't support reading the location of the output
                # because it's not clear to user code when it's ready
                ## text = re.sub("@@", "(out.loc)", text)
                text = re.sub("@([a-zA-Z_][a-zA-Z0-9_]*)", "(yy_\\1.loc)", text)
                self.fprint_multi(text, i)

    def syntax_error(self, loc, i):
        self.fprint(i + "raise ${prefix}SyntaxError(loc="+ loc + ")")

    def token_list(self, tokens):
        assert tokens
        tokens = sorted(t for t in tokens if t is not None)
        if len(tokens) > 1:
            return "(" + ", ".join(tokens) + ")"
        else:
            return "(" + tokens[0] + ",)"

    def if_token_in(self, tokens, i, pos=0):
        token_list = self.token_list(tokens)
        lead = "if" if pos == 0 else "elif"
        with self.extra_vars(token_list=token_list, lead=lead):
            self.fprint(i + "$lead (yield from tokens.peek()) in $token_list:")

    def if_token_not_in(self, tokens, i, pos=0):
        token_list = self.token_list(tokens)
        lead = "if" if pos == 0 else "elif"
        with self.extra_vars(token_list=token_list, lead=lead):
            self.fprint(i + "$lead (yield from tokens.peek()) not in $token_list:")

    def while_token_in(self, tokens, i):
        token_list = self.token_list(tokens)
        with self.extra_vars(token_list=token_list):
            self.fprint(i + "while (yield from tokens.peek()) in $token_list:")

    def while_token_not_in(self, tokens, i):
        token_list = self.token_list(tokens)
        with self.extra_vars(token_list=token_list):
            self.fprint(i + "while (yield from tokens.peek()) not in $token_list:")

    def gen(self, obj, tag, i):
        try:
            name = obj.name if isinstance(obj, (Expression, Token)) else None
            with self.extra_vars(
                name=name,
                tag_eq="" if not tag else "yy_%s = "%tag,
            ):
                self._gen(obj, tag, i)
        except Exception as e:
            raise RuntimeError("generating for " + obj.summary()) from e

    def _gen(self, obj, tag, i):
        if isinstance(obj, Token):
            self.fprint(i + "if (yield from tokens.peek()) != $name:")
            self.syntax_error("(yield from next(tokens)).loc", i + "    ")
            self.fprint(i + "$tag_eq(yield from next(tokens))")
            return

        if isinstance(obj, Maybe):
            self.if_token_in(obj.get_first(), i)
            self.gen(obj.seq, None, i+"    ")
            return

        if isinstance(obj, ZeroOrMore):
            self.while_token_in(obj.get_first(), i)
            self.gen(obj.seq, None, i+"    ")
            return

        if isinstance(obj, Branches):
            for n, seq in enumerate(obj.branches):
                self.if_token_in(seq.get_first(), i, pos=n)
                self.gen(seq, None, i+"    ")
            self.fprint(i + "else:")
            self.syntax_error("(yield from next(tokens)).loc", i + "    ")
            return

        if isinstance(obj, Recovery):
            self.fprint(i + "try:")
            self.gen(obj.seq, None, i+"    ")
            self.if_token_not_in(obj.after, i+"    ")
            self.fprint(i + "        raise ${prefix}SyntaxError()")
            self.fprint(i + "except ${prefix}SyntaxError:")
            self.while_token_not_in(obj.after, i+"    ")
            self.fprint(i + "        yield from next(tokens)")
            self.print_code(obj.code, i+"    ")
            return

        if isinstance(obj, Sequence):
            self.print_code(obj.precode, i)
            for term, tag, code in obj.terms:
                self.gen(term, tag, i)
                self.print_code(code, i)
            return

        if isinstance(obj, Expression):
            self.fprint(
                i + "${tag_eq}yield from _${prefix_}parse_$name(tokens${_u})"
            )
            return

        raise RuntimeError("unrecognized object type: " + type(obj).__name__)

    def define_fn(self, expr):
        with self.extra_vars(name=expr.name, summary=expr.seq.summary()):
            self.fprint("# $name = $summary")
            self.fprint("def _${prefix_}parse_$name(tokens${_u}):")
            self.fprint("    out = ${prefix}Expression()")
            self.gen(expr.seq, None, "    ")
            self.fprint("    return out")

    def define_user_api(self, root):
        with self.extra_vars(root=root):
            self.fprint_multi("""
            class ${prefix}${root}Parser:
                def __init__(self${_u}, repeat=False):
                    self.tokens = ${prefix}TokenStream()
                    self.out = None
                    self.args = ($u_)
                    self.gen = self.mkgen()
                    self.repeat = repeat
                    self.done = False

                def mkgen(self):
                    def _gen():
                        self.out = yield from _${prefix_}parse_$root(self.tokens, *self.args)
                    gen = _gen()
                    next(gen)
                    return gen

                def feed(self, t):
                    if self.done:
                        raise RuntimeError("parser is already done!")
                    try:
                        return self.gen.send(t)
                    except StopIteration:
                        self.gen = None
                        out = self.out
                        self.out = None
                        self.gen = self.mkgen()
                        if not self.repeat:
                            self.done = True
                        return out
            """)

    def gen_file(self):
        self.print_code(self.precode, "")
        if self.precode:
            self.fprint("")

        self.fprint_multi(r"""
            class ${prefix}SyntaxError(Exception):
                def __init__(self, msg=None, loc=None):
                    self.loc = loc

                def __repr__(self):
                    return str(msg) + ": " + str(self.loc)

            class ${prefix}Expression:
                def __init__(self, val=None, loc=None):
                    self.val = val
                    self.loc = loc

                def __repr__(self):
                    return "${prefix}Expression(val=%s)"%(str(self.val))

            class ${prefix}Token:
                def __init__(self, typ, val=None, loc=None):
                    self.type = typ
                    self.val = val
                    self.loc = loc

                def __repr__(self):
                    return "''' + self.prefix + '''Token(%s, %s)"%(str(self.type), str(self.val))

            class ${prefix}TokenStream:
                def __init__(self):
                    self.first = None

                def peek(self):
                    if self.first is None:
                        self.first = yield
                    return self.first.type

                def __next__(self):
                    if self.first:
                        out = self.first
                        self.first = None
                        return out
                    t = yield
                    return t
        """)
        self.fprint("")

        if self.span_fn is not None:
            self.fprint_multi("""
                def _${prefix_}span(loc, token_loc):
                    if loc is None:
                        return token_loc
                    return $span_fn(loc, token_loc)
            """)
            self.fprint("")

        for i, (name, _) in enumerate(self.grammar.sorted_tokens()):
            self.fprint(name + " = " + str(i))
        self.fprint("")

        for name, expr in self.grammar.sorted_expressions():
            self.define_fn(expr)
            self.fprint("")

        for root in self.roots:
            self.define_user_api(root)
            self.fprint("")

        self.print_code(self.postcode, "")


### Meta Parser ###

class ParsedName:
    def __init__(self, name, tag):
        self.name = name
        self.tag = tag

    def process(self, g, e):
        e.match(g.get_name(self.name), self.tag)

class ParsedSnippet:
    def __init__(self, text, tag=None):
        self.text = text
        self.tag = tag

    def process(self, g, e):
        e.exec(self.text, self.tag)

class ParsedBranches:
    def __init__(self):
        self.branches = []

    def process(self, g, e):
        # Collapse single-branch expressions.
        if len(self.branches) == 1:
            self.branches[0].process(g, e)
            return

        with e.branches() as b:
            for parsed_branch in self.branches:
                with b.branch():
                    parsed_branch.process(g, e)

class ParsedRecovery:
    def __init__(self, sub):
        self.sub = sub
        self.code = []

    def process(self, g, e):
        with e.recovery() as r:
            self.sub.process(g, e)
            for c in self.code:
                r.add_recovery_code(c.text, c.tag)

class ParsedMultiplier:
    def __init__(self, term, multiplier):
        self.term = term
        self.multiplier = multiplier

    def process(self, g, e):
        if self.multiplier == "1":
            self.term.process(g, e)
        elif self.multiplier == "?":
            with e.maybe():
                self.term.process(g, e)
        elif self.multiplier == "*":
            with e.zero_or_more():
                self.term.process(g, e)
        else:
            raise RuntimeError("unrecognized multiplier: " + str(self.multiplier))

class ParsedSequence:
    def __init__(self):
        self.elems = []

    def process(self, g, e):
        for elem in self.elems:
            elem.process(g, e)

class ParsedGenerator:
    def __init__(self, name):
        self.name = name

class ParsedKwarg:
    def __init__(self):
        self.key = None
        self.value = None
        self.tag = None

class ParsedRoot:
    def __init__(self):
        self.name = None
        self.tag = None

class ParsedFallback:
    def __init__(self):
        self.to_type = None
        self.tag = None
        self.from_types = []

class ParsedType:
    def __init__(self):
        self.name = None
        self.code = None
        self.destructor = None
        self.tag = None

class ParsedDoc:
    def __init__(self):
        self.precode = []
        self.defs = []
        self.postcode = []
        # directive handling
        self.generator = None
        self.kwargs = []
        self.roots = []
        self.types = []
        self.fallbacks = []
        # map of { from_type: to_type }
        self.rev_fallbacks = {}

    def add_stmt(self, stmt):
        if isinstance(stmt, ParsedGenerator):
            if self.generator is not None:
                raise RuntimeError("received multiple %generator directives")
            self.generator = stmt.name
            return
        if isinstance(stmt, ParsedKwarg):
            self.kwargs.append(stmt)
            return
        if isinstance(stmt, ParsedType):
            self.types.append(stmt)
            return
        if isinstance(stmt, ParsedRoot):
            self.roots.append(stmt)
            return
        if isinstance(stmt, ParsedFallback):
            self.fallbacks.append(stmt)
            return
        if isinstance(stmt, tuple):
            self.defs.append(stmt)
            return
        raise RuntimeError("unknown statement of type %s"%type(stmt).__name__)

    def build_grammar(self):
        g = Grammar()

        # first declare all tokens and expressions
        for name, val in self.defs:
            if val is None:
                _ = g.token(name.name, name.tag)
            else:
                e = g.expr(name.name)
                e.type = name.tag

        # then define all the expressions
        for name, val in self.defs:
            if val is None:
                continue
            e = g.get_name(name.name)
            val.process(g, e)

        g.check()

        return g

# BEGIN GENERATED CODE
# END GENERATED CODE

class TextLocation:
    def __init__(self, line, char):
        self.line = line
        self.char = char

    def __repr__(self):
        return "(line=%d, char=%d)"%(self.line, self.char)

class TextSpan:
    def __init__(self, start, end):
        self.start = start
        self.end = end

    def __repr__(self):
        return "(start=%s, end=%s)"%(self.start, self.end)

def text_span(loc, token_loc):
    return TextSpan(loc.start, token_loc.end)

class CharStream:
    def __init__(self):
        self._lookahead = []
        # We start in line 1, char 1.
        self.line = 1
        # char is a position within this line
        self.char = 1

    def peek(self, index=0):
        while len(self._lookahead) <= index:
            temp = yield
            self._lookahead.append(temp)
        return self._lookahead[index]

    def __next__(self):
        if self._lookahead:
            c = self._lookahead.pop(0)
        else:
            c = yield
        # newlines are considered the end of this current line
        loc = TextLocation(self.line, self.char)
        if c == '\n':
            self.line += 1
            self.char = 1
        else:
            self.char += 1
        return c, loc

    def empty(self):
        return not self._lookahead

class Discard:
    pass

class Tokenizer:
    def __init__(self):
        self.cs = CharStream()
        self.out = []

        def _gen():
            yield from self._tokenize_many()

        self.gen = _gen()
        next(self.gen)

    def feed(self, c):
        if self.gen is None:
            raise RuntimeError("tokenizer is already done!")

        if c is None:
            try:
                # This should finish the generator.
                self.gen.send(None)
                raise RuntimeError("tokenizer failed to exit after None")
            except StopIteration:
                return self.out

        self.gen.send(c)
        out = self.out
        self.out = []
        return out

    def _tokenize_many(self):
        while True:
            token = yield from self._tokenize()
            if token is None:
                break
            if isinstance(token, Discard):
                continue
            self.out.append(token)

    def _tokenize(self):
        c = yield from self.cs.peek()

        if c is None:
            return None

        if c in " \r\n\t":
            return (yield from self._ignore_whitespace())

        if c in "#":
            return (yield from self._ignore_comment())

        singles = {
            "*": ASTERISK,
            ":": COLON,
            "=": EQ,
            "<": LANGLE,
            "[": LBRACKET,
            "(": LPAREN,
            "%": PERCENT,
            "|": PIPE,
            "?": QUESTION,
            ">": RANGLE,
            "]": RBRACKET,
            ")": RPAREN,
            ";": SEMI,
        }
        if c in singles:
            _, loc = yield from next(self.cs)
            return MetaToken(singles[c], c, TextSpan(loc, loc))

        if c == "{":
            return (yield from self._tokenize_code())

        if c in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_":
            return (yield from self._tokenize_text())

        _, loc = yield from next(self.cs)
        msg = "invalid character in tokenizer: " + str(c.encode('utf8'))
        raise MetaSyntaxError(msg, TextSpan(loc, loc))

    def _ignore_whitespace(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c not in " \r\n\t":
                break
            yield from next(self.cs)
        return Discard()

    def _ignore_comment(self):
        # discard the rest of the line
        while True:
            c = yield from self.cs.peek()
            if c is None or c in "\r\n":
                break
            yield from next(self.cs)
        return Discard()

    def _tokenize_code(self):
        # Some number of leading '{' chars which we discard...
        count = 0
        start = None
        while True:
            c = yield from self.cs.peek()
            if c != "{":
                break
            _, loc = yield from next(self.cs)
            if start is None:
                start = loc
            count += 1

        # ... and code body, which we keep...
        text = ""
        while True:
            c = yield from self.cs.peek()
            if c is None:
                raise RuntimeError("unterminated code block:\n" + text)
            if c != "}":
                c, loc = yield from next(self.cs)
                text += c
                continue

            # ... and a matching number of closing '}' chars which we discard.
            matched = 1
            while matched < count:
                c = yield from self.cs.peek(matched)
                if c != "}":
                    break
                matched += 1

            if matched != count:
                # oops, these were actually part of the code body
                for _ in range(matched):
                    c, _ = yield from next(self.cs)
                    text += c
                continue

            # discard these, they're not part of the code body
            for _ in range(matched):
                _, loc = yield from next(self.cs)
            return MetaToken(CODE, text, TextSpan(start, loc))

    def _tokenize_text(self):
        text = ""
        start = None
        loc = None
        while True:
            c = yield from self.cs.peek()
            if c is None:
                break
            if c not in "abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ_0123456789":
                break
            c, loc = yield from next(self.cs)
            text += c
            if start is None:
                start = loc
        keywords = {
            "generator": GENERATOR,
            "kwarg": KWARG,
            "type": TYPE,
            "root": ROOT,
            "fallback": FALLBACK,
        }
        typ = keywords.get(text, TEXT)
        return MetaToken(typ, text, TextSpan(start, loc))

    def iter(self, chars):
        def _chars(self):
            yield from chars
            yield None

        for c in chars:
            tokens = self.feed(c)
            if tokens is None:
                break
            yield from tokens
        yield MetaToken(EOF, "", None)


@contextlib.contextmanager
def read_template(template, tag="GENERATED CODE", file=None):
    if template is None:
        yield 0
        return

    with open(template) as f:
        lines = iter(enumerate(f))
        start_offset = 0
        # passtru pre-tag lines
        for i, line in lines:
            print(line, end="", file=file)
            if "BEGIN " + tag in line:
                start_offset = i + 1
                break
        else:
            print('did not find "BEGIN ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # discard inter-tag lines
        for _, line in lines:
            if "END " in line:
                yield  # generated code gets printed here
                print(line, end="", file=file)
                break
        else:
            print('did not find "END ' + tag + '" line in template!', file=sys.stderr)
            sys.exit(1)

        # passtru post-tag lines
        for _, line in lines:
            print(line, end="", file=file)


def gen(
    grammar_text,
    generator,
    out,
    template=None,
    template_tag=None,
    action="gen_file",
    cli_kwargs=(),
):
    with read_template(template, template_tag, out) as start_offset:
        parser = MetadocParser()

        try:
            for t in Tokenizer().iter(grammar_text):
                # the last time this is called it should return non-None
                parsed_doc = parser.feed(t)
        except MetaSyntaxError as e:
            line = start_offset + e.loc.start.line
            char = e.loc.start.char
            print("syntax error at line=%d, char=%d:"%(line, char), file=sys.stderr)
            print(grammar_text.splitlines()[e.loc.start.line - 1].rstrip('\n'), file=sys.stderr)
            print(" "*(char - 1) + "^", file=sys.stderr)
            exit(1)

        assert parsed_doc, "didn't get a doc!"

        parsed_doc = parsed_doc.val

        g = parsed_doc.build_grammar()

        # TODO: support importing a user-defined generator at runtime
        generator_name = parsed_doc.generator
        if generator:
            # cli --generator overrides %generator directive
            generator_name = generator
        assert generator_name is not None, "neither --generator nor %generator was found"
        generator_cls = {"py": Python, "c": C}[generator_name]

        kwargs = {}
        # extract %kwarg directives
        for kwarg in parsed_doc.kwargs:
            if kwarg.tag not in (None, generator_cls.code_tag):
                continue
            assert kwarg.key not in kwargs, "got multiple %%kwarg for key '%s'"%kwarg.key
            kwargs[kwarg.key] = kwarg.value

        # cli kwargs override %kwarg directives
        for kwarg in cli_kwargs:
            assert "=" in kwarg, "kwarg '%s' is not of the form KEY=VALUE"%kwarg
            k, v = kwarg.split("=", 1)
            kwargs[k] = v

        # extract types
        types = []
        type_names = set()
        for t in parsed_doc.types:
            if t.tag not in (None, generator_cls.code_tag):
                continue
            assert t.name not in type_names, (
                "got multiple '%%type %s' directives"%t.name
            )
            types.append((t.name, t.spec, t.destructor))

        # extract roots
        roots = []
        root_names = set()
        for r in parsed_doc.roots:
            if r.tag not in (None, generator_cls.code_tag):
                continue
            assert r.name not in root_names, (
                "got multiple '%%root %s' directives"%r.name
            )
            assert r.name in g.exprs, (
                "'%%root %s': %s is not a known expression"%(r.name, r.name)
            )
            assert isinstance(g.exprs[r.name], Expression), (
                "'%%root %s': %s is a token, not an expression"%(
                    r.name, r.name
                )
            )
            roots.append(r.name)

        # extract fallbacks
        fallbacks = []
        to_types = set()
        fallback_keys = []
        rev_fallbacks = {}
        for f in parsed_doc.fallbacks:
            if f.tag not in (None, generator_cls.code_tag):
                continue
            assert f.to_type not in to_types, (
                "got duplicate %%fallback %s ..."%f.to_type
            )
            assert f.to_type in g.exprs, (
                "'%%fallback %s ...': %s is not a known token type"%(
                    f.to_type, f.to_type
                )
            )
            assert isinstance(g.exprs[f.to_type], Token), (
                "'%%fallback %s ...': %s is an expression, not a token"%(
                    f.to_type, f.to_type
                )
            )
            to_types.add(f.to_type)
            for from_type in f.from_types:
                assert from_type not in rev_fallbacks, (
                    "got conflicting %%fallbacks: '%%fallback %s %s' "
                    "vs '%%fallback %s %s'"%(
                        rev_fallbacks[from_type], f.to_type,
                        f.to_type, from_type,
                    )
                )
                assert from_type in g.exprs, (
                    "'%%fallback %s %s': %s is not a known token type"%(
                        f.to_type, from_type, from_type
                    )
                )
                assert isinstance(g.exprs[from_type], Token), (
                    "'%%fallback %s %s': %s is an expression, not a token"%(
                        f.to_type, from_type, from_type
                    )
                )
                rev_fallbacks[from_type] = f.to_type
            fallbacks.append((f.to_type, f.from_types))

        generator = generator_cls(
            g,
            roots,
            out,
            types=types,
            fallbacks=fallbacks,
            precode=parsed_doc.precode,
            postcode=parsed_doc.postcode,
            **kwargs,
        )

        try:
            action = getattr(generator, action)
        except AttributeError:
            print(
                "error: generator class '%s' has no method matching the given "
                "--action ('%s')"%(generator_cls.__name__, action),
                file=sys.stderr
            )
            sys.exit(1)
        action()


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("input", nargs="?")
    parser.add_argument("-t", "--template")
    parser.add_argument("-T", "--template-tag", default="GENERATED CODE")
    parser.add_argument("-o", "--output")
    parser.add_argument("-c", "--check", action="store_true")
    parser.add_argument("-g", "--generator")
    parser.add_argument("-a", "--action", default="gen_file")
    parser.add_argument("kwarg", nargs="*")
    args = parser.parse_args()

    assert not args.check or args.output, "--check requires --output"

    if args.input is None:
        text = sys.stdin.read()
    else:
        with open(args.input) as f:
            text = f.read()

    out = io.StringIO()

    gen(
        grammar_text=text,
        generator=args.generator,
        out=out,
        template=args.template,
        template_tag=args.template_tag,
        action=args.action,
        cli_kwargs=args.kwarg,
    )

    if args.check:
        with open(args.output) as f:
            old = f.read()
        if out.getvalue() == old:
            sys.exit(0)
        print(args.output + " would be modified", file=sys.stderr)
        sys.exit(1)

    if args.output is not None:
        with open(args.output, "w") as f:
            f.write(out.getvalue())
    else:
        print(out.getvalue(), end="", flush=True)
